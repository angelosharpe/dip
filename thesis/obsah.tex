%=========================================================================
% (c) Michal Bidlo, Bohuslav Køena, 2008

\chapter{Úvod}
Klasifikace dokumentù podle tématu je jednou z úloh oboru zpracování pøirozeného jazyka (Natural Language Processing - NLP).
Zpracování pøirozeného jazyka je oborem aplikace výpoèetních modelù pro øe¹ení úkolù, které se urèitým zpùsobem vztahují k textu libovolného pøirozeného jazyka.
Historie zpracování pøirozeného jazyka zaèíná v padesátých letech 20. století, kdy britský matematik Alan Turing ve svém èlánku \uv{Computing Machinery and Intelligence} \cite{turing_50} poprvé publikoval takzvaný \uv{Turingùv test}.
Od té doby doznal obor zpracování pøirozeného jazyka velkých zmìn.
V souèasné dobì patøí mezi nejèastìji pou¾ívané metody zpracování pøirozeného jazyka statistické metody a metody strojového uèení (machine learning).

Významnou úlohou øe¹enou v oboru zpracovávání pøirozeného jazyka je klasifikace textu.
Tato úloha spadá do vý¹e zmínìné skupiny statistických metod a metod strojového uèení.
Metody klasifikace textu lze pou¾ít pro velké mno¾ství úloh, které se týkají zpracovávání pøirozeného jazyka, a to pøedev¹ím díky jejich flexibilitì a ji¾ velmi dobøe zpracované teorii.

V této práci se nejprve zabýváme obecnými pøístupy pou¾ívanými pro øe¹ení úloh zpracovávání pøirozeného jazyka a následnì pøedstavujeme dvì zvolené klasifikaèní metody -- jednu probabilistickou a jednu neprobabilistickou.

Tou první, probabilistickou metodou, která je v této práci podrobnì popsána, je metoda nazývaná \textit{Bayesovský klasifikátor}, vyu¾ívající ke klasifikaci textu Bayesova teorému.
Bayesovský klasifikátor, nebo také Bayesovský filtr, jak je obèas nazýván, je velmi èasto pou¾íván e-mailovými klienty pro zji¹»ování, zda je e-mailová zpráva vy¾ádanou, èi nevy¾ádanou po¹tou.

Druhou zvolenou metodou je neporobabilistická klasifikaèní metoda \textit{SVM} (support vector machines), je¾ pomocí namapování vstupního textu do $n$-dimenzionálního prostoru a následným rozdìlením tohoto prostoru na dva poloprostory doká¾e vstupní text klasifikovat.

Obì metody jsou v této práci podrobnì rozebrány jak z teoretického, tak praktického hlediska.

Souèástí této práce je mimo toretické èásti také praktická, implementaèní èást, ve které je vytvoøen program, v nìm¾ jsou implementovány obì vý¹e zmínìné klasifikaèní metody, které jsou následnì testovány a porovnány.
Souèástí praktické èásti je také mno¾ina trénovacích a testovacích dat, které jsou potom pou¾ívány v testech.
V této práci je také popsána implementaèní èást, v ní¾ jsou specifikovány ve¹keré problémy a postupy, pou¾ité pøi implementaci praktické èásti.

Na závìr jsou prezentovány výsledky získané z porovnání dvou vý¹e zmínìných implementovaných klasifikátorù.

\section{Zadání práce a motivace} \label{UVOD-MOTIVACE}
Zadáním této práce bylo seznámit se s problematikou klasifikace dokumentù dle tématu a zvolit si dvì metody, kterými se budu podrobnìji zabývat.
Následnì tyto dvì metody analyzovat a implementovat program, který pomocí tìchto metod bude klasifikovat vstupní dokumenty do urèitých tøíd.
U obou metod jsem mìl dle zadání dbát na výbìr vhodných pøíznakù z textu.

Pro potøeby této práce jsem anotoval velkou sadu dokumentù, které budu následnì pou¾ívat pro trénování a testování mnou vytvoøeného programu.
Na tìchto datech následnì porovnám dvì zvolené metody klasifikace pomocí standardních metrik pro hodnocení klasifikátorù.

Téma diplomové práce jsem si zvolil, jeliko¾ jsem témìø dva roky pracoval na projektu M-Eco\footnote{http://www.meco-project.eu/} ve skupinì NLP na VUT FIT, kde jsem se zabýval klasifikací tweetù\footnote{Tweety -- pøíspìvky zveøejòované na sociální síti Twitter (http://twitter.com/)}.
Reálné vyu¾ití Bayesovského klasifikiátoru pøi této práci mne motivovalo k tomu, abych se klasifikaèními metodami zabýal dále.
Této práce bych proto chtìl vyu¾ít k tomu, abych prohloubil své znalosti o problematice klasifikace dokumentù a porozumìní klasifikaèním metodám.
V rámci klasifikaèních metod se chci zamìøit pøedev¹ím na klasifikátor zalo¾ený na SVM, jeliko¾ metoda SVM je velmi dobøe variabilní a lze ji pou¾ít pro øe¹ení velkého mno¾ství rùzných klasifikaèních, ale napøíklad i regresních úkolù.

Bayesovský klasifikátor implementovaný v této práci byl nasazen do reálného provozu v projektu M-Eco.

\section{Návaznost práce na semestrální projekt} \label{UVOD-NAVAZNOST}
Tato práce úzce navazuje na odevzdaný dokument semestrálního projektu, jen¾ byl vytvoøen pro stejnojmenný pøedmìt.

Semestrální projekt obsahoval úvodní kapitolu o problematice zpracování pøirozeného jazyka a popis problematiky naivního Bayesovského klasifikátoru, jen¾ byl pro tuto práci také implementován (viz \ref{BAYES}).
Byla vytvoøena malá testovací sada pro trénování a testování tohoto klasifikátoru.
Na této datové sadì a klasifikátorem byly následnì provedeny testy, v nich¾ byl zhodnocen vliv tokenizace (viz \ref{TOKENIZACE}) na klasifikaèní schopnosti implementovaného Bayesovského klasifikátoru.
V odevzdané práci je¹tì nebyly implementovány speciální pøíznaky (viz \ref{TOKENIZACE-SPEC}).



\chapter{Zpracování pøirozeného jazyka}
V této kapitole se budeme struènì zabývat historií (viz \ref{NLP-H}) oboru zpracovávání pøirozeného jazyka (natural language processing, dále té¾ NLP), poté hlavními úkoly, které se v oboru zpracovávání pøirozeného jazyka øe¹í (\ref{NLP-HU}) a nakonec podrobnìji i klasifikaèními úlohi(viz \ref{NLP-KU}), které jsou hlavním tématem této práce.

\section{Historie} \label{NLP-H}
Obor zpracovávání pøirozeného jazyka se vyvíjí soubì¾nì s historií vývoje výpoèetní techniky.
Po roce 1945, kdy spoleèensko politická situace ve svìtì umo¾nila zmìnu hlavních dosavadních smìrù výzkumu v oblasti výpoèetní techniky od pùvodnì pøevá¾nì vojenského vyu¾ití (napø. kryptografie, kryptoanalýza, atd.) k dal¹ím vìdním oborùm.
Díky tomu se zaèal rozvíjet také obor zpracovávání pøirozeného jazyka.

Jak ji¾ bylo zmínìno v úvodu, jedním z prvních významných mílníkù v historii zpracovávání pøirozeného jazyka byl èlánek Alana Turinga s názvem \uv{Computing Machinery and Intelligence} \cite{turing_50}, v nìm¾ Turing publikoval takzvaný \textit{Turingùv test} jako¾to kritérium inteligence.
Aby poèítaèový program pro¹el \textit{Turingovým testem}, nesmí nestranný soudce poznat z obsahu konverzace mezi programem a èlovìkem (konverzace probíhá v reálném èase), která strana je která.

Vìdci zabývající se zpracováváním pøirozeného jazyka logicky hledali pomoc v lingvistice.
V roce 1957  publikoval americký lingvista Noam Chomsky èlánek \uv{Syntactic structures}, který znamenal zcela nový pohled na lingvistiku.
Formuloval v nìm teorii takzvané \textit{Transformaèní gramatiky}, v ní¾ tvrdil, ¾e ka¾dá vìta libovolného jazyka má dvì úrovnì reprezentace
\begin{itemize}
 \item \textit{Hloubkovou} -- reprezentující sémantické vztahy ve vìtì. Hloubkovou úroveò lze pøevést na povrchovou úroveò
 \item \textit{Povrchovou} -- reprezentující fonémickou formu vìty
\end{itemize}
Hluboké úrovnì v¹ech jazykù podle Chomského vykazují znaènou podobnost, která se vytrácí v úrovních povrchových.
Chomského èlánek silnì ovlivnil témìø v¹echny následné výzkumy v oboru zpracování pøirozeného jazyka.

Jednou z prvních øe¹ených úloh v oboru zpracování pøirozené øeèi bylo vytvoøení automatických pøekladaèù, tedy programù, které bez lidského pøispìní doká¾ou pøelo¾it vstupní text z jednoho pøirozeného jazyka do druhého.
Uspokojivé øe¹ení tohoto úkolu v¹ak v té dobì nebylo nalezeno, a ani v roce 1966 je¹tì výzkumníci nebyli nikterak blízko k vyvinutí takového softwaru.
V roce 1966 proto vydal americký vládní výbor \uv{The Automatic Language Processing Advisory Committee} (ALPAC) zprávu shrnující dosavadní výsledky výzkumu a konstatující, ¾e \uv{Nebyl vyvinut ¾ádný strojový pøekladaè obecných vìdeckých textù a ¾ádný takový pøekladaè nebude vyvinut ani v blízké budoucnosti}.
Takto formulovaný závìr zprávy zapøíèinil výrazné ¹krty v rozpoètech vìdeckých týmù, zabývajících se výzkumem zpracovávání pøirozeného jazyka a pøekladaèù.

V 60. letech byl americkým matematikem nìmeckého pùvodu Josephem Weizenbaumem vytvoøen program ELIZA, co¾ byl jednoduchý robot, urèený pro konverzaci s èlovìkem (chatbot).
Konverzace s tímto robotem se zakládala na opakování výrokù, které u¾ivatel zadal a na kladení velmi jednoduchých otázek, zalo¾ených na klíèových slovech, nalezených v pøedchozí konverzaci.
Program se v diskusi choval jako psychoterapeut, reagující v¾dy bezprostøednì na pøedchozí výrok èlovìka, tedy \uv{pacienta}.
Bìhem sedmdesátých let bylo vyvinuto mnoho dal¹ích programù pro konverzaci s u¾ivatelem podobných programu \textit{ELIZA}, napøíklad \textit{PARRY} nebo \textit{Jabberwacky}.

Do 80. let 20. století pou¾ívala drtivá vìt¹ina systémù pro zpracovávání pøirozeného jazyka velmi slo¾itá ruènì zadávaná pravidla.
V 80. letech v¹ak byly poprvé pøedstaveny metody strojového uèení, které díky stále rostoucímu výkonu výpoèetní techniky umo¾òovaly generovat tato pravidla automaticky a dosahovat tak pøi zpracovávání pøirozeného jazyka stále lep¹ích výsledkù.
S nástupem strojového uèení se odvìtví zpracovávání pøirozeného jazyka zaèalo zamìøovat na statistické metody, je¾ k øe¹ení problémù pøistupovaly jinak ne¾ metody dosavadní.
Jejich hlavním principem byla práce s pravdìpodobnostními modely a váhami jednotlivých rozhodnutí.
Tímto zpùsobem bylo mo¾né efektivnìji øe¹it vìt¹inu NLP úkolù.
Vzhledem k tomu, ¾e obor zpracovávání pøirozeného jazyka pracuje s daty vytvoøenými èlovìkem, je¾ mohou obsahovat rùzné chyby, pracují statistické metody znaènì spolehlivìji ne¾ døívìj¹í ruènì psaná pravidla.

V souèasné dobì je výzkum zpracovávání pøirozeného jazyka orientován pøedev¹ím na vytvoøení autonomních a semiautonomních uèících agoritmù, tedy algoritmù, schopných uèit se z dat, která pøedtím nebyla ruènì anotována, a nebo z kombinace anotovaných a neanotovaných dat.
%%%%%%%%%
%%%%%%%%
%%%%%%%%
%%%%%%%%%%

\section{Hlavní úkoly øe¹ené v NLP} \label{NLP-HU}
Obor zpracovávání pøirozeného jazyka je velmi rozsáhlý a existuje v nìm mimo klasifikace textu velké mno¾ství dal¹ích úloh k øe¹ení.
V následující èásti této práce se podíváme na hlavní úkoly, kterými se oblast NLP zabývá.

\subsection{Automatická sumarizace}
Úkolem automatické sumariace je redukovat vstupní text nebo sadu textù do nìkolika slov, nebo krátkého odstavce, popisujícího sémantický obsah vstupního textu.
Základním principem automatické sumarizace je zpracovávání slov, frází a vìt ze vstupního souboru, respektive vstupních souborù.
Z vý¹e zmínìných dat program pro automatickou sumarizaci vygeneruje vnitøní sémantický popis tìchto dat.
Ze sémantického popisu potom mù¾e vygenerovat buï sadu vhodných slov, popisujících daný soubor, nebo je mo¾né pou¾ít metody pro generování pøirozeného jazyka a vytvoøit tak vìtný popis vstupního textu.

\subsection{Generování pøirozeného jazyka} \label{NLP-HU-generovani}
Generování pøirozeného jazyka má vytvoøit výstup v pøirozeném jazyce z interní reprezentace v poèítaèi.
De facto je generátor pøirozeného jazyka pøekladaè, pøekládající data z jednoho jazyka do druhého (z vnitøní reprezentace poèítaèe do pøirozeného jazyka).
Tato úloha je opakem úlohy porozumìní pøirozenému jazyku (\ref{NLP-HU-porozumeni}).

\subsection{Porozumìní pøirozenému jazyku} \label{NLP-HU-porozumeni}
Programy pro porozumìní pøirozenému jazyku mají pøevést vstupní text do podoby zpracovatelné poèítaèem, tedy porozumìt textu a vygenerovat vnitøní reprezentaci onìch vstupních dat.
Proces porozumìní pøirozenému jazyku je sice opakem úlohy generování pøirozeného jazyka (\ref{NLP-HU-generovani}), ale je znaènì slo¾itìj¹í, a to nejen kvùli rozmanitosti vstupního jazyka a tudí¾ mo¾nosti výskytu neznámých slov, ale také kvùli nutnosti zvolení vhodných syntaktických a sémantických schémat aplikovaných ve vstupním textu.

\subsection{Odpovídání na otázky}
V oblasti odpovídání na otázky se programátoøi pokou¹ejí vytvoøit program, který by dokázal korektnì odpovìdìt na u¾ivatelem zadanou vstupní otázku formulovanou v pøirozeném jazyce.
V roce 2011 vytvoøila spoleènost IBM poèítaè s názvem \textit{Watson} specializovaný na odpovídání na otázky, který následnì vyhrál americkou vìdomostní soutì¾ \textit{Jeopardy!}, kdy¾ porazil dva nejlep¹í hráèe této soutì¾e v její historii.

\subsection{Odstraòování víceznaènosti slov v textu}
Odstraòování víceznaènosti slov v textu je významnou úlohou napomáhající správnému porozumìní textu.
Víceznaèná slova jsou v textu identifikována a poté je vyhledáván jejich správný význam v kontextu vstupního textu.
Po zji¹tìní významu slova je slovo nahrazeno nevýceznaèným synonymem.
Tato úloha je velmi dùle¾itá napøíklad pro zlep¹ování kvality a pøesnosti internetových vyhledávaèù.
Velmi zajímavé øe¹ení tohoto problému publikovala Rada Mihalcea v \uv{Using Wikipedia for Automatic Word Sense Disambiguation} \cite{wp_word_sense}, která øe¹ila identifikaci víceznaèných slov za pomoci textového obsahu internetové encyklopedie \textit{Wikipedia}\footnote{http://wikipedia.org/}.

\subsection{Strojový pøeklad}
Posledním úkolem oboru zpracovávání pøirozeného jazyka, kterým se zde budeme zabývat, je strojový pøeklad.
Jak bylo vý¹e zmínìno v kapitole o historii zpracovávání pøirozeného jazyka, je strojový pøeklad jednou ze zásadních úloh, kterými se obor zpracovávání pøirozeného jazyka zabývá.
Z øady rozmanitých pøístupù k øe¹ení strojového pøekladu zmíníme nejstar¹í metodu, zalo¾enou na sadì pravidel, pomocí nich¾ je pøeklad uskuteèòován a také statistické pøekladaèe, jako napøíklad \textit{Google translator}, vyu¾ívající pro pøeklad statistických modelù.


\subsection{Klasifikaèní úlohy} \label{NLP-KU}
Nyní se podíváme na následující tøi klasifikaèní úlohy:
\begin{itemize}
 \item Anonymizace \ref{NLP-KU-anonym}
 \item Klasifikace tématu \ref{NLP-KU-klas_tem}
 \item Filtrování spamu \ref{NLP-KU-spam_filt}
\end{itemize}

\subsubsection{Anonymizace}\label{NLP-KU-anonym}
Klasifikaèní úloha anonymizace není pøi zpracovávání pøirozeného jazyka pøíli¹ èasto øe¹ená.
Jejím cílem je odstranìní citlivých referencí (napøíklad osobních informací -- rodného èísla, e-mailové adresy, atd.) z tìla daného textu, co¾ umo¾òuje následnì anonymizovaný text vyu¾ít napøíklad pro výzkumné úèely.
Anonymizace na rozdíl od spam filteringu vy¾aduje daleko jemnìj¹í pøístup ke klasifikaci, jeliko¾ pracuje nikoliv s celým dokumentem, ale pouze s jeho èástmi, nìkdy dokonce jen nìkolika slovy, nebo vìtami.
Pro anonymizaci jsou nejèastìji pou¾ívány tyto 3 postupy: 
\begin{itemize}
 \item Odstranìní -- odstranìní v¹ech citlivých informací z dokumentu a jejich nahrazení výplní
 \item Pseudoanonymizace -- nahrazení v¹ech citlivých informací náhodnými hodnotami stejného typu
 \item Kategorizace -- nahrazení v¹ech citlivých informací kategorií do které spadají.
\end{itemize}
Zpùsoby u¾ití anonymizace viz obr. \ref{NLP-KU-anonym-img}.
\begin{figure}[h]
    \begin{center}
      \includegraphics[width=15cm,keepaspectratio]{fig/anonymization}
      \caption{Zpùsoby pou¾ití anonymizace.} 
      \label{NLP-KU-anonym-img}
    \end{center}
\end{figure}

\subsubsection{Klasifikace tématu}\label{NLP-KU-klas_tem}
Klasifikace tématu je úloha pøiøazování názvù témat ke vstupním textovým dokumentùm.
Typicky daný vstupní text pokrývá vìt¹í mno¹ství témat.
Metody pro klasifikaci tématu mohou být zalo¾eny napøíklad na skrytých Markovových modelech.
Výstupem klasifikátoru tématu pro vstupní text bývá velmi èasto kromì seznamu tøíd témat, kterých se zøejme vstupní text týká, také seznam pravdìpodobností definující míru nále¾itosti do jednotlivých tìchto tøíd.

\subsubsection{Filtrování spamu}\label{NLP-KU-spam_filt}
Zájem o klasifikaèní problém filtrování spamu v posledních letech velmi výraznì vzrostl, a to pøedev¹ím kvùli mno¾ství nevy¾ádané po¹ty (spamu), kterou u¾ivatelé dostávají do svých e-mailových schránek.
Jsou dvì mo¾nosti jak úloha filtrování spamu mù¾e fungovat.
Buï jsou zprávy filtrovány na základì obsahu (a» u¾ textového, nebo jiného) nebo na základì metainformací v hlavièce zprávy.
Metody zabývající se filtrováním na základì obsahu e-mailu velmi èasto vyu¾ívají toho, ¾e vìt¹ina zpráv obsahuje nìjakou textovou informaci.
Podle ní potom klasifikují, zda je zpráva nevy¾ádanou po¹tou.
Klasifikátory urèující, zda je zpráva spam nebo ne mají pro klasifikaci e-mailù velmi èasto asymetrické ohodnocení pøi klasifikaci.
Chybné oznaèení vy¾ádané po¹ty jako spamu, vedoucí k následnému odstranìní zprávy, je toti¾ vìt¹í problém, ne¾ oznaèení spamu jako vy¾ádané po¹ty.

V tomto èlánku se budeme zabývat právì pøedev¹ím metodami filtrování spamu, a to nejen pro pou¾ítí pøi jeho odfiltrovávání z e-mailové po¹ty, ale také pro oznaèování relevantních textových vstupù vzhledem k danému tématu.

\chapter{Klasifikace dokumentù}
Jedním z úkolù této práce je získat pøehled o klasifikaci textových dokumentù, seznámit se s metodami klasifikace a aplikovat je na vytvoøenou datovou sadu.
\section{Definice klasifikace} \label{CLASS-DEF}

\begin{definition}
 Klasifikace je èinnost, která rozdìluje objekty do tøíd (kategorií) podle jejich spoleèných vlastností.
\end{definition}

Tøída v kontextu zpracovávání pøirozeného jazyka je mno¾ina objektù, vyznaèujících se urèitou spoleènou vlastností nebo vlastnostmi, která/é danou tøídu popisuje/í.
Klasifikace je potom èinnost, která pøiøazuje objekty do daných tøíd.
Nadále se budeme zabývat zejména klasifikací textu.

Obecnì mìjme tedy prostor objektù $X$ a mno¾inu tøíd $Y$.
Operaci klasifikace potom odpovídá funkci $f$:

\begin{equation}
f: X \rightarrow Y
\end{equation}

Klasifikaèní funkce $f$ tedy pøiøadí jeden objekt z mno¾iny objektù právì do jedné tøídy.
Mù¾e ov¹em nastat situace, kdy jeden objekt odpovídá kritériím pro zaøazení do více tøíd.
Napøíklad budeme klasifikovat textovou zprávu, je¾ mù¾e z hlediska obsahu zapadnout do více tøíd, napøíklad do tøídy osobních zku¹eností (pisatel pí¹e o vlastní zku¹enosti) a do tøídy relevantní k tématu zdraví (\textit{\uv{Bolí mì hlava.}}).
V tomto pøípadì musíme modifikovat klasifikaèní funkci $f$ následovnì:

\begin{equation}
f: X \rightarrow 2^Y
\end{equation}

kde $2^Y$ oznaèuje potenèní mno¾inu v¹ech tøíd.
Tato forma klasifikace se také oznaèuje jako \textit{multi-label klasifikace}.
V \textit{multi-label} klasifikaci se ke ka¾dé tøídì mù¾e pøiøadit reálné èíslo $p \in <0,1>$, které definuje míru nale¾itosti klasifikovaného objektu do pøiøazených tøíd.
Tomuto se také øíká \textit{soft klasifikace}.
Naproti tomu, kdy¾ je pøi klasifikaci pøiøazen objekt do tøídy \uv{napevno} (ano, patøí tam / ne, nepatøí tam), pak tento zpùsob klasifikace nazýváme \textit{hard klasifikace}.

\section{Klasifikaèní pøístupy}
Existují dva hlavní pøístupy pøi øe¹ení klasifikaèních úloh:
\begin{itemize}
 \item Probabilistické
 \item Neprobabilistické
\end{itemize}

\subsection{Probabilistické klasifikátory}
Jedním ze zpùsobù jakým lze vytvoøit funkèní klasifikátor je vyu¾ít teorie pravdìpodobnosti.
Klasifikátory fungující na bázi pravdìpodobnostních výpoètù urèují pravdìpodobnosti, se kterými daný objekt spadá do nìkteré tøídy.
Do které tøídy respektive kterých tøíd objekt spadá, je následnì urèeno pomocí pøedem definovaného prahu (threshold).

Mezi probabilistické klasifikaèní metody patøí napøíklad metoda zalo¾ená na Bayesovì teorému -- Bayesovský klasifikátor.
Bayesùv teorém a jeho pou¾ití pro klasifikaci je podrobnì popsán v kapitole \ref{BAYES}.

\subsection{Neprobabilistické klasifikátory}
Kromì probabilistických metod existují také neprobabilistické metody klasifikace.
Neprobabilistické klasifikátory se vìt¹inou sna¾í pøímo vymodelovat klasifikaèní funkci a nevyu¾ívají pro zaøazení objektù do tøíd pravdìpodobnostní výpoèty.
Asi nejznámìj¹í neprobabilistickou metodou je metoda SVM (support vector machines), která se sna¾í nalézt takovou nadrovinu v prostoru pøíznakù, která bude rozdìlovat trénovací data.
Ideální nadrovina rozdìluje data z trénovací mno¾iny tak, ¾e body v prostoru le¾í v opaèných poloprostorech a vzdálenosti v¹ech bodù od roviny jsou co nejvìt¹í.

\section{Klasifikace a clustering}\label{CLASS-APPROACHES-CLUSTERING}
Je¹tì relativnì nedávno byla klasifikace chápána jako podmno¾ina úlohy nazývané clustering.
Je sice pravda, ¾e klasifikace a clustering k sobì mají velmi blízko, a také mnoho technik pou¾ívaných v klasifikaci je mo¾né pou¾ít i v clusteringu, nicménì rozdíl mezi tìmito dvìma úlohami je v tom, ¾e pøi klasifikaci známe pøedem tøídy, do kterých budeme vstup klasifikovat.
U clusteringu tomu tak není.
Clustering dìlí vstupní text podle významných spoleèných pøíznakù ve vstupních datech.
Z definice klasifikace (viz \ref{CLASS-DEF}) víme, ¾e klasifikaèní úloha je definována klasifikaèní funkcí $f$,  která vstupnímu vzorku $x$ podle jeho pøíznakù pøiøadí oznaèení tøídy $y$, do ní¾ vzorek spadá.
Klasifikaèní úlohy v NLP se sna¾í tuto funkci $f$ co nejpøesnìjí aproximovat, aby simulovaly její výsledek.
Naroti tomu clustering nemá podobnou funkci jako klasifikace, která by urèovala, jak má vypadat výsledek.
Výsledná struktura tøíd clusteringu je vytvoøena za bìhu metody na základì znakù podobnosti vzorkù z mno¾iny vstupù $x$.

\subsection{Strojové uèení}
Jeliko¾ pro vytvoøení funkèního klasifikátoru je nutné, aby klasifikátor byl nauèen pomocí správné testovací mno¾iny, je strojové uèení s tématem klasifikace velmi úzce spojeno.
Strojové uèení je jedním z podtémat oboru umìlé inteligence, zabývající se vytvoøením algoritmù, umo¾òujících poèítaèovým programùm se uèit.
Algoritmy strojového uèení se dìlí do následujících tøí kategorií:
\begin{itemize}
 \item \textit{Uèení s uèitelem (supervised learning)} -- nejjednodu¹¹í zpùsob uèení. Problematiènost tohoto pøístupu spoèívá v tom, ¾e ve¹kerá data, která se program nauèí, musejí být ruènì anotována èlovìkem, z èeho¾ vyplývá jeho velká èasová nároènost.
 \item \textit{Uèení bez uèitele (unsupervised learning)} -- pøi uèení bez uèitele je uèenému programu pøedána sada trénovacích dat, ve které si uèící se program sám hledá význaèné vlastnosti, na jejich¾ základì poté vytvoøí vlastní klasifikaèní funkci. Jedním z mo¾ných pøístupù k øe¹ení této metody je metoda clustering (viz \ref{CLASS-APPROACHES-CLUSTERING}).
 \item \textit{Pøístup na pomezí mezi uèením s uèitelem a uèením bez uèitele (semi-supervised learning)} --  je metoda zalo¾ená na uèení pomocí vstupních trénovacích dat, kterých následnì metoda vyu¾ije k automatickému vytvoøení dal¹ích trénovacích dat.
\end{itemize}

\subsection{Boosting}
V roce 1988 vyslovil Michael Kearns ve své práci s názvem \uv{Thoughts on hypothesis boosting} otázku, zda lze z mno¾iny slabých klasifikátorù (takových, které mají nízké hodnoty korelace testovacích a výstupních dat) vytvoøit jeden klasifikátor, který by mìl výraznì lep¹í výsledky.
Ve své práci dokázal, ¾e tomu tak opravdu mù¾e být.
Od té doby bylo vyvinuto více algoritmù pro boosting klasifikace, nicménì vìt¹ina z nich se zakládá na iterativním uèení mno¾iny slabých klasifikátorù a jejich následném sdru¾ení do jednoho pøesného klasifikátoru.
Historicky nejvýznam¹j¹ím boostingovým algoritmem je zøejmì algoritmus \textit{AdaBoost}(adaptive boosting), vytvoøený Yoavem Freundem a Robertem Schapirem, který publikovali v práci \uv{A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting}\cite{boosting}.

\chapter{Bayesovský klasifikátor} \label{BAYES}
Bayesovský klasifikátor (naivní Bayesovský klasifikátor) je klasifikátor postavený na zjednodu¹eném Bayesoì teorému.
Zjednodu¹ení spoèívá v tom, ¾e existence nebo naopak neexistence jednoho tokenu (slova nebo ostatních pøíznakù -- viz \ref{TOKENIZACE}) není závislá na existenci nebo neexistenci jiného tokenu.
Tudí¾, i kdy¾ tokeny na sobì závisejí, Bayesovský klasifikátor s nimi pracuje jako se zcela nezávislými událostmi.
Tato vlastnost naivního Bayesovského filtrování je nevýhodou pro klasifikování pøirozeného jazyka pøedev¹ím proto, ¾e slova se v jazyce vyskytují ve slovních spojeních, je¾ stejnì jako samotná slova napomáhají klasifikaci.
Aèkoliv tato vlastnost Bayesovský klasifikátor omezuje, jeho vyu¾ití na reálných textech se osvìdèilo a je hojnì pou¾íván.
V posledních letech ale ji¾ existují jiné a lep¹í metody pro klasifikaci, napøíklad metoda \textit{support vector machines}, která je v této práci podrobnì popsána.

\section{Matematický model}
Bayesovský klasifikátor vyu¾ívá Bayesova teorému, který zní následovnì:
\begin{theorem}
  Mìjme dva náhodné jevy $A$ a $B$ s pravdìpodobnostmi $P(A)$ a $P(B)$, pøièem¾ $P(B) > 0$. Potom platí:
  \begin{equation}
    P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} \label{SF-BAYES-MAT-bayes_theorem}
  \end{equation}
\end{theorem}
\begin{proof}
  Dle podmínìných pravdìpodobností platí, ¾e pravdìpodobnost dvou událostí $A$ a $B$ -- $P(A \bigcap B)$ se rovná pravdìpodobnosti $A$ krát pravdìpodobnost $B$ za pøedpokladu, ¾e nastalo $A$ -- $P(B|A)$.
  \begin{equation}
    P(P(A \cap B)) = P(A) \cdot P(B|A)
  \end{equation}  
  Dále také platí, ¾e pravdìpodobnost $A$ a $B$ se rovná pravdìpodobnosti $B$ krát pravdìpodobnost $A$ za pøedpokladu ¾e nastalo $B$:
  \begin{equation}
    P(P(A \cap B)) = P(B) \cdot P(A|B)
  \end{equation}
  Z tìchto dvou vztahù vychází:
  \begin{equation}
    P(B) \cdot P(A|B) = P(A) \cdot P(B|A)
  \end{equation}
  Upravením této rovnice poté dostáváme:
  \begin{equation}
    P(A|B) = \frac{P(A) \cdot P(B|A)}{P(B)}
  \end{equation}
  Co¾ je Bayesùv teorém.
  \hfill \textbf{Q.E.D}.
\end{proof}

Pro klasifikaci textu není vhodné pou¾ít pøímo rovnici z Bayesova teorému, jeliko¾ by bylo tøeba zapamatovat si pro ka¾dý token tøi hodnoty a bylo by nutné provádìt velké mno¾ství výpoètù, nicménì je mo¾né rovnici upravit do tvaru, v nìm¾ si pro ka¾dý token vìty je tøeba pamatovat pouze jednu hodnotu pravdìpodobnosti a také není tøeba provádìt tolik výpoètù.
Upravená rovnice má nasledující tvar:

\begin{equation}
P = \frac{p_1 p_2 p_3 \ldots p_N}{p_1 p_2 p_3 \ldots p_N + (1 - p_1) (1 - p_2) (1 - p_3) \ldots (1 - p_N)} \label{equation_used} \\
\end{equation}
kde $P$ je pravdìpodobnost urèující míru nále¾itostí klasifikované zprávy do tøídy spam a $p_i, i=1 \ldots N$ udává tuto míru pro jednotlivé tokeny $i$.
Výsledné $P$ je potom porovnáno s urèitým prahem, který definuje, zda je oklasifikovaná zpráva spam nebo ham.


\subsection{Odvození rovnice pro klasifikaci} \label{SF-BAYES-MAT-bayes_theorem-deriv}
Nyní pøejdìme k odvození rovnice \ref{equation_used} z Bayesova teorému \ref{SF-BAYES-MAT-bayes_theorem}.
 
Mìjme $X$ a $Y$, pøedstavující dva tokoeny, $S$ znamenající \uv{je to spam} a $\neg S$ znamenající \uv{je to ham} (=není to spam).
Pro zjednodu¹ení budeme pou¾ívat pøípad se dvìma tokeny.

$$
 P(S | X \cap Y) = \frac{P(X \cap Y | S) \cdot P(S)}{P(X \cap Y)}
$$

Nyní vyu¾ijeme toho, ¾e platí

$$
P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)
$$

Tudí¾:
\begin{equation}
 P(S | X \cap Y) = \frac{P(X \cap Y | S) \cdot P(S)}{(P(S)*P(X \cap Y | S) + P(\neg S) \cdot P(X \cap Y | \neg S))} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq1}
\end{equation}
Nyní aplikujeme onu naivitu, kterou jsme zmiòovali vý¹e \ref{BAYES}.

To znamená, ¾e budeme pøedpokládat, ¾e pravdìpodobnost $X$ je nezávislá na pravdìpodobnosti $Y$, tak¾e mù¾eme pou¾ít vztah:
  $$P(A \cap B) = P(A) \cdot P(B)$$
Z toho vyplývá, ¾e rovnici \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq1} mù¾eme upravit do tvaru
\begin{equation}
  P(S | X \cap Y) = \frac{P(X|S) \cdot P(Y|S) \cdot P(S)}{P(S) \cdot P(X|S) \cdot P(Y|S) + P(\neg S) \cdot P(X| \neg S) \cdot P(Y| \neg S)} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq2}
\end{equation}
Vyjdeme opìt z Bayesova teorému, který øíká, ¾e platí:
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$
Toho vyu¾ijeme a dosadíme do rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq2}, dostaneme tedy:
\begin{equation}
  P(S | X \cap Y) = \frac{\frac{P(S|X) \cdot P(X)}{P(S)} \cdot \frac{P(S|Y) \cdot P(Y)}{P(S)} \cdot P(S)}{P(S) \cdot \frac{P(S|X) \cdot P(X)}{P(S)} \cdot \frac{P(S|Y) \cdot P(Y)}{P(S)} + P(\neg S) \cdot \frac{P(\neg S|X) \cdot P(X)}{P(\neg S)} \cdot \frac{P(\neg S|Y) \cdot P(Y)}{P(\neg S)}} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq3}
\end{equation}
Z rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq2} mù¾eme odstranit $P(X)$ a $P(Y)$:
$$
  P(S | X \cap Y) = \frac{\frac{P(S|X)}{P(S)} \cdot \frac{P(S|Y)}{P(S)} \cdot P(S)}{P(S) \cdot \frac{P(S|X)}{P(S)} \cdot \frac{P(S|Y)}{P(S)} + P(\neg S) \cdot \frac{P(\neg S|X)}{P(\neg S)} \cdot \frac{P(\neg S|Y)}{P(\neg S)}}
$$
Po zjednodu¹ení dostaneme:
\begin{equation}
  P(S | X \cap Y) = \frac{\frac{P(S|X) \cdot P(S|Y)}{P(S)}}{\frac{P(S|X) \cdot P(S|Y)}{P(S)} + \frac{P(\neg S|X) \cdot P(\neg S|Y)}{P(\neg S)}} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq4}
\end{equation}
Nyní mù¾eme pøistoupit k závìreèné úpravì, která ov¹em pøedpokládá, ¾e zprávy v uèící mno¾inì jsou rovnomìrnì rozlo¾eny, tzn. ¾e zhruba 50\% nauèených zpráv je spam a zhruba 50\% je ham.
Potom  platí, ¾e $P(S) \approx P(\neg S)$, a tak dostaneme z rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq4} rovnici následující:
\begin{equation}
  P(S | X \cap Y) = \frac{P(S|X) \cdot P(S|Y)}{P(S|X) \cdot P(S|Y) + P(\neg S|X) \cdot P(\neg S|Y)} 
\end{equation}
kde $P(\neg A) = 1 - P(A)$ tudí¾ $P(\neg A|B)$ mù¾eme pøepsat na $1-P(A|B)$.
Z toho získáme rovnici \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq5} ekvivalentní s rovnicí \ref{equation_used}.
\begin{equation}
  P(S | X \cap Y) = \frac{P(S|X) \cdot P(S|Y)}{P(S|X) \cdot P(S|Y) + (1 - P(S|X)) \cdot (1- P(S|Y))} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq5}
\end{equation}
Jestli¾e není uèící mno¾ina vyvá¾ená (podobné mno¾ství spamu a hamu), je vhodné pro klasifikaci pou¾ít rovnici \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq4}, proto¾e pøi pou¾ití rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq5} mù¾e docházet ke zkreslení klasifikace.

\section{Princip Bayesovského klasifikátoru} \label{SF-BAYES-PRINCIP}
Bayesovský klasifikátor si do databáze pøi uèení ukládá pravdìpodobnosti zda jsou jednotlivé tokeny ze vstupních vìt spam, tzn. ukládá si hodnotu  $P(S|X)$.
Po nauèení dostateènì velkého objemu trénovacích dat potom vyu¾ívá tyto pravdìpodobnosti pro výpoèet, zda je klasifikovaný vstupní text spam, nebo ham.

Klasifikace tedy probíhá tak, ¾e vstupní text $X$ se rozdìlí na tokeny $X_i$.
Klasifikátor se podívá do databáze a zjistí pravdìpodobnost $P(S|X_i)$ se kterou jednotlivé tokeny nále¾í do tøídy spamu.
Jestli¾e klasifikátor pøi klasifikaci textu narazí na token, který dosud není v jeho databázi, pak je mu pøiøazena hodnota 0,5 (tzn. klasifikátor neumí urèit s jakou pravdìpodobností je tento token spam èi ham).
Pravdìpodobnost $P(X)$ ¾e vstupní text je spam je potom vypoèten pomocí dosazení $P(S|X_i)$ do rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq5}.

\section{Pøíklad klasifikace}
Uveïme si nyní jednoduchý pøíklad klasifikace textu:
mìjme vstupní vìtu \textit{'Aaaa, my stomach hurts.'}.
Tuto vìtu si klasifikátor rozdìlí podle zadaného pravidla na seznam tokenù (v tomto pøípadì pouzze slov).
$$['Aaaa', 'my', 'stomach', 'hurts']$$
V databázi klasifikátoru máme napøíklad:
$$\{'my':0.6, 'stomach':0.2, 'hurts':0.2\}$$
V¹imnìme si, ¾e v databázi tokenù klasifikátoru se nevyskytuje slovo \texttt{'Aaaa'}.
To znamená, ¾e klasifikátor se pøi svém uèení s tímto slovem doposud nesetkal, a tak mu pøiøadí pravdìpodobnost 0,5.
Jeho databáze tokenù pro klasifikaci vstupní vìty \textit{'Aaaa, my stomach hurts.'} bude tedy vypadat následovnì:
$$\{'my':0.6, 'stomach':0.2, 'hurts':0.2, 'Aaaa':0.5\}$$
Nyní u¾ má klasifikátor v¹e potøebné k tomu, aby vypoèítal pravdìpodobnost, zda je vstupní text spam èi nikoliv.
Bude postupovat podle rovnice \ref{equation_used}.
\begin{align*}
P &= \frac{p_1 p_2 p_3 \ldots p_N}{p_1 p_2 p_3 \ldots p_N + (1 - p_1) (1 - p_2) (1 - p_3) \ldots (1 - p_N)} \\
P &= \frac{0.2 \cdot 0.6 \cdot 0.2 \cdot 0.5}{0.2 \cdot 0.6 \cdot 0.2 \cdot 0.5 + (1 - 0.2) \cdot (1 - 0.6) \cdot (1 - 0.2) \cdot (1 - 0.5)} \\
P &= 0.0857
\end{align*}
Dle tohoto výpoètu se tedy pravdìpodobnost, ¾e vìta \textit{'Aaaa, my stomach hurts.'} je spam, rovná 0,0857.
Co¾ znamená, ¾e vìta je rozhodnì ham.

\chapter{SVM klasifikátor} \label{SVM}
Klasifikaèní metoda pomocí SVM (support vector machines) je spoleènì s vý¹e popsaným Bayesovským klasifikátorem dal¹í klasifikaèní metodou, kterou sev této práci zabýváme.
Jedná se o metodu uèení s uèitelem, která je v souèasné dobì nejvíce pou¾ívanou metodou v NLP.
Mimo NLP má v¹ak také velkou ¹kálu vyu¾ití, a» u¾ pro klasifikaèní úèely, nebo tøeba napøíklad pro úèely regresní analýzy (pro pøedvídání chování finanèních trhù, atd.).

V této práci nebude mo¾né popsat problematiku klasifikátoru support vector machines do úplných detailù, nicménì zde budou popsány základní principy jeho fungování v rozsahu potøebném pro pozdìj¹í implementaci této metody.
Pro podrobnìj¹í popis SVM doporuèuji knihu Vladimira N. Vapnika z roku 1998 s názvem \uv{Statistical Learning Theory} \cite{Vapnik1998} a èlánek \uv{Support-Vector Networks} \cite{Vapnik1995}.

\section{Obecné}
SVM klasifikátor je metoda dosahující velmi dobrých výsledkù ve velkém mno¾ství odvìtví a aplikací.
Poprvé byl tento algoritmus pøedstaven v roce 1995 ve èlánku Vladimira N. Vapnika a kolektivu s názvem \uv{Support-Vector Networks} \cite{Vapnik1995}.
Metoda SVM byla navrhnuta jako klasifikaèní algoritmus.
Aby bylo mo¾né metodu SVM vyu¾ít pro klasifikaci, musíme ji nejprve nauèit pomocí urèité sady dat, která je rozdìlena na trénovací a testovací data.
Ka¾dý jeden záznam v trénovacích a testovacích datech má definovanou jistou hodnotu (label), která urèuje, do které tøídy daný záznam spadá.
Ka¾dý záznam se skládá z nìkolika tokenù, pomocí kterých se SVM uèí a následnì klasifikuje.
Napøíklad klasifikujeme-li pomocí SVM text tìmito tokeny budou mimo jiné jednotlivá slova, délka textu, e-mailové adresy, Twitter tagy atd.

Nyní pøejdìme k matematickému popisu problematiky SVM.
Pøedpokládejme urèitý klasifikaèní problém a jistou datovou sadu, ve které je mno¾ství záznamù, které patøí buï do pozitivní (ham), nebo negativní (spam) tøídy. 
Trénovací sada $X$ pro tento problém obsahuje $l$ záznamù.
Jeden záznam v této sadì je tedy definován jako vektor $x_i \in \mathbb{R}^n$ kde $1 \leq i \leq l$.
Ka¾dý tento vektor $x_i$ se skládá z mno¾iny pøíznakù $[x_1, x_2,\ldots,x_n] \in x_i$, kde $x_1,\ldots,x_n$ jsou jednotlivé atributy daného záznamu.
Label $y$ ka¾dého záznamu z dat je definován jako $y \in \{1,-1\}$ v závislosti na tom, do které tøídy daný záznam spadá.
Trénovací sadu dat lze tedy zapsat jako:

\begin{equation}
  X = \{(x_i,y_i)\}_{i=1}^{l}
\end{equation}
\begin{equation}
  x \in \mathbb{R}^{n}, x_i = [x_1, x_2, \ldots, x_n], i \in 1, \ldots |X|
\end{equation}
\begin{equation}
  y \in \{1,-1\}
\end{equation}

Cílem SVM klasifikátoru je najít rozhodovací hranici, která rozdìluje pøíznaky v trénovacích datech tak, aby rozdìlení odpovídalo jednotlivým tøídám a souèasnì se sna¾í o maximalizaci vzdálenosti v¹ech bodù od rozhodovací hranice. 
Rozhodovací hranice je definována jako \emph{hyperplocha (nadrovina)}, co¾ je prostor s $n$ dimenzemi, který dìlí prostor s $n+1$ dimenzemi do dvou podprostorù.
Libovolnou hyperplochu lze definovat jako \emph{diskriminaèní funkci (discriminant function)}\footnote{diskriminaèní funkce je funkce, která optimálnì dìlí prostor pøíznakù. \cite{Seong-Wook}} v následující formì:

\begin{equation}
  f(x) = w^T x + b
\end{equation}


$b$ je takzvaný \emph{bias} a $w^T x$ definuje skalární souèin mezi váhovým vektorem $w$ a vektorem pøíznakù $x$.
Tento skalární souèin je definován jako:

\begin{equation}
  w^T x = \sum_{j=1}^{n}w_j x_j
\end{equation}

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=10cm,keepaspectratio]{fig/hyperplanes}
      \caption{Nìkolik hyperploch v prostoru $R^2$ s vektorem $ w^T x$} 
      \label{SVM-THEORY-HYPERPLANES}
    \end{center}
\end{figure}


\section{Nelineární SVM a jádra}
Jeliko¾ je u základního klasifikátoru SVM hyperplocha dìlící prostor pøíznakù lineární, pova¾ujeme tento klasifikátor za lineární.
Proto¾e je lineární dìlicí hyperplocha základního SVM klasifikátoru omezující, je mo¾né pou¾it nelineární dìlící hyperplochu, její¾ pomocí mù¾e klasifikátor dosahovat daleko lep¹ích výsledkù.
Problémem takto nelineárních klasifikátorù ale je zvy¹ování komplexicity výpoètù pøi uèení vìt¹ích poètù pøíznakù (vysoká dimenzionalita dat).
Pro vyøe¹ení vý¹e zmínìného problému byly klasické lineární SVM klasifikátory roz¹íøeny o podporu takzvaných \emph{jaderných funkcí (kernel functions)} za pomocí nich¾ jsou lineární klasifikátory schopny vytvoøit nelineární dìlící hyperplochy.
Pro vysvìtlení zpùsobu, jakým se této nelinearity dosáhne pøedpokládejme klasický lineární klasifikátor.

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=13cm,keepaspectratio]{fig/XtoFTransformation}
      \caption{Transformace vstupního prostoru $X$ do prostoru pøíznakù $F$ pomocí mapovací funkce $\varphi(x)$. $f(x)$ je znázornìní diskriminaèní funkce} 
      \label{SVM-KERNEL-TRANSFORMACE}
    \end{center}
\end{figure}

Nyní pøevedeme trénovací sadu $X$, také známou jako \emph{vstupní prostor (input space)} do vysoce dimenzionálního prostoru pøíznakù $F$ za pomocí nelineární mapovací funkce $\varphi : X \rightarrow F$ viz. obrázek \ref{SVM-KERNEL-TRANSFORMACE}.
Pro prostor pøíznakù tedy budeme mít následující diskriminaèní funkci:

\begin{equation}
  f(x) = w^T \varphi(x) + b
\end{equation}

Jeliko¾ je pøi výpoètu diskriminaèní funkce $f(x)$ tøeba vypoèíst mapovací funkci $\varphi(x)$, velmi výraznì roste nároènost jejího výpoètu s poètem dimenzí vstupù.
Mìjme kupøíkladu následující mapovací funkci:

\begin{equation}
  \varphi(x) = (x^2_1, \sqrt{2}x_1 x_2, x^2_2)^T
\end{equation}

Z této mapovací funkce jsme nyní schopni spoèítat diskriminaèní funkci v prostoru pøíznakù:

\begin{equation}
  f(x) = w_1 x^2_1 + \sqrt{2}w_2 x_1 x_2 + w_1 x^2_2 + b
\end{equation}

Je zcela zøejmé, ¾e tento zpùsob výpoètu je neúnosný.
Museli bychom toti¾ pro výpoèet transformovat celý vstupní prostor do prostoru pøíznakù, co¾ by kvadraticky zvý¹ilo èasovou a pamì»ovou nároènost klasifikátoru.
Tento zpùsob tedy není vhodný.
Existuje v¹ak øe¹ení, kterým lze spoèítat diskriminaèní funkci $f(x)$, ani¾ bychom museli znát, chápat a poèítat mapovací funkci do prostoru $F$.
Pro toto øe¹ení je tøeba si vyjádøit váhový vektor $w$ jako lineární kombinaci jednotlivých záznamù z tréninkové sady.

\begin{equation}
  w = \sum^{n}_{i=1}\alpha_i x_i
\end{equation}

Z toho plyne, ¾e diskriminaèní funkce vstupního prostoru $X$ je:

\begin{equation}
  f(x) = \sum^{n}_{i=1}\alpha_i x^T_i \cdot x + b
\end{equation}

a diskriminaèní funkce prostoru pøíznakù $F$ je:

\begin{equation}
  f(x) = \sum^{n}_{i=1}\alpha_i \varphi(x_i)^T \varphi(x) + b
\end{equation}

Existence tìchto dvou reprezentací diskriminaèní funkce pro vstupní prostor $X$ a pro prostor pøíznakù vzhledem k promìnné $\alpha_i$ bývá nazývána takzvanou \emph{duální reprezentací (dual representation)}. 

Kartézský souèin $\varphi(x_i)^T \varphi(x)$, kde $x_i, x \in X$ tedy pøedstavuje takzvanou \emph{jadernou funkci (kernel function)}.
Jaderná funkce je tedy definována jako:

\begin{equation}
  k(x,z) = \varphi(x_i)^T \varphi(x)
\end{equation}

Jeliko¾ jadernou funkci lze vypoèítat pouze pomocí pøíznakù ve vstupním prostoru, je mo¾né vypoèítat diskriminaèní funkci, ani¾ bychom znali potøebnou mapovací funkci.
Díky této vlastnosti jaderných funkcí nemusíme transformovat celý vstupní prostor do nového prostoru pøíznakù $F$, ale spoèítáme pouze jadernou funkci pro jednotlivé pøíznaky.
Tudí¾ nám poèet dimenzí prostoru $F$ neovlivní komplexnost výpoètu.
Vý¹e zmínìná operace, kdy vyu¾ijeme pouze kartézský souèin bodù v daném prostoru, bývá èasto v literatuøe nazývána jako takzvaný \emph{Kernel trick}.
Pro výpoèet diskriminaèní funkce $f(x) = w^T \varphi(x) + b$ tedy pou¾ijeme jadernou funkci $k(x,z) = \varphi(x)^T \varphi(z)$ a dostaneme $f(x) = k(x,z) + b$.

Nyní si nejprve si znázorníme na pøíkladu nároènost pøi pøevádìní v¹ech souøadnic do prostoru $F$.
Pøedpokládejme prostor $X \in \mathbb{R}^2$ (tzn. $x = (x_1, x_2)$) dále pak polynomiální mapovací funkci druhého øádu

\begin{equation}
\varphi(x) = (1, x_1, x_2, x^2_1, x^2_2, x_1 x_2)
\end{equation}

která pøevede vektor pøíznakù ze vstupního prostoru $X$ do prostoru $F$.
Abychom nyní pomocí této mapovací funkce vypoèítali jadernou funkci, musíme oba body pøevést do prostoru $F$ a provést skalární souèin v prostoru $F$.
Tudí¾:

\begin{equation}
  K(x, z) = \varphi(x)^T \varphi(z)
\end{equation}
\begin{equation}
  K(x, z) = 1 + x_1 z_1 + x_2 z_2 + x^2_1 z^2_1 + x^2_2 z^2_2 + x_1 z_1 x_2 z_2
\end{equation}

Nyní si pøedstavme, ¾e nemáme dvoudimenzionální prostor, ale velmi vysoce dimenzionální prostor.
Pøevádìní $x$ a $z$ do prostoru $F$ by bylo velmi výpoèetnì nároèné.

Nyní si na pøíkladu pøedvedeme pou¾ití vý¹e zmínìné metody \emph{kernel trick}, která nám umo¾ní vypoèíst jadernou funkci, ani¾ bychom potøebovali spoèítat transformace do prostoru pøíznakù $F$.
Pøedpokládejme jadernou funkci:

\begin{equation}
  K(x, z) = (1 + x^T z)^2
  \label{SVM-KERNEL-2POLYNOMIAL}
\end{equation}

kterou roznásobíme a dostaneme:

\begin{equation}
  K(x, z) = 1 + x^2_1 z^2_1 + x^2_2 z^2_2 + 2 x_1 z_1 + 2 x_2 z_2 + 2 x_1 z_1 x_2 z_2
\end{equation}

Co¾ je skuteènì skalární souèin definovaný v prostoru pøíznakù s mapovací funkcí 

\begin{equation}
\varphi(x) = (1, x^2_1, x^2_2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)
\end{equation}

Tím jsme dokázali, ¾e opravdu není tøeba pøevádìt celý prostor pøíznakù, ale staèí nám vypoèítat jednoduchou funkci $K(x, z) = (1 + x^T z)^2$, a tím dosáhnout daleko jednodu¹eji ký¾eného výsledku.

\subsection{Jádra} \label{SVM-KERNELS}
\label{SVM-THEORY-KERNELS}
Jádra, nebo také jaderné funkce, jsou tedy funkce, pomocí nich¾ je klasifikátor SVM  schopen vypoèítat diskriminaèní rovnici pro optimální rozdìlení prostoru pøíznakù.
V závislosti na tom jaké jádro je pøi trénování klasifikátoru pou¾ito, je klasifikátor lineární nebo nelineární a dosahuje rozdílných výsledkù na stejných datech.

V praxi je pou¾íváno nìkolik rùzných jader, nejèastìji \emph{RBF (radial basis function)}, \emph{polynomiální jaderná funkce} a nìkdy je pou¾ívána i základní \emph{lineární jaderná funkce}.

\subsubsection{Lineární jaderná funkce}
Lineární jaderná funkce je nejzákladnìj¹í ze v¹ech jaderných funkcí.
Poèítá toti¾ skalární souèin z bodù ve vstupním prostoru, tudí¾ je schopna pouze lineární klasifikace.
Lineární jaderná funkce je definována následovnì:

\begin{equation}
  K(x, z) =  x^T z
\end{equation}

\subsubsection{Polynomiální jaderná funkce}
Rovnice \ref{SVM-KERNEL-2POLYNOMIAL} popisuje tedy polynomiální jadernou funkci druhého øádu, nicménì v praxi jsou èasto pou¾ívány polynomiální jaderné funkce vy¹¹ích øádù $Q$, obecnì definované jako:

\begin{equation}
  K(x, z) = (x^T z + c)^Q
\end{equation}
kde $c \geq 0$ urèuje vliv vy¹¹ích a ni¾¹ích øádù polynomu.

Polynomiální jaderné funkce jsou v NLP pomìrnì hodnì oblíbené \cite{ACLShort}.
Pøi pou¾ití jaderné funkce vy¹¹ích øádù, ale mù¾e nastat takzvaný overfitting, kdy se SVM klasifikátor nauèí na velmi specifické pøíznaky, které nejsou pro korektní klasifikaci smìrodatné, co¾ zpùsobí markantní zhor¹ení schopností SVM klasifikátoru.
Z tohoto dùvodu je nejèastìji pou¾ívána polynomiální jaderná funkce 2. a 3. øádu, která není schopna tak dokonale kopírovat tvar kolem pøíznakù.

\subsubsection{Radiální jaderná funkce}
Radiální jaderná funkce, èasto oznaèovaná jako \emph{RBF jaderná funkce}, nebo \emph{RBF kernel} je dal¹í velmi oblíbenou jadernou funkcí, zalo¾enou na radiální bázové funkci \emph{RBF (radial basis function)}
Je definována jako:
 
\begin{equation}
  K(x, z) = exp(-\frac{||x-z||^2}{2\gamma^2})
\end{equation}

Obèas je také pou¾ívána jiná forma definice pro RBF jaderné funkce, nicménì základní my¹lenka, ¾e èím vzdálenìj¹í bod v prostoru pøíznakù, tím men¹í má vliv na rozhodování, zùstává ve v¹ech tìchto definicích zachována.
Promìnná $\gamma$, pro kterou musí platit $\gamma \geq 0$, definuje jak moc bude klasifikátor brát v úvahu vzdálenìj¹í body.
Nevhodným výbìrem této promìnné mù¾e podobnì jako v pøípadì polynomiální jaderné funkce dojít k overfittingu, proto je dùle¾ité zvolit její vhodnou hodnotu.

\subsubsection{Dal¹í jádra}
Tato tøi vý¹e zmínìná jádra/jaderné funkce samozøejmì nejsou jediná.
Kdykoliv je mo¾né vytvoøit novou jadernou funkci, která bude popisovat nìjaký jiný prostor.
Taková jaderná funkce, kterou vytvoøíme, v¹ak nemusí popisovat ¾ádný reálný prostor.
Existují tøi pøístupy, pomocí kterých se mù¾eme ujistit, ¾e námi vytvoøená jaderná funkce skuteènì nìjaký reálný prostor popisuje:
\begin{itemize}
\item \emph{Konstrukce} -- jádro zkonstruujeme z transformaèní funkce $\varphi(x)$.
\item \emph{Matematické podmínky jádra (Mercerovy podmínky)} -- pokud jaderná funkce $K(x, z)$ splòuje následující dvì podmínky, pak existuje prostor, který je danou jadernou funkcí popsán:
 \begin{itemize}
  \item $K(x, x^{'})$ musí být symetrická funkce (tzn. $K(x, x^{'}) = K(x^{'}, x)$,$\forall x, x^{'} \in X$)
  \item pro matici:
  	$\begin{bmatrix}
       K(x_1, x_1) & K(x_1, x_2) & \cdots  & K(x_1, x_N)  \\[0.3em]
       K(x_2, x_1) & K(x_2, x_2) & \cdots  & K(x_2, x_N)  \\[0.3em]
       \cdots      & \cdots      & \cdots  & \cdots       \\[0.3em]
       K(x_N, x_1) & K(x_N, x_2) & \cdots  & K(x_N, x_N)  \\[0.3em]
    \end{bmatrix}$ \\
    musí platit, ¾e je pozitivnì semi-definitní pro libovolné $x_1,\dots, x_N \in X$
 \end{itemize} 
\item \emph{Do we even care?} -- tento pøístup je velmi zvlá¹tní, nicménì bývá obèas také pou¾íván.
	Sopèívá ve vytvoøení libovolné jaderné funkce a následné aplikaci v klasifikátoru SVM a pokud klasifikace funguje, nehraje roli, ¾e dané jádro nepopisuje ¾ádný reálný prostor.
	Tento pøístup v¹ak rozhodnì není doporuèitelným.
\end{itemize}

\section{Vytvoøení rozhodovací hranice}
Jak ji¾ bylo vý¹e zmínìno, SVM klasifikátor se pøi uèení sna¾í vytvoøit optimální rozhodovací hranici, tedy hyperplochu, tak, aby dìlila prostor pøíznakù na dva podprostory, kde ka¾dý obsahuje objekty v¾dy pouze z jedné tøídy a v¾dy se sna¾í maximalizovat vzdálenost rozhodovací hranice ode v¹ech bodù v prostoru.


\begin{figure}[H]
    \begin{center}
      \includegraphics[width=10cm,keepaspectratio]{fig/margins}
      \caption{Ukázka mo¾ných marginù, vlevo maximální -- ¹iroký; vpravo tenký} 
      \label{SVM-THEORY-MARGINS}
    \end{center}
\end{figure}

Pøedpokládejme diskriminaèní funkci $f(x)$, která definuje urèitou hyperplochu, lineárnì rozdìlující prostor $X$ na pozitivní a negativní tøídy.
Nejbli¾¹í vektory $x \in X$ k této hyperplo¹e jsou oznaèeny $x_+$ pro pozitivní a $x_-$ pro negativní tøídy.
Definujeme takzvaný \emph{geometrický margin} hyperplochy $f$ na trénovacích datech $X$, co¾ je \emph{maximální ¹íøka (margin)} volného prostoru mezi rozdìlovací hyperplochou $f$ a nejbli¾¹ími body v prostoru $X$ (viz obrázek \ref{SVM-THEORY-MARGINS}).
Margin je definován jako:

\begin{equation}
  m_x(f) = \frac{1}{2} \widehat{w}^T (x_+ - x_-)
\end{equation}

kde vektor $\widehat{w}$ je jednotkový vektor vektoru $w$ a body $x_+$ a $x_-$ jsou od hyperplochy stejnì daleko, co¾ znamená, ¾e existuje nìjaká konstanta $\alpha > 0$, pro kterou platí:

\begin{equation}
  f(x_+) = w^T  x_+ = \alpha
\end{equation}
\begin{equation}
  f(x_-) = w^T  x_- = -\alpha
\end{equation}

Nyní nastavíme hodnotu promìnné $\alpha$ na $\alpha = 1$ a po úpravách získáme následující definici maximálního marginu:

\begin{equation}
  m_X(f) = \frac{1}{||w||}
\end{equation}

Abychom tedy dostali optimální rozdìlovací hranici s maximálním marginem, budeme muset maximalizovat hodnotu maximálního marginu.
Tato operace je ov¹em ekvivalentní s minimalizací $\frac{1}{2}||w||^2$.
Nalezení specifické diskriminaèní funkce s maximálním geometrickým marginem je tedy ekvivalentní s následujícím optimalizaèním problémem:

\begin{equation}
\begin{aligned}
& \underset{w,b}{\text{minimalizace}}
& & \frac{1}{2}||w||^2 \\
& \text{kde}
& & y_n(w^T x_i + b) \geq 1, & n = 1,\ldots,l
\end{aligned}
\end{equation}

Tato optimalizace pøedpokládá, ¾e trénovací mno¾ina $X$ je lineárnì separovatelná.
Podmínka $y_n(w^T x_n + b) \geq 1$ potom zaji¹»uje, ¾e diskriminaèní funkce oklasifikuje v¹echna data v trénovací mno¾inì korektnì.

Nicménì mù¾e nastat problém, ¾e trénovací mno¾ina $X$ nemusí být lineárnì separovatelná.
Aby byl klasifikátor schopný nauèit se i na takové trénovací mno¾inì, musíme jej upravit tak, aby mohl nekorektnì oklasifikovat nìjaké záznamy z trénovací mno¾iny a tím, i za cenu chyb, diskriminaèní funkci najít.
Toto opatøení mù¾e nìkdy pomoci nalézt ¹ir¹í margin, a tím zlep¹it výsledky klasifikátoru oproti pøedchozímu, u¾¹ímu marginu.
Tuto úpravu udìláme za pomocí takzvané \emph{chybové promìnné (slack variable)} $\xi_i$, kterou odeèteme od pravé strany optimalizaèní podmínky -- tzn. umo¾níme chybu.
Takto upravený problém nyní vypadá následovnì:

\begin{equation}
\begin{aligned}
& \underset{w,b}{\text{minimalizace}}
& & \frac{1}{2}||w||^2 \\
& \text{kde}
& & y_n(w^T x_i + b) \geq 1 - \xi_i, & n = 1,\ldots,l
\end{aligned}
\end{equation}

Pokud tedy platí, ¾e $y_i(w^T x_i + b) < 1$ respektive $\xi_i > 1$, pak je záznam $x_i \in X$ ¹patnì klasifikován.
Pokud ale platí, ¾e $0 \leq \xi_i \leq 1$, pak záznam $x_i \in X$ le¾í uvnitø marginu.
Záznam $x_i$ je tedy klasifikován korektnì, jestli¾e platí, ¾e $\xi_i \leq 0$.
Z tohoto vyplývá, ¾e suma v¹ech chybových promìnných reprezentuje míru chybovosti:

\begin{equation}
  \xi(X) = \sum^n_{i=1}\xi_i
\end{equation}
\begin{equation}
  X = \{(x_i,y_i)\}^l_{i=1}
\end{equation}

Abychom byli schopni minimalizovat míru chybovosti (penalizaci za ¹patnou klasifikaci a marginové chyby) vzhledem k maximalizaci marginu, zavedeme konstantu $C > 0$, kterou budeme míru chybovosti pøenásobovat.
Tato konstanta se nazývá \emph{konstanta soft-margin (soft-margin constant)}.
Na¹e optimalizaèní úloha tedy s touto soft-margin konstantou vypadá následovnì:

\begin{equation}
\label{SVM_C}
\begin{aligned}
& \underset{w,b}{\text{minimalizace}}
& & \frac{1}{2}||w||^2 + C \sum^{n}_{i=1}\xi_i \\
& \text{kde}
& & y_n(w^T x_i + b) \geq 1 - \xi_i, & n = 1,\ldots,l
\end{aligned}
\end{equation}

Nyní pro vyøe¹ení tohoto optimalizaèního problému pou¾ijeme metodu Lagrangeových multiplikátorù a získáme optimalizaèní problém:

\begin{equation}
\begin{aligned}
& \underset{\alpha}{\text{maximalizace}}
& & \sum^{n}_{i=1}\alpha_i - \frac{1}{2}\sum^{n}_{i=1}\sum^{n}_{j=1}y_iy_j\alpha_i\alpha_jx^T_i x_j \\
& \text{kde}
& & \sum^n_{i=0} y_i\alpha_i = 0, & 0 \leq \alpha_i \leq C
\end{aligned}
\end{equation}


Skalární souèin $x^T_i x_j$ v maximalizaèní rovnici mù¾eme nahradit libovolnou jadernou funkcí a dosáhnout tím nelineární transformace, a tedy i ¹irokého marginu ve vysoce dimenzionálních prostorech pøíznakù. (viz \ref{SVM-THEORY-KERNELS}).

Díky vyu¾ití Lagrangeovy funkce má výsledek øe¹ení optimalizaèního problému zajímavé vlastnosti.
Bylo napøíklad dokázáno, ¾e takto získané øe¹ení je v¾dy globální díky tomu, ¾e formulace problému je konvexní \cite{Burges1988}.

Zajímavou vlastností support vector machines je, ¾e ne v¹echny vektory z tréninkové mno¾iny se podílejí na výsledném øe¹ení.
Pou¾ijme rovnici pro výpoèet váhového vektoru $w$ získaného derivací pøi øe¹ení Lagrangeovy funkce, a to:

\begin{equation}
  w = \sum^{n}_{i=1}y_i \alpha_i x_i
\end{equation}

V¹echny vektory $x_i$, pro které tedy platí, ¾e  $\alpha_i > 0$, jsou vektory, které le¾í na pomezí marginu, uvnitø marginu, nebo jsou ¹patnì klasifikovány.
Tyto vektory jsou nazývány \emph{support vektory (support vectors)}.
Vektory, které v¹ak mají hodnotu $\alpha_i \leq 0$ nejsou vùbec do øe¹ení zahrnuty. 
Tudí¾ tyto vektory by mohly být zcela odstranìny z tréninkové sady a nemìlo by to ¾ádný vliv na výsledné øe¹ení.
Díky této vlastnosti jsou SVM ménì náchylné k overfittingu a také klasifikaèní model, který tyto vektory tvoøí, je díky tomu velmi malý a rychlý.


\chapter{Tokenizace a výbìr vhodných pøíznakù} \label{TOKENIZACE}
V této kapitole se budeme zabývat tvorbou tokenù, které klasifikátory pou¾ívají pro trénovaní a klasifikaci textu.

Nejprve si ale definujme, co to tokenizace je:

\begin{definition}
Tokenizace je proces rozdìlování vstupního textu do vhodných tokenù (nejèastìji slov, frází, symbolù a jiných pøíznakù).
Seznam takto vytvoøených tokenù se potom pou¾ívá pro dal¹í zpracování (v pøípadì této práce jako vstup klasifikátorù).
\end{definition}

Pro potøeby praktické èásti této práce rozdìlujeme tokeny na dvì skupiny:
\begin{itemize}
	\item Speciální pøíznaky -- v textu se mohou nacházet skryté znaky, které vìt¹inou nejsou klasifikátory bìhem klasifikace schopny zachytit a které mohou výrazným podílem napomoct pøesnosti klasifikace.
Ka¾dý druh klasifikovaného textu ale mù¾e mít citlivost k rùzným znakùm zcela jinou.
Proto je nutné dbát na to, jak vybrat optimální znaky pro daný vstupní text a pou¾itou klasifikaèní metodu.
	\item Textové tokeny -- mimo vý¹e zmínìných speciálních pøíznakù jsou v¹ak v textu pøíznaky jednotlivých slov, které jsou pro klasifikaci nejdùle¾itìj¹ími nositeli informace.
Proto také velmi zále¾í na výbìru, popøípadì úpravì, vstupních slov a zpùsobu jejich zpracování.
\end{itemize}

Nyní pøistoupíme k detailnìj¹ímu popisu vý¹e popsaných skupin tokenù tak, jak je k nim pøistupováno v praktické èásti této práce.
Následnì popí¹eme práci naivního Bayesovského klasifikátoru a klasifikátoru SVM s vytvoøenými tokeny.

\section{Speciální pøíznaky v textu} \label{TOKENIZACE-SPEC}
V této èásti práce budeme popisovat jednotlivé druhy speciálních pøíznakù, které jsou z textu extrahovány a které jsou následnì pøedávány klasifikátorùm, je¾ s nimi dále pracují.
Tyto speciální pøíznaky jsou rovnì¾ implementovány v praktické èásti této práce.

Speciální pøíznaky v textu jsou mnohdy v textu skryté a klasickými pøístupy pro tokenizaci textu (viz \ref{Textove_tokeny}) nejsme schopni tato data z textu získat.
Tyto speciální pøíznaky jsou nositeli dùle¾ité informace proto, ¾e mohou z textu napøíklad urèovat náladu pisatele, zobecòovat informace obsa¾né v textu atd., a tím umo¾nit pøesnìj¹í klasifikaci.
Vzhledem k tomu, ¾e takovýchto speciálních pøíznakù je jistì mo¾né definovat velké mno¾ství, je nereálné je popsat a implementovat v¹echny.
V praktické èásti této bylo zvoleno nìkolik typù tìchto speciálních pøíznakù, které byly implementovány a budou zde tedy popsány.

V následujícím popisu pøíznaky dìlíme do skupin podle typu informace, kterou se pomocí nich sna¾íme z textu z
ískat.
Tyto skupiny jsou potom dìleny na typy pøíznakù ze skupiny, které popisují jednotlivé varianty implementace této varianty, které jsouv praktické èásti implementovány.

\subsection{URL}
Skupina pøíznakù s názvem URL je skupinou, hledající v klasifikovaném textu internetové odkazy ("http://adresa.com", "ftp://adresa.cz", "adresa.cz/odkaz.html", atd.) a sna¾ící se z tìchto odkazù vytvoøit tokeny co nejvhodnìj¹í pro klasifikaci.
 
Skupina URL má v praktické èásti práce následující mo¾né varianty speciálních pøíznakù:

\begin{itemize}
  \item \emph{Celé URL} -- celé URL nalezené v textu je bráno jako pøíznak.
  \item \emph{Doména URL} -- doména daného URL je brána jako pøíznak (napøíklad: \emph{"http://healthland.time.com/2013/04/02/bird-flu-is-back-in-china-but-this-time-its-h7n9/"} $\longrightarrow$ \emph{"time.com"}).
  \item \emph{Existence URL ANO/NE} -- existuje-li v textu URL, pak se vytvoøí pro daný text pøíznak definující, ¾e se v textu URL vyskytovalo.
  Pokud se v textu ale URL nenachází vytvoøí se namísto pøíznaku \emph{ANO} pøíznak \emph{NE}, definující neexistenci URL v textu.
  \item \emph{Existence URL ANO} -- tento typ pøíznaku URL je variací na pøedchozí pøíznak \emph{Existence URL ANO/NE}.
  Pøíznak se vytvoøí pouze tehdy, pokud se v textu URL nachází.
  Jestli¾e se v textu URL nenachází, pøíznak vytvoøen není. 
  \item \emph{Nepou¾ívat URL} -- tato varianta zcela zru¹í pou¾ívání URL jako pøíznaku pro klasifikaci.
\end{itemize}

\subsection{E-mailové adresy}
Skupina pøíznakù E-mailové adresy sdru¾uje dohromady varianty pøíznakù, pracující s nalezenými e-mailovými adresami v textu.

Mo¾né varianty pøíznakù v této skupinì jsou:
\begin{itemize}
  \item \emph{Celý e-mail} -- celý e-mail nalezený v textu je brán jako pøíznak.
  \item \emph{Existence e-mailu ANO/NE} -- pøíznak definuje, zda se v textu nachází nìjaká e-mailová adresa.
  Pokud ano, je vytvoøen pøíznak \emph{ANO} a pokud ne, je vytvoøen pøíznak \emph{NE}.
  \item \emph{Existence emailu ANO} -- nachází-li se v textu e-mailová adresa, vytvoøí se pøíznak definující existenci této e-mailové adresy.
  Na rozdíl od pøedchozí varianty \emph{Existence e-mailu ANO/NE} se nevytváøí pøíznak NE v pøípadì nenalezení e-mailové adresy v textu.
  \item \emph{Nepou¾ívat e-mailové adresy} -- tato varianta zcela zru¹í pou¾ívání pøíznakù e-ailových adres jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Emotikony}
Dal¹í skupinou pøíznakù, které v této práci rozpoznáváme a pøedáváme klasifikátorùm, jsou pøíznaky emotikonù (smajlíkù).

Tuto skupinu pøíznakù pro potøeby na¹í implementace dìlíme na následující typy:
\begin{itemize}
  \item \emph{Existence jednotlivých emotikonù} -- pro ka¾dý nalezený emotikon je vytvoøen pøíznak.
  \item \emph{Existence emotikonù obecnì ANO/NE} -- jestli¾e se v textu nachází nìjaké emotikony, je vytvoøen pøíznak \emph{ANO}, jestli¾e se nenacházejí, je vytvoøen pøíznak \emph{NE}.
  \item \emph{Existence emotikonù obecnì ANO} -- obdobnì jako \emph{Existence emotikonù obecnì ANO/NE}, pouze s tím rozdílem, ¾e jestli¾e se v textu nenacházejí ¾ádné emotikony, ¾ádný pøíznak se nevytvoøí.
  \item \emph{Nálada emotikonù} -- v textu jsou vyhledávány emotikony a zji¹»uje se jejich nálada. 
  Typy emotikonù jsou rozdìleny do tøech tøíd -- smutné(":-(", ":'(", \ldots), veselé(":-)", ":-P", \ldots ) a ostatní ("o.O", ":-O", \ldots).
  Jestli¾e je v textu nalezen emotikon z vý¹e zmínìných tøíd, pak je vytvoøen odpovídající pøíznak \emph{SAD}, \emph{HAPPY}, nebo \emph{OTHER}.
  Nachází-li se v textu více typù emotikonù, je samozøejmì vytvoøeno více odpovídajících pøíznakù (tzn. napøíklad pokud se v textu nachází následující mno¾ina emotikonù: ":-)", ":-P", ":(", jsou pro daný text vytvoøeny dva pøíznaky \emph{HAPPY} a \emph{SAD}).
  \item \emph{Nepou¾ívat emotikony} -- tato varianta zcela zru¹í pou¾ívání emotikonù jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Tagy}
Takzvané tagy jsou textové znaèky (klíèová slova), které autoøi pøipisují k textu, aby daný text pøiøadili k nìjakému tématu a umo¾nili tak ostatním u¾ivatelùm snáze nalézt pøíspìvky k jimi hledanému tématu.
Tyto tagy bývají ve tvaru \#TAG (napøíklad "\#influenza", "\#sick", \ldots).
Pro potøeby této práce jsme do této skupiny zaøadili také tagy u¾ivatelù, pou¾ívané ve formátu @JMÉNO\_U®IVATELE, které odkazují na zmínìného u¾ivatele.

V této práci rozli¹ujeme v pøípadì této skupiny následující typy:
\begin{itemize}
  \item \emph{Celý tag} -- celý tag nalezený v textu je pou¾it jako pøíznak pro klasifikaci.
  \item \emph{Existence tagù ANO/NE} -- jestli¾e se v textu nacházejí nìjaké tagy, je vytvoøen pøíznak \emph{ANO}, jestli¾e se nenacházejí, je vytvoøen pøíznak \emph{NE}.
  \item \emph{Existence tagù ANO} -- obdobnì jako v pøípadì typu \emph{Existence tagù ANO/NE}, ale pokud ¾ádný tag nebyl nalezen, pøíznak NE není vytvoøen.
  \item \emph{Nepou¾ívat tagy} -- tato varianta zcela zru¹í pou¾ívání tagù jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Vìty}
V pøípadì této skupiny se jedná pouze o jediný typ pøíznaku.
V celém vstupním textu se spoèítá poèet napsaných vìt, ketrý je následnì pou¾it jako pøíznak pro klasifikaci.
Samozøejmì, stejnì jako ve vý¹e popsaných skupinách pøíznakù, i tento pøíznak je mo¾né zcela zru¹it a nepou¾ívat.

\subsection{Èasy}
V textu se velmi èasto nacházejí rùzné èasové údaje, které by mohly velkou mìrou pøispìt lep¹ím výsledkùm na¹eho klasifikátoru.
Proto byla vytvoøena skupina pøíznakù nazvaná \emph{Èasy}, která vytváøí pøíznaky z nalezených èasových informací v textu.

Obsahuje tyto typy:
\begin{itemize}
  \item \emph{Celý èas} -- jako pøíznaky pro klasifikaci se pou¾ijí v¹echny èasové informace nalezené v textu.
  \item \emph{Èas ve 24h formátu} -- èasové informace nalezené v textu jsou konvertovány do 24 hodinového formátu a následnì pou¾ity jako pøíznaky.
  \item \emph{Pouze hodiny ve 24h formátu} -- èasové informace jsou stejnì jako v pøípadì typu \emph{Èas ve 24h formátu} pøevedeny do 24h formátu, ale jako pøíznaky jsou pou¾ity pouze informace o hodinách.
  \item \emph{Nepou¾ívat èasy} -- tato varianta zcela zru¹í pou¾ívání èasù jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Data}
Podobnì jako skupina \emph{Èasy}, funguje i skupina \emph{Data}, která vytváøí pøíznaky z údajù o datu.
V následujícím textu budeme písmenem D oznaèovat dny, písmenem M mìsíce a písmenem Y roky (tzn. datum 15.2.1998 je ve formátu DMY).

Obsahuje tyto mo¾né typy pøíznakù:

\begin{itemize}
  \item \emph{Celé datum} -- jako pøíznak je v pøípadì tohoto typu pøíznakù pou¾ito datum nalezené v textu.
  \item \emph{Datum ve formátu DMY} --v pøípadì tohoto typu je datum nalezené v textu pøevedeno do formátu DMY a v tomto formátu je pou¾ito jako pøíznak (tzn. DMY).
  \item \emph{Datum ve formátu MY} -- podobnì jako v pøedchozím typu je datum pøevedeno do formátu DMY, ale jako pøíznak jsou pou¾ity pouze informace o mìsíci a roku (tzn. MY).
  \item \emph{Datum ve formátu Y} -- opìt podobné jako \emph{Datum ve formátu DMY} ale pro vytvoøení pøíznaku je pou¾it pouze rok z data nalezeného v textu (tzn. Y).
  \item \emph{Nepou¾ívat data} -- tato varianta zcela zru¹í pou¾ívání udajù o datu jako pøíznakù pro klasifikaci.
\end{itemize}

\section{Textové tokeny} \label{Textove_tokeny}
Jak ji¾ bylo zmínìno v pøedchozím textu, 

Pro tokenizaci obecnì existuje velmi mnoho pøístupù a zpùsobù zde v¹ak popí¹eme pouze tøi, jejich¾ výsledky následnì v kapitole \ref{TESTY} porovnáne.


\subsubsection{Základní pøístup k tokenizaci}
Základní tokenizace dìlí vstupní vìtu pouze na jednotlivá slova, která jsou posléze pou¾ita jako tokeny. 

To znamená:

Mìjme vstupní text \textit{'Damned headache. I have to sleep.'}.
Takovýto text se rozdìlí na slova, tzn. vznikne následující seznam slov:

$['Damned', 'headache', 'I', 'have', 'to', 'sleep']$.

Takto vytvoøená slova jsou tedy tokeny, s nimi¾ se klasifikátor uèí, nebo které klasifikuje.

\subsubsection{Základní pøístup k tokenizaci se stematizací}
Tento pøístup je velmi podobný pøedchozímu.
Li¹í se od nìj pouze tím, ¾e slova, která jsou vytvoøena rozdìlením pomocí stematizaèního algoritmu pøevede na koøeny (=stemateizace).
Tímto krokem se sní¾í poèet slov ve slovníku klasifikátoru.
To zpùsobí, ¾e u Bayesovského klasifikátoru dojde k výraznému zmen¹ení velikosti slovníku vytvoøeného pøi uèení a u klasifikátoru SVM se sní¾í poèet dimenzí pøíznakù a zjednodu¹í se tak nalezení optimální nadroviny.

To znamená:

Mìjme vstupní text \textit{'Damned headache. I have to sleep.'}.
Takovýto text se rozdìlí na slova, tzn. vznikne následující seznam slov:

$['Damned', 'headache', 'I', 'have', 'to', 'sleep']$.

Nyní v¹echna tato slova pøevedeme na tvar koøene (stematizace), èím¾ získáme seznam následujících slov:

$['Damn', 'headache', 'I', 'have', 'to', 'sleep']$.

Takto vytvoøené koøeny slov se ji¾ pou¾ijí jako tokeny pro uèení klasifikátorù, a nebo mù¾e být vstupní text rozdìlený na tato slova klasifikátory klasifikován do daných tøíd.


\subsubsection{Alternativní pøístup k tokenizaci}
Pøi vyu¾ití základního pøístupu k tokenizaci je problémem skuteènost, ¾e tokeny jsou zcela samostatné bloky, které se pou¾ité klasifikátor (a» u¾ Bayesovský klasifikátor, nebo klasifikátor SVM), uèí ani¾ by byly schopny z tìchto tokenù (slov) urèit kontext.
V¾dy» jak bylo ji¾ bylo vý¹e zmínìno, Bayesovský klasifikátor je oznaèován jako naivní právì proto, ¾e nebere v potaz závislosti jednotlivých tokenù na sobì (co¾ vychází z odvození rovnice \ref{equation_used} pou¾ívané pro klasifikaci -- viz. \ref{SF-BAYES-MAT-bayes_theorem-deriv}).
Proto jsem se sna¾il tuto nevýhodu kompenzovat tím, ¾e jsem pou¾il alternativní pøístup k tokenizaci zachovávající nejbli¾¹í kontext tokenù.
Ten na rozdíl od základní tokenizace nevytváøí tokeny pouze z jednotlivých slov, ale také z $N$ za sebou jdoucích slov, èím¾ bere v potaz jejich kontext ve vìtì.
Nevýhodou tohoto pøístupu je, ¾e pro velké uèící mno¾iny výraznì roste velikost slovníku u Bayesovského klasifikátoru a u SVM klasifikátoru se zvìt¹uje poèet dimenzí pøíznakù, co¾ vede k obtí¾nìj¹ímu nalezení optimální dìlící nadroviny.
Tato vlastnost ale opìt mù¾e být alespoò èásteènì kompenzována stematizací jednotlivých slov, a tím i sní¾ením celkového poètu rùzných tokenù.
Uka¾me si alternativní pøístup k tokenizaci na pøíkladu:

Mìjme opìt zprávu \textit{'Damned headache. I have to sleep.'}.
Prvním krokem pøi zpracování této zprávy je její rozdìlení na vìty, proto¾e kontext za sebou jdoucích slov funguje v¾dy v rámci jedné vìty.
Pro rozdìlení vstupního textu na jednotlivé vìty je s úspìchem pou¾ita knihovna NLTK (viz. \ref{IMP-NLTK}).
Vstupní text je nyní v tomto tvaru
$$['Damned\ headache.', 'I\ have\ to\ sleep']$$\\
Na rozdíl od základního pøístupu, který vytváøí tokeny pouze z jednotlivých slov, vytváøíme tokeny z libovolných za sebou jdoucích $N$-tic o maximální délce $N$ v¾dy tak, ¾e $N$-tice se skládají v¾dy jen ze slov dané vìty.
Hodnota $N$ je nastavena na optimální hodnotu urèenou experimentálnì za pomocí testù tak, aby stále je¹tì vylep¹ovala výslednou pøesnost klasifikátoru a pøitom aby velikost slovníku nepøesáhla pøijatelnou mez.
Pøed tím, ne¾ se samozøejmì ze slov vytvoøí tokeny, pøevedou se v¹echna slova na koøeny.
Tokeny pro vstupní text a $N = 3$ budou tedy pro první vìtu vypadat následovnì:
$$['Damn', 'headache', ('Damn', 'headache')]$$
a pro druhou vìtu:
$$['I', 'have', 'to', 'sleep', ['I', 'have'], ['have', 'to'], ['to', 'sleep'], ['I', 'have', 'to'], ['have', 'to', 'sleep']]$$
S tìmito tokeny pak klasifikátor pracuje stejnì jako s jakýmikoliv jinými tokeny.

\section{Pøíznaky a tokeny v Bayesovském klasifikátoru}

Bayesovský klasifikátor bere postupnì v¹echny tokeny a pøíznaky z trénovací mno¾iny a v závisosti na 
labelech trénovacích dat poèítá pravdìpodobnosti s jakými dané tokeny a pøíznaky nále¾ejí do tøídy spamu.
Takto vypoèítané pravdìpodobnosti pro jednotlivé tokeny a pøíznaky si ukládá do svého slovníku, pomocí nìho¾ následnì klasifikuje nové vstupní texty.

V Bayesovském klasifikátoru je velmi dùle¾ité vybrat pro danou trénovací mno¾inu dat vhodné pøíznaky, které se z textu budou extrahovat a pou¾ívat pro klasifikaci.
Proto je v této práci klasifikátor trénován se v¹emi mo¾nými kombinacemi pou¾itých pøíznakù a jsou zvoleny ty pøíznaky, pro které mají výsledky klasifikátoru na testovacích datech s tìmito daty nejvìt¹í korelaci.

\section{Pøíznaky a tokeny v SVM klasifikátoru}

Na rozdíl od Bayesovského klasifikátoru, problém volby vhodných pøíznakù a tokenù u SVM klasifikátoru odpadá, co¾ vychází z definice SVM.
Jeliko¾ si klasifikátor pøi uèení vytváøí váhový vektor, který urèuje míru dùle¾itosti jednotlivých pøíznakù v prostoru pøíznakù pro výsledek klasifikace, je jasné, ¾e klasifikátor doká¾e irelevantní pøíznaky ignorovat a øídit se hlavnì pøíznaky, které na výslednou klasifikaci budou mít nejlep¹í vliv.











\chapter{Vlastní implementace} \label{IMPLEMENTACE}
V této kapitole se budeme zabývat vlastní implementací obou klasifikaèních algoritmù z kapitol \ref{BAYES} a \ref{SVM}.
Kromì pou¾itých knihoven zde popí¹eme také architekturu programu implementovaného v praktické èásti této práce.
Také zde budou pøedstaveny problémy, ke kterým pøi implementaci do¹lo a rovnì¾ bude prezentováno, jakým zpùsobem byly vyøe¹eny.

Celý program je implementován v programovacím jazyce Python 2.7.
Jazyk Python byl zvolen hlavnì z toho dùvodu, ¾e pro nìj existuje mnoho knihoven, které mohly být v této práci pou¾ity a za jejich¾ pomoci se implementace výraznì zjednodu¹ila.
Takto byla vyu¾ita zajména knihovna pro zpracování pøirozené øeèi \texttt{NLTK}, knihovna \texttt{NumPy} (Numerical python), která umo¾òuje v prostøedí jazyka Python velmi jednodu¹e pracovat se slo¾itìj¹í matematikou a knihovna \texttt{CVXOPT} vyu¾itá k implementaci SVM klasifikátoru pro øe¹ení úloh kvadratického programování.

Tyto knihovny budou mimo jiné také podrobnìji popsány dále v této kapitole.

\section{Pou¾ité knihovny} \label{IMPL_LIB}
\subsection{NLTK} \label{IMP-NLTK}
\texttt{NLTK}, neboli Natural Language ToolKit je balík knihoven pro skriptovací jazyk Python 2.7, urèený pro symbolické a statistické zpracovávání pøirozené øeèi.
Nabízí jednoduché rozhraní pro velké mno¾ství rùzných nástrojù pro zpracování textu, ale také velké mno¾ství ukázkových dat, která lze pou¾ít jako testovací a trénovací data pøi vyvíjení softwaru pracujícího s pøirozeným jazykem.
Díky vynikající dokumentaci je \texttt{NLTK} skvìlou knihovnou výraznì zjednodu¹ující implementaci softwaru pracujícího nìjakým zpùsobem s pøirozeným jazykem.

V praktické èásti této práce je knihovna \texttt{NLTK} pou¾ívána jako souèást pøedzpracování vstupního textu urèeného buï k uèení klasifikátorù, nebo ji¾ pøímo pro klasifikaci.
Je pou¾ita k tìmto dvìma operacím s textem:
\begin{enumerate}
 \item \emph{Rozdìlení vìt} -- prvním modulem z této knihovny, který je v této práci pou¾it, je modul \texttt{punkt} (\texttt{nltk/tokenize/punkt}), jen¾ rozdìluje vstupní text na seznam vìt.
 \item \emph{Pøevedení slov na jejich koøeny} -- druhým modulem z této knihovny je modul \texttt{snowball} (\texttt{nltk.stem.snowball}), jeho¾ úkolem je z jednotlivých slov vstupního textu udìlat koøen slov (více viz \ref{Textove_tokeny}).
\end{enumerate}

\subsection{NumPy} \label{IMP-NUMPY}
\texttt{NumPy} je základním balíkem knihoven pro numerické výpoèty v programovacím jazyce Python.
Tento balík knihoven umo¾òuje jednoduchou práci s mnohadimenzionálními poli a dal¹ími objekty od nich odvozenými (masked arrays, matice $\ldots$).
Mimo to v sobì knihobna zahrnuje velké mno¾ství rychlých matematických operací nad poli, zahrnujících diskrétní Fourierovy transformace, tøídìní, základní lineární algebru a mnoho dal¹ích.
Vyu¾ití této knihovny je pro programy øe¹ící vìt¹í mno¾ství slo¾itìj¹ích matematických výpoètù prakticky nutností, nebo» výraznì zrychlí výpoèty a programátorovi zjednodu¹í práci s matematickými daty.
Mnoho dal¹ích knihoven øe¹ících nìjaké rozsáhlej¹í matematické úkony je zalo¾eno právì na knihovnì \texttt{NumPy}.
Mezi tyto dal¹í knihovny patøí i balík knihoven \texttt{CVXOPT} (viz \ref{IMP-CVXOPT}), pou¾itý pro implementaci SVM klasifikátoru v praktické èásti této práce.

\subsection{CVXOPT} \label{IMP-CVXOPT}
\texttt{CVXOPT} je volnì dostupný balík knihoven pro øe¹ení konvexních optimalizaèních problémù (mimo jiné do této skupiny problémù patøí i kvadratické programování, je¾ je pou¾ito pro implementaci SVM klasifikátoru) v programovacím jazyce Python 2.7.
Hlavním úkolem tohoto balíku knihoven je zjednodu¹it vývoj softwaru, jen¾ potøebuje pro svùj bìh øe¹it konvexní optimalizaèní úlohy, ani¾ by bylo potøeba implementovat slo¾ité optimalizaèní algoritmy.


\section{Architektura a implementace programu}
V této èásti kapitoly struènì pøedstavíme architekturu, tedy návrh struktury implementovaného programu a implementaci klasifikaèních algoritmù z popsaných v teoretické èásti této práce.

\subsection{Architektura programu}
\subsubsection{Fyzická architektura}
Fyzická architektura programu pøedstavuje popis fyzického rozdìlení zdrojových souborù programu.
Toto rozdìlení zde bude popsáno, aby se ètenáø orientoval v následujícím textu z implementace programu z praktické èásti této práce.

V koøenovém adresáøi tohoto projektu se nachází spou¹tìcí skript \texttt{control.py}, pomocí nìho¾ je program ovládán a který nabízí ve¹keré operace, které je mo¾né s programem provést.
Výpis nápovìdy k programu bude vypsán v Pøíloze XXXXX. 

Nad koøenovým adresáøem se nachází adresáø \texttt{/src/} obsahující tøi podadresáøe, ka¾dý obsahující jednu logickou èást programu:

\begin{enumerate}
 \item Adresáø \texttt{common} -- v tomto adresáøi se nacházejí ve¹keré operace, které jsou pro oba implementované klasifikátory spoleèné, a to extrakce textových tokenù a tokenù speciálních pøíznakù z textu.
 \item Adresáø \texttt{bayes} -- v tomto adresáøi se nachází kompletní implementace naivního Bayesovského klasifikátoru.
 \item Adresáø \texttt{svm} -- v tomto adresáøi se nachází kompletní implementace SVM klasifikátoru.
\end{enumerate}

Mimo èást se zdrojovými kódy se potom nachází adresáø \texttt{data}, který obsahuje ve¹kerá ruènì anotovaná data a natrénované klasifikaèní modely pro oba klasifikátory, které umo¾òují klasifikátor pou¾ívat bez toho, ani¾ by se musel poka¾dé znovu spou¹tìt trénovací algoritmus na mno¾inì trénovacích dat.

\subsubsection{Architektura tøíd}
V této èásti dokumentu bude popsána architektura tøíd s jejich krátkým popisem, která umo¾ní rychle pochopit architekturu a funkènost programu.

Program, implementovaný jako praktická èást této práce, je, jak ji¾ bylo vý¹e zmínìno (\ref{IMPLEMENTACE}), implementován  v programovacím jazyce Python, jen¾ umo¾òuje vyu¾ití objektovì orientovaného pøístupu k vývoji aplikací.
Díky této vlastnosti Pythonu bylo mo¾né implementovat program objektovì a vyu¾ít tak výhod tohoto pøístupu.

Jak Bayesovský klasifikátor, tak klasifikátor zalo¾ený na support vector machines, jsou v implementaèní èásti této práce vytvoøeny tak, aby bylo mo¾né je pou¾ívat jako modulù programovacího jazyka Python -- zcela nezávisle na sobì.
To znamená, ¾e lze oba klasifikátory importovat jako modul do novì implementované aplikace a pou¾ít pro klasifikaci libovolného vstupního textu.
Této vlastnosti bylo vyu¾ito v projektu M-Eco, v nìm¾ byl Bayesovský klasifikátor pou¾it ve skriptu vkládajícím nová data do databáze, kde filtroval relevantní data od nerelevantní a relevantních data byla následnì vkládána do databáze.
V projektu jsou tedy implementovány dvì základní tøídy \texttt{SVM} a \texttt{BayesianClassifier}.
Obì tyto tøídy obsahují metody pro trénování klasifikátoru z trénovacích dat a metody umo¾òující po pøedchozím nauèení klasifikátoru klasifikovat data do daných tøíd.

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=15cm,keepaspectratio]{fig/Features-diag}
      \caption{Diagram tøíd dìdících z tøídy Features} 
      \label{features-diag}
    \end{center}
\end{figure}

Jeliko¾ jak Bayesovský klasifikátor, tak klasifikátor zalo¾ený na support vector machines pracují se vstupním textem prakticky stejnì, byla vytvoøena tøída \texttt{Entry} a tøídy dìdící z rodièovské tøídy \texttt{Feature} (viz obr. \ref{features-diag}), nacházející se v adresáøi \texttt{common}.
Tyto tøídy pou¾ívají oba klasifikátory pro práci s ve¹kerým vstupním textem, tedy jak pro klasifikaci, tak pro uèení.
Instance tøídy \texttt{Entry} se vytváøí pro ka¾dý text vstupující do klasifikátoru.
Jejím úkolem je text rozdìlit na tokeny tak, aby samotné klasifikátory ji¾ s textem nemusely nijak pracovat, ale aby dostaly pouze seznam textových a speciálních tokenù, je¾ vstupní text obsahoval.
Tato tøída tedy zodpovídá za vyhledávání jednotlivých speciálních tokenù (viz \ref{TOKENIZACE-SPEC}), které potom jako instance tøíd dìdících ze tøídy \texttt{Features} spoleènì s $N$-ticemi jednotlivých textových tokenù (viz \ref{Textove_tokeny}) pøedává klasifikátorùm.

Aby bylo mo¾né oba klasifikátory testovat a posléze i vzájemnì porovnávat, byly pro ka¾dý klasifikátor vytvoøeny tøídy, je¾ mají na starost testování.
Tyto tøídy se nazývají \texttt{SVMTest} pro klasifikátor zalo¾ený na support vector machines a 
\texttt{BayesianTest} pro Bayesovský klasifikátor.
Obì tyto tøídy obsahují metody pro spou¹tìní testù klasifikátorù a jejich následné vyhodnocení.

Jeliko¾ oba implementované klasifikátory vyu¾ívají pro klasifikaci diametrálnì odli¹ných pøístupeù, má ka¾dý klasifikátor potøebu jinak nakládat s tokeny získanými z instancí tøídy \texttt{Entry}.
Proto ka¾dý klasifikátor obsluhuje nìkolik tøíd, je¾ pøi klasifikaci nebo uèení pou¾ívá.

V pøípadì Bayesovského klasifikátoru je takovouto dal¹í pou¾ívanou tøídou pouze jediná tøída s názvem \texttt{WordDictionary}, která se stará o slovník tokenù Bayesovského klasifikátoru.
Tento slovník tokenù obsahuje pravdìpodobnostní hodnoty jednotlivých tokenù nalezených bìhem uèení a pravdìpodobnost jejich nále¾itosti do urèité tøídy. 
Tato tøída také zabezpeèuje ve¹keré operace provádìné s tímto slovníkem tokenù, jako napøíklad jeho ulo¾ení do souboru a opìtovné naètení, díky èemu¾ je mo¾né Bayesovský klasifikátor, stejnì jako klasifikátor zalo¾ený na metodì support vector machines pou¾ívat, ani¾ bychom je museli bezprostøednì pøed klasifikací uèit, co¾ mù¾e být velmi zdlouhavý proces.
Mimo operací exportu a importu slovníku také tato tøída umo¾òuje exportovat daný slovník do souboru v jazyce XML.

Podobnì jako Bayesovský klasifikátor, potøebuje i klasifikátor zalo¾ený na metodì support vector machines tøídu, je¾ pracuje se vstupním textem a tokeny z nìj vytvoøenými a pøipraví tokeny do podoby, ve které je klasifikátor schopen s nimi pracovat.
Tato tøída se u SVM klasifikátoru jmenuje \texttt{Data}.
Její funkcí je vytvoøit prostor pøíznakù ze vstupních dat a data následnì rozdìlit na bloky trénovacích a testovacích dat.
Dal¹í specifickou tøídou, ji¾ klasifikátor zalo¾ený na SVM pou¾ívá, je tøída \texttt{Kernel} a její podtøídy, které klasifikátor potøebuje pro výpoèet jaderné funkce pro klasifikaci.
Tuto tøídu dìdí v implementovaném programu následující tøi specifické jaderné funkce:

\begin{itemize}
  \item Lineární jaderná funkce
  \item Polynomiální jaderná funkce
  \item Radiální bázová jaderná funkce (RBF)
\end{itemize}

Poslední tøídou která je v souvislosti s klasifikátorem zalo¾eným na SVM spojena, je tøída \texttt{Annealing}, je¾ øe¹í obtí¾ný úkol vhodného výbìru parametrù u klasifikátoru a jím pou¾ívané jaderné funkce.
Více o tomto výbìru vhodných parametrù bude popsáno v èásti o implementaci SVM klasifikátoru (viz \ref{IMPLEMENTATION-SVM}).

Tímto jsme velmi obecnì pro¹li v¹echny tøídy pou¾ité v implementaèní èásti této práce.
Pro podrobnìj¹í popis tøíd je vygenerována dokumentace pomocí dokumentaèního nástroje \texttt{epydoc}.
Dokumentace je pøilo¾ena na CD, je¾ je souèástí této práce.

\section{Implementace}
V této èásti kapitoly se budem podrobnìji zabývat postupem implementace jednotlivých vybraných èásti programu z praktické èásti této práce.
Pokusíme se zde rozebrat problémy, jen¾ pøi implementaci nastaly a jejich øe¹ení.
Tato sekce bude pro vìt¹í pøehlednost rozdìlena na implementaci Bayesovského klasifikátoru a klasifikátoru zalo¾eného na SVM, ve kterých budeme rozebírat odpovídající implementaèní problémy.


\section{Implementace Bayesovského klasifikátoru}
Pøi implementaci Bayesovského klasifikátoru nenastaly prakticky ¾ádné problémy. 
Bylo pouze tøeba vyøe¹it ukládání dat slovníku obsahujícího pravdìpodobnostní hodnoty jednotlivých tokenù na nì¾ klasifikátor pøi svém uèení narazil do souboru tak, aby se klasifikátor nemusel pøed ka¾dou klasifikací znovu uèit v¹echna data z trénovací sady, co¾ by bylo velmi zdlouhavé.
Pro tuto potøebu jsem zvolil knihovní funkci programovacího jazyku Python s názvem \textit{pickle}, které umo¾òuje provést takzvanou serializaci libovolné datové struktury programovacího jazyka do souboru a zpìt.


\section{Implementace klasifikátoru SVM}\label{IMPLEMENTATION-SVM}
Na rozdíl od implementace Bayesovského klasifikátoru nastaly pøi implementaci klasifikátoru zalo¾eného na support vector machines urèité problémy, a to zejména tyto dva:

\begin{itemize}
\item Jaké parametry pøedat balíèku kvadratického programování
\item Jak optimálnì zvolit parametr $C$ (viz. \ref{SVM_C}) a parametry jaderných funkcí.
\end{itemize}

Mimo tìchto dvou problémù byla v implementaci pou¾ita takzvaná gram matice, která výraznì zrychlila proces trénování klasifikátoru \ref{IMP-GRAM}.

\subsection{Parametry kvadratického programování}

Jak ji¾ bylo zmínìno vý¹e v této kapitole o pou¾itých knihovnách (viz \ref{IMPL_LIB}) pro øe¹ení optimalizaèního problému, vycházejícího z matematického popisu klasifikátoru SVM (viz \ref{SVM}) byl pou¾it balík knihoven pro programovací jazyk Python s názvem \texttt{CVXOPT}, pøesnìji modul \texttt{qp} z tohoto balíku knihoven umo¾òující øe¹it optimalizaèní problémy pomocí kvadratického programování.

Optimalizaèní problém, který potøebujeme øe¹it je popsán na konci kapitoly \ref{SVM} a vypadá následovnì:

\begin{equation}
\label{IMP_SVM_MAX}
\begin{aligned}
& \underset{\alpha}{\text{maximalizace}}
& & \sum^{n}_{i=1}\alpha_i - \frac{1}{2}\sum^{n}_{i=1}\sum^{n}_{j=1}y_iy_j\alpha_i\alpha_jx^T_i x_j \\
& \text{kde}
& & \sum^n_{i=0} y_i\alpha_i = 0, & 0 \leq \alpha_i \leq C
\end{aligned}
\end{equation}

Zde se nám ji¾ ukazuje první problém, který je tøeba vyøe¹it, a sice ten, ¾e optimalizaèní algoritmus kvadratického programování v balíku \texttt{CVXOPT.qp} je implemntován tak, aby øe¹il minimalizaèní problémy. 
Musíme tedy pøevést problém \ref{IMP_SVM_MAX} na ekvivalentní minimalizaèní problém:

\begin{equation}
\label{IMP_SVM_MIN}
\begin{aligned}
& \underset{\alpha}{\text{minimalizace}}
& & \frac{1}{2}\sum^{n}_{i=1}\sum^{n}_{j=1}y_iy_j\alpha_i\alpha_jx^T_i x_j - \sum^{n}_{i=1}\alpha_i\\
& \text{kde}
& & \sum^n_{i=0} y_i\alpha_i = 0, & 0 \leq \alpha_i \leq C
\end{aligned}
\end{equation}

V dokumentaci balíku \texttt{CVXOPT} je definován formát kvadratických problémù, jen¾ je modul \texttt{qp} schopen øe¹it.
Vypadá takto:

\begin{equation}
\label{IMP_SVM_CVXOPT}
\begin{aligned}
& \underset{\alpha}{\text{minimalizace}}
& & \frac{1}{2} \alpha^T P \alpha + q^T \alpha\\
& \text{kde}
& & G\alpha \leq h \\
& & & A\alpha = b
\end{aligned}
\end{equation}

Museli jsme tedy urèit, jaké hodnoty dosadit za parametry $P$ (kvadratické koeficienty), $q$ (lineární koeficienty), $G$ a $h$ (podmínky nerovnosti), $A$ a $b$ (podmínky rovnosti).

Za promìnnou $P$ jsme dosadili matici 

\begin{equation}P = 
\begin{bmatrix}
y_1y_1 \; x^T_1x_1 & y_1y_2 \; x^T_1x_2 & y_1y_3 \; x^T_1x_3 & \cdots & y_1y_N \; x^T_1x_N \\
y_2y_1 \; x^T_2x_1 & y_2y_2 \; x^T_2x_2 & y_2y_3 \; x^T_2x_3 & \cdots & y_2y_N \; x^T_2x_N \\
\vdots            & \vdots           & \vdots          & \vdots & \vdots          \\
y_Ny_1 \; x^T_Nx_1 & y_Ny_2 \; x^T_Nx_2 & y_Ny_3 \; x^T_Nx_3 & \cdots & y_Ny_N \; x^T_Nx_N
\end{bmatrix}
\end{equation}

kde platí, ¾e $x_N \in X$ a $y_N \in Y$.
Tato matice v¹ak platí pouze v pøípadì, ¾e nepou¾íváme jaderných funkcí pro vytvoøení nelineární diskriminaèní funkce.
Jestli¾e v¹ak pou¾íváme jadernou funkci $K(x_1, x_2)$ poèítající skalární souèin v prostoru pøíznakù (viz \ref{SVM-KERNELS}), tak musíme nahradit skalární souèin $x^T_mx_n$ za tuto jadernou funkci, do ní¾ dosadíme jednotlivé vektory.
Matice $P$ pøi pou¾ití jaderných funkcí bude tedy vypadat následovnì:

\begin{equation}P = 
\begin{bmatrix}
y_1y_1 \; K(x_1, x_1) & y_1y_2 \; K(x_1, x_2) & y_1y_3 \; K(x_1, x_3) & \cdots & y_1y_N \; K(x_1, x_N) \\
y_2y_1 \; K(x_2, x_1) & y_2y_2 \; K(x_2, x_2) & y_2y_3 \; K(x_2, x_3) & \cdots & y_2y_N \; K(x_2, x_N) \\
\vdots                & \vdots                & \vdots                & \vdots & \vdots                \\
y_Ny_1 \; K(x_N, x_1) & y_Ny_2 \; K(x_N, x_2) & y_Ny_3 \; K(x_N, x_3) & \cdots & y_Ny_N \; K(x_N, x_N)
\end{bmatrix}
\end{equation}

Dal¹ím parametrem, který musíme kvadratickému programování poskytnout je parametr lineárních koeficientù $q$.
Tím bude pouze vektor stejné délky, jako je poèet vstupních dat obsahující hodnoty -1, tedy:

\begin{equation}
q = (-1, -1, \ldots, -1), \hspace{5em} |q| = |X|
\end{equation}

Nyní se dostáváme k parametrùm $G$, $h$ a $A$, $b$ tyto parametry jak bylo ji¾ vý¹e zmínìno definují podmínky (constrains) optimalizaèního procesu. 
Jak vychází z podmínek rovnice \ref{IMP_SVM_MIN}, musíme zadat jednu podmínku rovnosti, a sice:

\begin{equation}
\sum^n_{i=0} y_i\alpha_i = 0
\end{equation}

To je provedeno velmi jednoduchým zpùsobem, a to ¾e promìnná $A$ bude obsahovat vektor $Y$ (labely trénovacích dat) a promìnná $b$ bude hodnota 0, tedy:

\begin{equation}
A = (y_1, y_2, \ldots, y_N), \hspace{5em} y_i \in Y; i \in |Y|
\end{equation}
\begin{equation}
b = 0
\end{equation}

V programu je mo¾né zvolit, zda chce u¾ivatel pou¾ívat promìnnou $C$ -- zda umo¾ní poru¹ovat margin (viz \ref{SVM-THEORY-MARGINS}).
Pokud se promìnná nepou¾ívá, pak má optimalizaèní problém následující tvar:

\begin{equation}
\label{IMP_SVM_MIN_BEZ_C}
\begin{aligned}
& \underset{\alpha}{\text{minimalizace}}
& & \frac{1}{2}\sum^{n}_{i=1}\sum^{n}_{j=1}y_iy_j\alpha_i\alpha_jx^T_i x_j - \sum^{n}_{i=1}\alpha_i\\
& \text{kde}
& & \sum^n_{i=0} y_i\alpha_i = 0, & 0 \leq \alpha_i \leq \infty
\end{aligned}
\end{equation}

Podíváme-li se podrobnìji na omezení podmínky $\sum^n_{i=0} y_i\alpha_i = 0$, vidíme, ¾e $\alpha$ mù¾e nabývat libovolné hodnoty od 0 do nekoneèna, tudí¾ parametry podmínky nerovnosti budou takovéto:

\begin{equation}
\label{IMP-G}
G = \begin{bmatrix}
-1 & 0 & \cdots & 0\\
0 & -1 & \cdots & 0\\
\vdots & \vdots & \ddots & 0\\
0 & 0 & \cdots & -1 \\
\end{bmatrix}
\end{equation}
\begin{equation}
\label{IMP-h}
h = (0, 0, \ldots, 0), \hspace{5em} |h| = |X|
\end{equation}

Promìnné $G$ a $h$ (\ref{IMP-G} a \ref{IMP-h}) po vlo¾ení do kvadratického programování popisují právì podmínku $0 \leq \alpha_i \leq \infty$.

Jestli¾e ale u¾ivatel zadá, ¾e chce pou¾ívat promìnnou $C$, situace se tro¹ku komplikuje.
Minimalizaèní úloha, kterou potøebujeme vyøe¹it se nepatrnì zmìní do následující podoby:

\begin{equation}
\label{IMP_SVM_MIN_S_C}
\begin{aligned}
& \underset{\alpha}{\text{minimalizace}}
& & \frac{1}{2}\sum^{n}_{i=1}\sum^{n}_{j=1}y_iy_j\alpha_i\alpha_jx^T_i x_j - \sum^{n}_{i=1}\alpha_i\\
& \text{kde}
& & \sum^n_{i=0} y_i\alpha_i = 0, & 0 \leq \alpha_i \leq C
\end{aligned}
\end{equation}

Podmínka, jen¾ v pøedchozím pøípadì (bez $C$) nebyla zhora nijak omezená je najednou limitována hodnotu promìnné $C$.
Parametr $G$ a $h$ se tedy oproti pøedchozímu pøípadu bude muset roz¹íøit tak, aby pojal i pravou èast podmínky.
Bude tedy vypadat takto:

\begin{equation}
\label{IMP-G}
G = \begin{bmatrix}
-1 & 0 & \cdots & 0\\
0 & -1 & \cdots & 0\\
\vdots & \vdots & \ddots & 0\\
0 & 0 & \cdots & -1 \\
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & 0\\
0 & 0 & \cdots & 1 \\
\end{bmatrix}
\end{equation}
\begin{equation}
\label{IMP-h}
h = (0, 0, \ldots, 0, C, C, \ldots, C), \hspace{5em} |h| = 2 \cdot |X|
\end{equation}

Pokud tyto parametry tedy poskytneme modulu kvadratického programování, získáme výsledné hodnoty lagrangeových multiplikátorù, které definují support vektory, jen¾ se z trénovací mno¾iny uèením vytvoøily. 
S tìmito lagrangeovými multiplikátory pak dále pracujeme a vytvoøíme z nich klasifikaèní model. (viz \ref{SVM-THEORY-MARGINS})


\subsection{Gramova matice} \label{IMP-GRAM}
Abychom docílili zrychlení uèení klasifikátoru svm, pøistoupili jsme k výpoètu takzvané gramovy matice.
Gramova matice je matice $gram$ skalárních souèinù skalárních souèinù vstupního vektoru, tedy $ gram_{i,j} = x_i^Tx_j$ kde $x_i, x_j \in X$.
Tím, ¾e vypoèteme tuto matici pøed samotným uèením klasifikátoru, zamezíme zbyteènému nìkolikanásobnému výpoètù kartézských souèinù mezi stejnými vektory vstupních trénovacích dat, èím¾ se výraznì zrychlí výpoèet.

Gramova matice pro lineární SVM vypadá takto:

\begin{equation}
\label{IMP-G}
gram_{linear} = \begin{bmatrix}
x_1^Tx_1 & x_1^Tx_2 & x_1^Tx_3 & \cdots & x_1^Tx_N\\
x_2^Tx_1 & x_2^Tx_2 & x_2^Tx_3 & \cdots & x_2^Tx_N\\
\vdots   & \vdots   & \vdots   & \vdots & \vdots  \\
x_N^Tx_1 & x_N^Tx_2 & & x_N^Tx_3 \cdots & x_N^Tx_N\\
\end{bmatrix}
\end{equation}

Gramova matice lze samozøejmì pou¾ít i pou¾ívají-li se jaderné funkce.
V takovém pøípadì je pro výpoèet gramovy matice $gram$ pou¾ita, namísto standardního kartézského souèinu v prostoru pøíznakù $X$, jaderná funkce, která ale vlastnì pøedstavuje kartézský souèin v prostoru $F$, do kterého lze prostor $X$ pøevést pomocí nìjaké transformaèní funcke $\varphi$ (\ref{SVM}).

Gramova matice pro klasifikátor SVM vyu¾ívající jaderných metzod vypadá následovnì:

\begin{equation}
\label{IMP-G}
gram_{nonlinear} = \begin{bmatrix}
K(x_1, x_1) & K(x_1, x_2) & K(x_1, x_3) & \cdots & K(x_1, x_N)\\
K(x_2, x_1) & K(x_2, x_2) & K(x_2, x_3) & \cdots & K(x_2, x_N)\\
\vdots      & \vdots      & \vdots      & \vdots & \vdots     \\
K(x_N, x_1) & K(x_N, x_2) & K(x_N, x_3) & \cdots & K(x_N, x_N)\\
\end{bmatrix}
\end{equation}

V praktické èásti této práce je gramova matice implementována ve tvaru $gram_{nonlinear}$ a v závislosti na pou¾ité jaderné funkci (jaderná funkce mù¾e být i obyèejným kartézským souèinem v prostoru pøíznakù $X$) je matice pøedpoèítána pro pozdìj¹í pou¾ití pøi trénování klasifikátoru.

\subsection{Volba vhodných volných parametrù klasifikátoru a jaderných funkcí}
Vhodný výbìr volných parametrù klasifikátoru a jaderných funkcí je velmi dùle¾tou souèástí pøi implementaci klasifikátoru zalo¾eného na metodì SVM.
Nejsou-li vybrány vhodné parametry, mù¾e klasifikátor dosahovat vskutku výraznì hor¹ích výsledkù, ne¾ jsou li vybrány tyto parametry optimálnì.
V implementaèní èásti této práce bylo tøeba optimálnì urøit parametr $C$ (míru chybovosti vzhledem k maximalizaci marginu -- viz \ref{SVM_C}) a parametr pou¾ité jaderné funkce (urèující vlastnosti pou¾ité jaderné funkce -- \ref{SVM-KERNELS}), jen¾ budeme nadále nazývat $\gamma$.

Nastává ov¹em problém, jakým zpùsobem tyto volné parametry volit tak, aby byly výsledky klasifikace co nejpøesnìj¹í.
Ve své knize /uv{The Nature of Statistical Learning Theory} \cite{vapnik2000nature} Vladimir Vapnik doporuèil nastavovat volné parametry v závislosti na znalosti mno¾iny trénovacích dat.

Jestli¾e takovými znalostmi o datech nedisponujeme, mù¾eme pou¾ít velmi èasto pou¾ívanou metodu hledání hodnot optimálních parametrù v møí¾ce.
Tato metoda je sice pou¾itelná, ale aby byla dostateènì pøesná, tak je tøeba ji poèítat pro dostateènou hustotu bodù v møí¾ce, co¾ mù¾e být velmi nároèné.

V této práci byla pro hledání optimálních metod pou¾ita metoda \emph{simulovaného ¾íhání (simulated annealing)}.
Simulované ¾íhání je pravdìpodobnostní optimalizaèní metoda pro nalezení globální maximální nebo minimálni energie odpovídající souøadnicím ve stavovém prostoru.
Tato metoda je inspirovaná fyzikou procesu ¾íhání oceli, pøi kterém se materiál pøedehøeje na urèitou teplotu a postupnì se nechá ochlazovat, èím¾ se odstraní napìtí v materiálu, ketrý se takto homogenizuje.
U optimalizaèního procesu simulovaného ¾íháni se nastaví poèáteèní souøadnice ve stavovém prostoru a poèáteèní teplota, která se postupnì po krocích sni¾uje a¾ do úplného vychládnutí.

Pøi ka¾dém kroku se vygenerují náhodné souøadnice ve stavovém prostoru a spoèítá se k nim odpovídající energie.
Jestli¾e je tato energie ni¾¹í (pokud maximalizujeme, pak vy¹¹í), ne¾ energie pøedchozího stavu, pak je tento stav pou¾it pro dal¹í iteraci.
Pokud v¹ak energie ni¾¾¹í nebyla, spoèítá se pravdìpodobnost skoku na dané souøadnice.
Tato pravdìpodobnost závisí na teplotì -- èím vy¹¹í teplota, tím více umo¾òuje algoritmus skoèit na hor¹í hodnoty energie.
Tímto se významì eliminuje riziko uváznutí v lokálním minumu nebo maximu.


\paragraph{Popis algoritmu simulovaného ¾íhání}

Pro co nejjasnìj¹í pøedvedení pou¾itého algoritmu simulovaného ¾íhání pøedstavíme jeho pseudokód:

\begin{algorithm}
\caption{Pseudokód algoritmu simulovaného ¾íhání -- minimalizace energie}
\label{IMP-ANNEALING-PSEUDOCODE}
\begin{algorithmic}[1]
\STATE $state \leftarrow STATE0$
\STATE $energy \leftarrow calculate\_energy(s)$
\STATE $state\_best \leftarrow state$, $energy\_best \leftarrow energy$
\STATE $temp \leftarrow INIT\_TEMP$
\WHILE{$temp > STOP\_TEMP$}
	\STATE $temp \leftarrow temp \cdot COOLING\_FACTOR$
	\STATE $state\_new \leftarrow generate\_neighbors(state)$
	\STATE $energy\_new \leftarrow calculate\_energy(state\_new)$
	\IF{$P(energy, energy\_new, temp) > random()$}
		 \STATE $state \leftarrow state\_new$, $energy \leftarrow energy\_new$
	\ENDIF
	\IF{$energy\_new < energy\_best$}
		 \STATE $state\_best \leftarrow state\_new$, $energy\_best \leftarrow energy\_new$
	\ENDIF
\ENDWHILE
\RETURN $state\_best$
\end{algorithmic}
\end{algorithm}

Promìnné $State0$ -- poèáteèní stav (za stav budeme nadále pova¾ovat bod v prostoru souøadnic volných parametrù SVM klasifikátoru), $INIT\_TEMP$ -- poèáteèní teplota, $STOP\_TEMP$ -- koneèná teplota a $COOLING\_FACTOR$ -- faktor chládnutí jsou parametry, které u¾ivatel algoritmu zadá.
Funkce $calculate\_energy(stav)$ potom poèítá energii stavu $stav$ a funkce $generate_neighbor(stav)$ vygeneruje mo¾ný sousední stav ke stavu $stav$.
V pseudokódu je také pou¾ita funkce random(), která generuje náhodná èísla v rozmezí $<0,1>$.

Na následujících øádcích se podrobnìji podíváme na nìkteré zajímavé souèásti tohoto algoritmu.


\subparagraph{Výpoèet energie stavu}
Abychom byli schopni najít minumální energii systému v souøadnicích, musíme být schopni vypoèítat energii libovolného vygenerovaného stavu.
Pro tento optimalizaci volných parametrù klasifikátoru SVM jsme vycházeli ze dvou po¾adavkù, které na SVM klasifikátor máme, a sice:

\begin{itemize}
\item sna¾íme se maximalizovat pøesnost klasifikátoru
\item sna¾íme se minimalizovat poèet support vektorù klasifikátoru
\end{itemize}

Výpoèet energie tedy bude záviset na tìchto dvou po¾adavcích.

Vzhledem k tomu, ¾e implementovaná metoda simulovaného ¾íhání je minimalizaèní, musíme pøevést reprezentaci tìchto po¾adavkù tak, aby vytvoøily metriku umo¾òující vyu¾ití v procesu minimalizace.

Podíváme-li se na první po¾adavek, je zøejmé, ¾e maximalizaci musíme pøevést na minimalizaci.
Tudí¾ se ze snahy o maximalizaci pøesnosti klasifikátoru dostanema snahu o minimalizaci chybovosti klasifikátoru.
Takto tedy dostaneme vzorec pro výpoèet energie pøesnosti $E_p$ klasifikátoru:

\begin{equation}
E_p = 1 - (\frac{spravne\_klasifikovane\_vstupy}{vsechny\_vstupy}), \hspace{5em} E_p \in <0,1>
\end{equation}

Pomocí tohoto vzorce vypoèteme energii pøesnosti klasifikátoru, jen¾ se budeme sna¾it minimalizovat

Druhým po¾adavkem na minimalizaci volných parametrù SVM, je ¾e po nauèení by mìl klasifikátor co nejvíce minimalizovat poèet svých support vektorù.
Tento po¾adavek ji¾ má minimalizaèní charakter, a proto jej nemusíme nijak pøevádìt.
Pro výpoèet energie tototo po¾adavku $E_{SV}$ dostanem následující vzorec:

\begin{equation}
E_{SV} = \frac{pocet_pouzitych_SV}{vsechny\_SV}, \hspace{5em} E_{SV} \in <0,1>
\end{equation}

Výpoèet celkové energie $E$ stavu se tudí¾ bude rovnat souètu energie pøesnosti a energie support vektorù, tedy:

\begin{equation}
E =  E_p + E_{SV} , \hspace{5em} E \in <0,2>
\end{equation}


\subparagraph{Generování mo¾ného sousedního stavu}
Pro generování mo¾ného sousedního stavu byla pou¾ita metoda navrhnutá v èlánku \uv{Parameter determination of support vector machine and feature selection using simulated annealing approach} \cite{annealing}.
Tato metoda je zalo¾ena na vygenerování náhodného vektoru od aktuálního stavu


\subparagraph{Pravdìpodobnost pøímutí nového stavu}

\chapter{Testy} \label{TESTY}

%./control.py common -d ../data/tweets/annotated.db -n 5 -c 100000 -t 1 -k RBF -a 0.5 -b 0.5
%##################################
%########## svm restults ##########
%##################################
%True positive = 93.0
%True negative = 92.8
%False positive = 7.2
%False negative = 7.2
%Precision = 0.928143712575
%Recall = 0.928143712575
%Accuracy = 0.928071928072
%F-measure = 0.928143712575
%##################################
%
%##################################
%########## bayes restults ##########
%##################################
%True positive = 90.0
%True negative = 91.2
%False positive = 10.0
%False negative = 10.0
%Unknown = 0.0
%Precision = 0.9
%Recall = 0.9
%Accuracy = 0.900596421471
%F-measure = 0.9
%Corelation = 0.836320369874
%##################################
%{'emoticon': 3, 'sentence': 1, 'url': 2, 'tag': 1, 'time': 0, 'date': 0, 'email': 2}

%./control.py common -d ../data/tweets/annotated.db -n 5 -c 100000 -t 2 -k RBF -a 0.5 -b 0.5
%##################################
%########## svm restults ##########
%##################################
%True positive = 94.0
%True negative = 91.4
%False positive = 8.6
%False negative = 8.6
%Precision = 0.916179337232
%Recall = 0.916179337232
%Accuracy = 0.915103652517
%F-measure = 0.916179337232
%##################################
%
%##################################
%########## bayes restults ##########
%##################################
%True positive = 90.0
%True negative = 91.2
%False positive = 10.0
%False negative = 10.0
%Unknown = 0.0
%Precision = 0.9
%Recall = 0.9
%Accuracy = 0.900596421471
%F-measure = 0.9
%Corelation = 0.836320369874
%##################################
%
%{'emoticon': 3, 'sentence': 1, 'url': 2, 'tag': 1, 'time': 0, 'date': 0, 'email': 2}




\section{Data}\label{TEST-DATA}
Jedním z úkolù vyplývajících ze zadání této práce bylo vytvoøit datovou sadu pro uèení a testování vytvoøených klasifikátorù.
Zdrojem tìchto dat byla databáze tweetù projektu M-eco\footnote{http://www.meco-project.eu/}, ve kterém participuje výzkumná skupina NLP pracující pøi Fakultì informaèních technologii VUT v Brnì.

Byly vytvoøeny dvì datové sady, jedna pro anglický jazyk a druhá pro nìmecký.
Celkovì bylo anotováno 4500 tweetù pro anglický jazyk a 3000 tweetù pro nìmecký.
Tyto datové sady potom byly rozdìleny na dvì èásti, z nich¾ jedna byla pou¾ita pro trénování klasifikátoru a druhá pro testování.
Aby klasifikátor fungoval správnì, mìlo by rozlo¾ení relevantních a nerelevantních tweetù v trénovací mno¾inì být zhruba 50/50.
Tento pomìr vycházi z odvození klasifikaèní rovnice (viz \ref{SF-BAYES-MAT-bayes_theorem-deriv}).
Pro anglický jazyk jsou tweety v trénovací mno¾inì zhruba takto rozlo¾eny, nicménì pro nìmecký jazyk jsou tweety rozlo¾eny nerovnomìrnì (asi 80\% relevantních a 20\% nerelevantních).
Mno¾ina nìmeckých trénovacích dat se v¹ak bude je¹tì roz¹iøovat a tento pomìr bude vyrovnán.


\section{Úloha implementovaného klasifikátoru}
Jak bylo ji¾ zmínìno v úvodu práce (\ref{UVOD-MOTIVACE}), jedním z mých cílù je v této práci vytvoøit klasifikátor pro klasifikaci tweetù v projektu M-eco.
Implementovaný klasifikátor je nauèen na rozdìlení vstupních tweetù do dvou tøíd -- \textbf{zabývající se osobními zku¹enostmi pisatelù s nemocemi (ale také zku¹enostmi z pisatelova okolí)}, nebo \textbf{nezabývající se tìmito zku¹enostmi}.
Tedy napøíklad tweet \textit{'I have a huge headache'} nebo \textit{'My dad feels sick'} jsou relevantní, naproti tomu \textit{'Canadians should expect to see more severe cases of swine flu.'} je irelevantní. 

\section{Testovací metriky} \label{TEST-METRIKY}
Aby bylo mo¾né nìjakým zpùsobem porovnat rùzné výsledky klasifikátoru a také posléze klasifikátory mezi sebou, je tøeba zavést vhodné metriky popisující vlastnosti klasifikátoru.
V této èásti práce budou tyto pou¾ité metriky vysvìtleny.
\subsection{Korelace}
Korelace je statistické metoda, která definuje vzájemný vztah mezi velièinami $X$ a $Y$.
Míru korelace urèuje koeficient korelace nabývající hodnot $<-1, 1>$.
Jestli¾e korelaèní koeficient nabývá hodnotu -1, pak to znaèí, ¾e velièiny $X$ a $Y$ jsou na sobì zcela nezávislé.
Naopak nabývá-li korelaèní koeficient hodnoty 1, pak jsou na sobì velièiny pøímo závislé.

Korelaèní koeficient se vypoèítá jako 
$$
  K(X,Y) = \frac{E(XY) - E(X)E(Y)}{\sqrt{E(X^2 - E(X)^2)} \sqrt{(E(Y^2) - E(Y)^2)}}
$$
kde $X$ jsou klasifikátorem automaticky spoèítané pravdìpodobnosti a $X$ jsou pravdìpodobnosti zadané u¾ivatelem pøi anotaci ($0.01 = spam$ a $0.99 = ham$).

Pøi testování klasifikátoru se poèítá korelace mezi u¾ivatelem zadanou hodnotou vstupního textu (pøi anotaci testovací mno¾iny) a výsledkem klasifikátoru.
Èím více se tedy korelaèní koeficient blí¾í hodnotì 1, tím lep¹í výsledek klasifikátoru pøedstavuje.

\subsection{Výsledek klasifikace}\label{TEST-VYS_KLAS}
Abychom byli schopni pøesnì zjistit jak klasifikátor testovací mno¾inu oklasifikoval, rozdìlíme oklasifikovaná data do ètyø skupin.
\begin{itemize}
 \item \textit{True positive} -- poèet tweetù, které byly klasifikátorem správnì zaøazeny do relevantní tøídy.
 \item \textit{True negative} -- poèet tweetù, které byly klasifikátorem správnì zaøazeny do nerelevantní tøídy.
 \item \textit{False positive} -- poèet tweetù, které byly klasifikátorem ¹patnì zaøazeny do relevantní tøídy.
 \item \textit{False negative} -- poèet tweetù, které byly klasifikátorem ¹patnì zaøazeny do nerelevantní tøídy.
\end{itemize}
Tyto hodnoty jsou tedy výsledkem porovnání pøedpokládaného výstupu klasifikátoru s reálným výstupem implementovaného klasifikátoru.
Je zøejmé, ¾e èím ménì záznamù se nachází ve False negative a False positive, tím lépe klasifikátor funguje.

\subsection{Precision, Recall, Accuracy, F--measure}
Dal¹ími metrikami, pomocí nich¾ budeme moci porovnávat výsledky klasifikátorù, jsou funkce Precision(pøesnost), Recall(odezva), Accuracy(pøesnost\footnote{'Accuracy' a 'Precision' se do èeského jazyka pøekládají stejnì, tzn. jako pøesnost, nicménì jde o jiné metriky pro porovnávání klasifikátorù. Z toho dùvodu budeme metriky nazývat anglickými názvy.}) a F--measure.
Pro následující metody se pøedpokládá, ¾e jsme schopni vypoèítat hodnoty \textit{True positive}, \textit{True negative}, \textit{False positive} a \textit{False negative} (viz \ref{TEST-VYS_KLAS})
\subsubsection{Accuracy}
Nejjednodu¹¹í metrikou pro porovnávání pøesnosti klasifikátorù je metrika Accuracy.
Lze ji jednodu¹e vypoèítat následovnì:
$$
  Accuracy = \frac{true\_positive + true\_negative}{true\_positive + true\_negative + false\_positive + false\_negative}
$$
Nevýhodou této metriky v¹ak je, ¾e nebere v potaz poèty záznamù v jednotlivých tøídách.

\subsubsection{Precision. Recall}
$$
  Precision = \frac{true\_positive}{true\_positive + false\_positive}
$$
$$
  Recall = \frac{true\_positive}{true\_positive + false\_negative}
$$
Precision tedy mù¾eme definovat jako pomìr správnì oklasifikovaných relevantních záznamù vùèi v¹em oklasifikovaným relevantním záznamùm.
Recall je potom pomìr správnì oklasifikovaných relevantních záznamù vùèi skuteènì relevantním záznamùm
\subsubsection{F--measure}
$$
  Fmeasure = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
$$
F--measure je jedna z metrik nejèastìji pou¾ívaných pro hodnocení klasifikátorù.
De facto jde o váhovaný prùmìr Precision a Recall.
F--measure nabývá hodnot $<0,1>$, kde 0 je nejhor¹í skóre popisující výsledek klasifikátoru a 1 je nejlep¹í.

\section{Testy Bayesovského klasifikátoru}
Nyní pøistupme k pøedstavení výsledku testù Bayesovského klasifikátoru.
Hlavním cílem jeho testování bylo nalézt optimální délku tokenù pøi pou¾ití nového zpùsobu tokenizace (viz \ref{IMP-BAYES-TOKEN}).
Testy jsou provádìny na manuálnì anotovaných datech(viz. \ref{TEST-DATA}.
Testy provádíme samostatnì pro anglický jazyk na anglických tweetech a pro nìmecký jazyk na nìmeckých tweetech.

\subsection{Angliètina}
Velikost testovací sady pro anglický jazyk bylo 120 tweetù, z nich¾ 60 bylo relevantních a dal¹ích 60 irelevantních k tématu.
Pro zji¹tìní ideální délky $N$-tic tokenù jsme aplikovali testy pro postupnì zvìt¹ující se $N$, a to dokud se zlep¹ovala hodnota korelaèního koeficientu (korelace) a F--measure (viz \ref{TEST-METRIKY}).
Následující tabulka øíká, jak se klasifikátor choval pøi zmìnì nastavení maximální délky za sebou jdoucích $N$-tic slov(tokenù).
\begin{center}
\begin{scriptsize}
\begin{tabular}{| c || c | c | c | c | c | c |}
  N-length       & 1             & 2              & 3             & 4              & 5              & 6              \\
  Korelace       & 0.5184        & 0.5409         & 0.5541        & 0.559          & 0.5668         & 0.5668         \\
  True positive  & 51.0 (42.5\%) & 52.0 (43.33\%) & 54.0 (45.0\%) & 55.0 (45.83\%) & 55.0 (45.83\%) & 55.0 (45.83\%) \\
  True negative  & 48.0 (40.0\%) & 48.0 (40.0\%)  & 48.0 (40.0\%) & 48.0 (40.0\%)  & 48.0 (40.0\%)  & 48.0 (40.0\%)  \\
  False positive & 12.0 (10.0\%) & 12.0 (10.0\%)  & 12.0 (10.0\%) & 12.0 (10.0\%)  & 12.0 (10.0\%)  & 12.0 (10.0\%)  \\
  False negative & 9.0 (7.5\%)   & 8.0 (6.67\%)   & 6.0 (5.0\%)   & 5.0 (4.17\%)   & 5.0 (4.17\%)   & 5.0 (4.17\%)   \\
  Precision      & 0.8095        & 0.8125         & 0.8182        & 0.8209         & 0.8209         & 0.8209         \\
  Recall         & 0.85          & 0.8667         & 0.9           & 0.9167         & 0.9167         & 0.9167         \\
  Accuracy       & 0.825         & 0.8334         & 0.85          & 0.8583         & 0.8583         & 0.8583         \\
  F-measure      & 0.829         & 0.8387         & 0.8572        & 0.8661         & 0.8661         & 0.8661         \\
\end{tabular}
\end{scriptsize}
\end{center}

\subsection{Nìmecký jazyk}
Pro nìmecký jazyk jsme spou¹tìli obdobné testy jako pro jazyk anglický, tzn. zji¹»ovali jsme optimální délku $N$-tic pro tokenizaci.
Testovací mno¾ina pro nìmecký jazyk sestává z 25 relevantních tweetù a 191 irelevantních. 
\begin{center}
\begin{scriptsize}
\begin{tabular}{| c || c | c | c | c | c | c |}
  N-length       & 1              & 2               & 3               & 4               & 5               & 6               \\
  Korelace       & 0.2723         & 0.2423          & 0.2433          & 0.2429          & 0.2429          & 0.2429          \\
  True positive  & 22.0 (10.19\%) & 22.0 (10.19\%)  & 22.0 (10.19\%)  & 22.0 (10.19\%)  & 22.0 (10.19\%)  & 22.0 (10.19\%)  \\
  True negative  & 103.0 (47.69\%)& 106.0 (49.07\%) & 107.0 (49.54\%) & 107.0 (49.54\%) & 107.0 (49.54\%) & 107.0 (49.54\%) \\
  False positive & 88.0 (40.74\%) & 85.0 (39.35\%)  & 84.0 (38.89\%)  & 84.0 (38.89\%)  & 84.0 (38.89\%)  & 84.0 (38.89\%)  \\
  False negative & 3.0 (1.39\%)   & 3.0 (1.39\%)    & 3.0 (1.39\%)    & 3.0 (1.39\%)    & 3.0 (1.39\%)    & 3.0 (1.39\%)    \\
  Precision      & 0.2            & 0.2056          & 0.2075          & 0.2075          & 0.2075          & 0.2075          \\
  Recall         & 0.88           & 0.88            & 0.88            & 0.88            & 0.88            & 0.88            \\
  Accuracy       & 0.5787         & 0.5926          & 0.5972          & 0.5972          & 0.5972          & 0.5972          \\
  F-measure      & 0.3259         & 0.3333          & 0.3358          & 0.3358          & 0.3358          & 0.3358          \\
\end{tabular}
\end{scriptsize}
\end{center}

\subsection{Shrnutí výsledkù testù}
Z výsledkù pro jednotlivé jazyky je zøejmé, ¾e pro anglický jazyk klasifikátor pracuje vcelku spolehlivì a s pomìrnì vysokou pøesností (F--measure = 0.8661).
Pro nìmecký jazyk jsou v¹ak výsledky klasifikace pomìrnì ¹patné (F--measure = 0.3358).
Nepøesnost klasifikátoru je zøejmì zpùsobena nevyvá¾eností trénovacích dat, ale také tím, ¾e nìmecký jazyk je gramaticky výraznì slo¾itìj¹í, ne¾ jazyk anglický.

Zamìøme se nyní na vylep¹ení klasifikátoru, které pøiná¹í alternativní pøístup k tokenizaci oproti klasickému pøístupu (viz \ref{IMP-BAYES-TOKEN}).
Podíváme-li se do tabulky výsledkù klasifikátoru, je jasnì vidìt, ¾e pøesnost klasifikátoru roste s rostoucím $N$.
Tento rùst pøesnosti v¹ak není nekoneèný, a to pøedev¹ím kvùli tomu, ¾e slova která jsou spolu v kontextu, bývají ve vìtì velmi èasto blízko sebe.
Pokud je $N$ nastaveno na 1, pak je pro tokenizaci pou¾it klasický pøístup.

Pøi testování klasifikátoru na anglických tweetech je z tabulky zøejmé, ¾e hodnota korelace roste a¾ do $N = 5$, tzn. tokeny jsou vytvoøeny a¾ z pìti za sebou jdoucích slov.
Bylo by tedy vhodné pøi nasazení klasifikátoru nastavit $N$ na hodnotu 5.
I v tabulce popisující klasifikátor nìmeckých tweetù je vidìt, ¾e pou¾ití nové metody tokenizace je výhodné, aèkoliv v tomto pøípadì jsou výsledky klasifikátoru zlep¹eny jen malou mìrou.

\chapter{Závìr} 
V této semestrální práci jsem se vìnoval obecným klasifikaèním metodám se zamìøením na klasifikaci textu.
Uvedl jsem zde také struènì historický vývoj oboru zpracování pøirozeného jazyka a velmi struèný výèet nìkterých jeho základních úloh.

Hlavním tématem v¹ak byla klasifikace textu podle tématu, co¾ bylo také zadáním této práce.
Jako hlavní metoda pro øe¹ení tohoto problému jsem vybral metodu filtrování spamu nazývanou Bayesovský klasifikátor (Bayesian Classifier).
Rozebral jsme dopodrobna matematický model této metody a pøedstavil jsem pøíklady jejího u¾ití pøi klasifikaci textových dokumentù a nìkteré mo¾nosti jejího vylep¹ení.

V dal¹í èásti práce jsem implementován Bayesovský klasifikátor pro klasifikaci tweetù z internetové sociální sítì Twitter.
Klasifikátor mìl tyto tweety rozdìlit do dvou tøíd urèujících relevanci, nebo naopak nerelevanci daných tweetù z hlediska jejich vyu¾ití v epidemiologických systémech.
Pro tokenizaci vstupního tweetu jsem vytvoøil pøístup sna¾ící se alespoò èásteènì odstranit slabinu Bayesovského klasifikátoru zpùsobující, ¾e klasifikátor nebere v potaz kontext slov ve vìtì, ale pouze samostatná slova.
Tato vlastnost klasifikátoru vychází z odvození rovnice, pou¾ité pro klasifikaci, z Bayesova teorému.
Pomocí testù jsem nakonec dokázal, ¾e implementovaný pøístup vylep¹ení klasické tokenizace pomáhá zlep¹it výsledky Bayesovského klasifikátoru.

V této práci bych rád pokraèoval a roz¹íøil ji na diplomovou práci.
V té bych chtìl na doposud implementovaný Bayesovský klasifikátor pou¾ívající alternativní metodu tokenizace pou¾ít metodu boosting pro dal¹í zlep¹ení pøesnosti klasifikace.
Druhým klasifikátorem, který bych pro svou diplomovou práci chtìl implementovat, je neprobabilistický klasifikátor SVM (Support vector machine).
Tyto dva implementované klasifikátory zamý¹lím porovnat podle zavedených metrik a lep¹í klasifikátor vyu¾ít rovnì¾ ve své dal¹í práci pøi klasifikaci tweetù pro projekt M-eco.

\nocite{automatic_sumarization}
\nocite{anonymisation}
\nocite{nlg}
\nocite{clustering}
\nocite{Manning}
\nocite{SRM}
\nocite{boosting_bayesian}
\nocite{Malik2007thesis}

%=========================================================================
