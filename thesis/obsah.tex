make%=========================================================================
% (c) Michal Bidlo, Bohuslav Køena, 2008

\chapter{Úvod}
Klasifikace dokumentù podle tématu je jednou z úloh oboru zpracování pøirozeného jazyka (Natural Language Processing - NLP).
Zpracování pøirozeného jazyka je oborem aplikace výpoèetních modelù pro øe¹ení úkolù, které se urèitým zpùsobem vztahují k textu libovolného pøirozeného jazyka.
Historie zpracování pøirozeného jazyka zaèíná v padesátých letech 20. století a od té doby doznal obor zpracování pøirozeného jazyka díky vysokému nárùstu výpoèetního výkonu velkých zmìn.
V souèasné dobì patøí mezi nejèastìji pou¾ívané metody zpracování pøirozeného jazyka statistické metody a metody strojového uèení (machine learning).

Významnou úlohou øe¹enou v oboru zpracovávání pøirozeného jazyka je klasifikace textu.
Tato úloha spadá do vý¹e zmínìné skupiny statistických metod a metod strojového uèení.
Metody klasifikace textu lze pou¾ít pro velké mno¾ství úloh, které se týkají zpracovávání pøirozeného jazyka, a to pøedev¹ím díky jejich flexibilitì a ji¾ velmi dobøe zpracované teorii.

V této práci se nejprve zabýváme obecnými pøístupy pou¾ívanými pro øe¹ení úloh zpracovávání pøirozeného jazyka a následnì pøedstavujeme dvì zvolené klasifikaèní metody -- jednu probabilistickou a jednu neprobabilistickou.

Tou první, probabilistickou metodou, která je v této práci podrobnì popsána, je metoda nazývaná \textit{Bayesovský klasifikátor}, vyu¾ívající ke klasifikaci textu Bayesova teorému.
Bayesovský klasifikátor, nebo také Bayesovský filtr, jak je obèas nazýván, je velmi èasto pou¾íván e-mailovými klienty pro zji¹»ování, zda je e-mailová zpráva vy¾ádanou, èi nevy¾ádanou po¹tou.

Druhou zvolenou metodou je neporobabilistická klasifikaèní metoda \textit{SVM} (support vector machines), je¾ pomocí namapování vstupního textu do $n$-dimenzionálního prostoru a následným rozdìlením tohoto prostoru na dva poloprostory doká¾e vstupní text klasifikovat.

Obì metody jsou v této práci podrobnì rozebrány jak z teoretického, tak praktického hlediska.

Souèástí této práce je mimo teoretické èásti také èást praktická a èást implementaèní.
Praktická èást se nachází v druhé polovinì tohoto dokumentu a popisuje implementaci a pøístupy k øe¹ení problémù, které nastaly pøí vytváøení implementaèní èásti.
Také se v ní nacházejí výsledky testù, které byly nad implementovaným programem provedeny a jejich interpretace.
Souèástí implementaèní èásti této práce je potom samotný program, v nìm¾ jsou implementovány obì vý¹e zmínìné klasifikaèní metody.
Souèástí praktické èásti je také mno¾ina trénovacích a testovacích dat, které jsou potom pou¾ívány v testech.
V této práci je také popsána implementaèní èást, v ní¾ jsou specifikovány ve¹keré problémy a postupy, pou¾ité pøi implementaci praktické èásti.

Na závìr jsou prezentovány výsledky získané z porovnání dvou vý¹e zmínìných implementovaných klasifikátorù.

\section{Zadání práce a motivace} \label{UVOD-MOTIVACE}
Zadáním této práce bylo seznámit se s problematikou klasifikace dokumentù dle tématu a zvolit si dvì metody, kterými se budu podrobnìji zabývat.
Následnì tyto dvì metody analyzovat a implementovat program, který pomocí tìchto metod bude klasifikovat vstupní dokumenty do urèitých tøíd.
U obou metod jsem mìl dle zadání dbát na výbìr vhodných pøíznakù z textu.

Pro potøeby této práce jsem anotoval velkou sadu dokumentù, které budu následnì pou¾ívat pro trénování a testování mnou vytvoøeného programu.
Na tìchto datech následnì porovnám dvì zvolené metody klasifikace pomocí standardních metrik pro hodnocení klasifikátorù.

Téma diplomové práce jsem si zvolil, jeliko¾ jsem témìø dva roky pracoval na projektu M-Eco\footnote{http://www.meco-project.eu/} ve skupinì NLP na VUT FIT, kde jsem se zabýval klasifikací tweetù\footnote{Tweety -- pøíspìvky zveøejòované na sociální síti Twitter (http://twitter.com/)}.
Reálné vyu¾ití Bayesovského klasifikiátoru pøi této práci mne motivovalo k tomu, abych se klasifikaèními metodami zabýal dále.
Této práce bych proto chtìl vyu¾ít k tomu, abych prohloubil své znalosti o problematice klasifikace dokumentù a porozumìní klasifikaèním metodám.
V rámci klasifikaèních metod se chci zamìøit pøedev¹ím na klasifikátor zalo¾ený na SVM, jeliko¾ metoda SVM je velmi dobøe variabilní a lze ji pou¾ít pro øe¹ení velkého mno¾ství rùzných klasifikaèních, ale napøíklad i regresních úkolù.

Bayesovský klasifikátor implementovaný v této práci byl nasazen do reálného provozu v projektu M-Eco.

\section{Návaznost práce na semestrální projekt} \label{UVOD-NAVAZNOST}
Tato práce úzce navazuje na odevzdaný dokument semestrálního projektu, jen¾ byl vytvoøen pro stejnojmenný pøedmìt.

Semestrální projekt obsahoval úvodní kapitolu o problematice zpracování pøirozeného jazyka a popis problematiky naivního Bayesovského klasifikátoru, jen¾ byl pro tuto práci také implementován (viz \ref{BAYES}).
Byla vytvoøena malá testovací sada pro trénování a testování tohoto klasifikátoru.
Na této datové sadì a klasifikátorem byly následnì provedeny testy, v nich¾ byl zhodnocen vliv tokenizace (viz \ref{TOKENIZACE}) na klasifikaèní schopnosti implementovaného Bayesovského klasifikátoru.
V odevzdané práci je¹tì nebyly implementovány speciální pøíznaky (viz \ref{TOKENIZACE-SPEC}).



\chapter{Zpracování pøirozeného jazyka}
V této kapitole se budeme struènì zabývat historií (viz \ref{NLP-H}) oboru zpracovávání pøirozeného jazyka (natural language processing, dále té¾ NLP), poté hlavními úkoly, které se v oboru zpracovávání pøirozeného jazyka øe¹í (\ref{NLP-HU}) a nakonec podrobnìji i klasifikaèními úlohami(viz \ref{NLP-KU}), které jsou hlavním tématem této práce.

\section{Historie} \label{NLP-H}
Obor zpracovávání pøirozeného jazyka se vyvíjí soubì¾nì s historií vývoje výpoèetní techniky.
Po roce 1945, kdy spoleèensko politická situace ve svìtì umo¾nila zmìnu hlavních dosavadních smìrù výzkumu v oblasti výpoèetní techniky od pùvodnì pøevá¾nì vojenského vyu¾ití (napø. kryptografie, kryptoanalýza, atd.) k dal¹ím vìdním oborùm.
Díky tomu se zaèal rozvíjet také obor zpracovávání pøirozeného jazyka.

Jedním z prvních významných mílníkù v historii zpracovávání pøirozeného jazyka byl èlánek Alana Turinga s názvem \uv{Computing Machinery and Intelligence} \cite{turing_50}, v nìm¾ Turing publikoval takzvaný \textit{Turingùv test} jako¾to kritérium inteligence.
Aby poèítaèový program pro¹el \textit{Turingovým testem}, nesmí nestranný soudce poznat z obsahu konverzace mezi programem a èlovìkem (konverzace probíhá v reálném èase), která strana je která.

Jednou z prvních øe¹ených úloh v oboru zpracování pøirozené øeèi bylo vytvoøení automatických pøekladaèù, tedy programù, které bez lidského pøispìní doká¾ou pøelo¾it vstupní text z jednoho pøirozeného jazyka do druhého.
Uspokojivé øe¹ení tohoto úkolu v¹ak v té dobì nebylo nalezeno, a ani v roce 1966 je¹tì výzkumníci nebyli nikterak blízko k vyvinutí takového softwaru.
V roce 1966 proto vydal americký vládní výbor \uv{The Automatic Language Processing Advisory Committee} (ALPAC) zprávu shrnující dosavadní výsledky výzkumu a konstatující, ¾e \uv{Nebyl vyvinut ¾ádný strojový pøekladaè obecných vìdeckých textù a ¾ádný takový pøekladaè nebude vyvinut ani v blízké budoucnosti}.
Takto formulovaný závìr zprávy zapøíèinil výrazné ¹krty v rozpoètech vìdeckých týmù, zabývajících se výzkumem zpracovávání pøirozeného jazyka a pøekladaèù.

Do 80. let 20. století pou¾ívala drtivá vìt¹ina systémù pro zpracovávání pøirozeného jazyka velmi slo¾itá ruènì zadávaná pravidla.
V 80. letech v¹ak byly poprvé pøedstaveny metody strojového uèení, které díky stále rostoucímu výkonu výpoèetní techniky umo¾òovaly generovat tato pravidla automaticky a dosahovat tak pøi zpracovávání pøirozeného jazyka stále lep¹ích výsledkù.
S nástupem strojového uèení se odvìtví zpracovávání pøirozeného jazyka zaèalo zamìøovat na statistické metody, je¾ k øe¹ení problémù pøistupovaly jinak ne¾ metody dosavadní.
Jejich hlavním principem byla práce s pravdìpodobnostními modely a váhami jednotlivých rozhodnutí.
Tímto zpùsobem bylo mo¾né efektivnìji øe¹it vìt¹inu NLP úkolù.
Vzhledem k tomu, ¾e obor zpracovávání pøirozeného jazyka pracuje s daty vytvoøenými èlovìkem, je¾ mohou obsahovat rùzné chyby, pracují statistické metody znaènì spolehlivìji ne¾ døívìj¹í ruènì psaná pravidla.

V souèasné dobì je výzkum zpracovávání pøirozeného jazyka orientován pøedev¹ím na vytvoøení autonomních a semiautonomních uèících algoritmù, tedy algoritmù, schopných uèit se z dat, která pøedtím nebyla ruènì anotována, a nebo z kombinace anotovaných a neanotovaných dat.

\section{Hlavní úkoly øe¹ené v NLP} \label{NLP-HU}
Obor zpracovávání pøirozeného jazyka je velmi rozsáhlý a existuje v nìm mimo klasifikace textu velké mno¾ství dal¹ích úloh k øe¹ení.
Nyní velmi struènì popí¹eme nìkolik halvních úkolù øe¹ených v oboru zpracování pøirozeného jazyka:

\begin{itemize}
\item \emph{Automatická sumarizace} -- úkolem automatické sumarizace je redukovat vstupní text nebo sadu textù do nìkolika slov, nebo krátkého odstavce, popisujícího sémantický obsah vstupního textu.
\item \emph{Generování pøirozeného jazyka} -- generování pøirozeného jazyka má vytvoøit výstup v pøirozeném jazyce z interní reprezentace v poèítaèi.
\item \emph{Porozumìní pøirozenému jazyku} -- programy pro porozumìní pøirozenému jazyku mají pøevést vstupní text do podoby zpracovatelné poèítaèem, tedy porozumìt textu a vygenerovat vnitøní reprezentaci onìch vstupních dat.
\item \emph{Odpovídání na otázky} -- v oblasti odpovídání na otázky se programátoøi pokou¹ejí vytvoøit program, který by dokázal korektnì odpovìdìt na u¾ivatelem zadanou vstupní otázku formulovanou v pøirozeném jazyce.
\item \emph{Odstraòování víceznaènosti slov v textu} -- odstraòování víceznaènosti slov v textu je významnou úlohou napomáhající správnému porozumìní textu.
Víceznaèná slova jsou v textu identifikována a poté je vyhledáván jejich správný význam v kontextu vstupního textu.
\item \emph{Strojový pøeklad} -- je úloha zpracování pøirozeného jazyka, která pøevádí vstupní text v urèitém vstupním jazyce na výstupní text v jazyce jiném.
\item \emph{Klasifikaèní úlohy} -- mají za úkol libovolné vstupy pøiøadit do pøedem urèených tøíd.
Tyto tøídy mohou být buï 2 (takzvané binární klasifikace), nebo více (takzvaná multilabel klasifikace).
\end{itemize}


\section{Klasifikaèní úlohy} \label{NLP-KU}
V této èásti kapitoly se podrobnìji podíváme na následující tøi klasifikaèní úlohy:
\begin{itemize}
 \item Anonymizace
 \item Klasifikace tématu
 \item Filtrování spamu
\end{itemize}

\subsection{Anonymizace}\label{NLP-KU-anonym}
Klasifikaèní úloha anonymizace není pøi zpracovávání pøirozeného jazyka pøíli¹ èasto øe¹ená.
Jejím cílem je odstranìní citlivých referencí (napøíklad osobních informací -- rodného èísla, e-mailové adresy, atd.) z tìla daného textu, co¾ umo¾òuje následnì anonymizovaný text vyu¾ít napøíklad pro výzkumné úèely.
Anonymizace na rozdíl od filtrování spamu vy¾aduje daleko jemnìj¹í pøístup ke klasifikaci, jeliko¾ pracuje nikoliv s celým dokumentem, ale pouze s jeho èástmi, nìkdy dokonce jen nìkolika slovy, nebo vìtami.
Pro anonymizaci jsou nejèastìji pou¾ívány tyto 3 postupy: 
\begin{itemize}
 \item Odstranìní -- odstranìní v¹ech citlivých informací z dokumentu a jejich nahrazení výplní
 \item Pseudoanonymizace -- nahrazení v¹ech citlivých informací náhodnými hodnotami stejného typu
 \item Kategorizace -- nahrazení v¹ech citlivých informací kategorií do které spadají.
\end{itemize}
Zpùsoby u¾ití anonymizace viz obr. \ref{NLP-KU-anonym-img}.
\begin{figure}[h]
    \begin{center}
      \includegraphics[width=15cm,keepaspectratio]{fig/anonymization}
      \caption{Zpùsoby pou¾ití anonymizace.} 
      \label{NLP-KU-anonym-img}
    \end{center}
\end{figure}

\subsection{Klasifikace tématu}\label{NLP-KU-klas_tem}
Klasifikace tématu je úloha pøiøazování názvù témat ke vstupním textovým dokumentùm.
Typicky daný vstupní text pokrývá vìt¹í mno¾ství témat.
Metody pro klasifikaci tématu mohou být zalo¾eny napøíklad na skrytých Markovových modelech.
Výstupem klasifikátoru tématu pro vstupní text bývá velmi èasto kromì seznamu tøíd témat, kterých se zøejmì vstupní text týká, také seznam pravdìpodobností definující míru nále¾itosti do jednotlivých tìchto tøíd.

\subsection{Filtrování spamu}\label{NLP-KU-spam_filt}
Zájem o klasifikaèní problém filtrování spamu v posledních letech velmi výraznì vzrostl, a to pøedev¹ím kvùli mno¾ství nevy¾ádané po¹ty (spamu), kterou u¾ivatelé dostávají do svých e-mailových schránek.
Jsou dvì mo¾nosti jak úloha filtrování spamu mù¾e fungovat.
Buï jsou zprávy filtrovány na základì obsahu (a» u¾ textového, nebo jiného) nebo na základì metainformací v hlavièce zprávy.
Metody zabývající se filtrováním na základì obsahu e-mailu velmi èasto vyu¾ívají toho, ¾e vìt¹ina zpráv obsahuje nìjakou textovou informaci.
Podle ní potom klasifikují, zda je zpráva nevy¾ádanou po¹tou.
Klasifikátory urèující, zda je zpráva spam nebo ne mají pro klasifikaci e-mailù velmi èasto asymetrické ohodnocení pøi klasifikaci.
Chybné oznaèení vy¾ádané po¹ty jako spamu, vedoucí k následnému odstranìní zprávy, je toti¾ vìt¹í problém, ne¾ oznaèení spamu jako vy¾ádané po¹ty.

V této práci se budeme zabývat právì pøedev¹ím metodami filtrování spamu, a to nejen pro pou¾ití pøi jeho odfiltrovávání z e-mailové po¹ty, ale také pro oznaèování relevantních textových vstupù vzhledem k danému tématu.

\chapter{Klasifikace dokumentù}
Jedním z úkolù této práce je získat pøehled o klasifikaci textových dokumentù, seznámit se s metodami klasifikace a aplikovat je na vytvoøenou datovou sadu.
\section{Definice klasifikace} \label{CLASS-DEF}

\begin{definition}
 Klasifikace je èinnost, která rozdìluje objekty do tøíd (kategorií) podle jejich spoleèných vlastností.
\end{definition}

Tøída v kontextu zpracovávání pøirozeného jazyka je mno¾ina objektù, vyznaèujících se urèitou spoleènou vlastností nebo vlastnostmi, která/é danou tøídu popisuje/í.
Klasifikace je potom èinnost, která pøiøazuje objekty do daných tøíd.
Nadále se v této práci budeme zabývat klasifikací textu.

Obecnì mìjme tedy prostor objektù $X$ a mno¾inu tøíd $Y$.
Operaci klasifikace potom odpovídá funkci $f$:

\begin{equation}
f: X \rightarrow Y
\end{equation}

Klasifikaèní funkce $f$ tedy pøiøadí jeden objekt z mno¾iny objektù právì do jedné tøídy.
Mù¾e ov¹em nastat situace, kdy jeden objekt odpovídá kritériím pro zaøazení do více tøíd.
Napøíklad budeme klasifikovat textovou zprávu, je¾ mù¾e z hlediska obsahu zapadnout do více tøíd, napøíklad do tøídy osobních zku¹eností (pisatel pí¹e o vlastní zku¹enosti) a do tøídy relevantní k tématu zdraví (\textit{\uv{Bolí mì hlava.}}).
V tomto pøípadì musíme modifikovat klasifikaèní funkci $f$ následovnì:

\begin{equation}
f: X \rightarrow 2^Y
\end{equation}

kde $2^Y$ oznaèuje potenèní mno¾inu v¹ech tøíd.
Tato forma klasifikace se také oznaèuje jako \textit{multi-label klasifikace}.
Mimo multi-label klasifikace ov¹em existuje druhá forma klasifikace -- takzvaná \textit{binární klasifikace}, u které se text klasifikuje právì do dvou tøíd.
V této práci se nadále budeme zabývat právì binární klasifikací. 

Pøi klasifikaci se ke ka¾dé tøídì mù¾e pøiøadit reálné èíslo $p \in <0,1>$, které definuje míru nále¾itosti klasifikovaného objektu do pøiøazených tøíd.
Tomuto se také øíká \textit{soft klasifikace}.
Naproti tomu, kdy¾ je pøi klasifikaci pøiøazen objekt do tøídy \uv{napevno} (ano, patøí tam / ne, nepatøí tam), pak tento zpùsob klasifikace nazýváme \textit{hard klasifikace}.


\section{Klasifikaèní pøístupy}
Existují dva hlavní pøístupy pøi øe¹ení klasifikaèních úloh:
\begin{itemize}
 \item Probabilistické
 \item Neprobabilistické
\end{itemize}

\subsection{Probabilistické klasifikátory}
Jedním ze zpùsobù jakým lze vytvoøit funkèní klasifikátor je vyu¾ít teorie pravdìpodobnosti.
Klasifikátory fungující na bázi pravdìpodobnostních výpoètù urèují pravdìpodobnosti, se kterými daný objekt spadá do nìkteré tøídy.
Do které tøídy respektive kterých tøíd objekt spadá, je následnì urèeno pomocí pøedem definovaného prahu (threshold).

Mezi probabilistické klasifikaèní metody patøí napøíklad metoda zalo¾ená na Bayesovì teorému -- Bayesovský klasifikátor.
Bayesùv teorém a jeho pou¾ití pro klasifikaci je podrobnì popsán v kapitole \ref{BAYES}.

\subsection{Neprobabilistické klasifikátory}
Kromì probabilistických metod existují také neprobabilistické metody klasifikace.
Neprobabilistické klasifikátory se vìt¹inou sna¾í pøímo vymodelovat klasifikaèní funkci a nevyu¾ívají pro zaøazení objektù do tøíd pravdìpodobnostní výpoèty.
Asi nejznámìj¹í neprobabilistickou metodou je metoda SVM (support vector machines), která se sna¾í nalézt takovou nadrovinu v prostoru pøíznakù, která bude rozdìlovat trénovací data.
Ideální nadrovina rozdìluje data z trénovací mno¾iny tak, ¾e body v prostoru le¾í v opaèných poloprostorech a vzdálenosti v¹ech bodù od roviny jsou co nejvìt¹í.

\section{Klasifikace a clustering}\label{CLASS-APPROACHES-CLUSTERING}
Je¹tì relativnì nedávno byla klasifikace chápána jako podmno¾ina úlohy nazývané clustering.
Je sice pravda, ¾e klasifikace a clustering k sobì mají velmi blízko, a také mnoho technik pou¾ívaných v klasifikaci je mo¾né pou¾ít i v clusteringu, nicménì rozdíl mezi tìmito dvìma úlohami je v tom, ¾e pøi klasifikaci známe pøedem tøídy, do kterých budeme vstup klasifikovat.
U clusteringu tomu tak není.
Clustering dìlí vstupní text podle významných spoleèných pøíznakù ve vstupních datech.
Z definice klasifikace (viz \ref{CLASS-DEF}) víme, ¾e klasifikaèní úloha je definována klasifikaèní funkcí $f$,  která vstupnímu vzorku $x$ podle jeho pøíznakù pøiøadí oznaèení tøídy $y$, do ní¾ vzorek spadá.
Klasifikaèní úlohy v NLP se sna¾í tuto funkci $f$ co nejpøesnìji aproximovat, aby simulovaly její výsledek.
Naproti tomu clustering nemá podobnou funkci jako klasifikace, která by urèovala, jak má vypadat výsledek.
Výsledná struktura tøíd clusteringu je vytvoøena za bìhu metody na základì znakù podobnosti vzorkù z mno¾iny vstupù $x$.

\subsection{Strojové uèení}
Jeliko¾ pro vytvoøení funkèního klasifikátoru je nutné, aby klasifikátor byl nauèen pomocí správné testovací mno¾iny, je strojové uèení s tématem klasifikace velmi úzce spojeno.
Strojové uèení je jedním z podtémat oboru umìlé inteligence, zabývající se vytvoøením algoritmù, umo¾òujících poèítaèovým programùm se uèit.
Algoritmy strojového uèení se dìlí do následujících tøí kategorií:
\begin{itemize}
 \item \textit{Uèení s uèitelem (supervised learning)} -- nejjednodu¹¹í zpùsob uèení. Problematiènost tohoto pøístupu spoèívá v tom, ¾e ve¹kerá data, která se program nauèí, musejí být ruènì anotována èlovìkem, z èeho¾ vyplývá jeho velká èasová nároènost.
 \item \textit{Uèení bez uèitele (unsupervised learning)} -- pøi uèení bez uèitele je uèenému programu pøedána sada trénovacích dat, ve které si uèící se program sám hledá význaèné vlastnosti, na jejich¾ základì poté vytvoøí vlastní klasifikaèní funkci. Jedním z mo¾ných pøístupù k øe¹ení této metody je metoda clustering (viz \ref{CLASS-APPROACHES-CLUSTERING}).
 \item \textit{Pøístup na pomezí mezi uèením s uèitelem a uèením bez uèitele (semi-supervised learning)} --  je metoda zalo¾ená na uèení pomocí vstupních trénovacích dat, kterých následnì metoda vyu¾ije k automatickému vytvoøení dal¹ích trénovacích dat.
\end{itemize}

\subsection{Boosting}
V roce 1988 vyslovil Michael Kearns ve své práci s názvem \uv{Thoughts on hypothesis boosting} otázku, zda lze z mno¾iny slabých klasifikátorù (takových, které mají nízké hodnoty korelace testovacích a výstupních dat) vytvoøit jeden klasifikátor, který by mìl výraznì lep¹í výsledky.
Ve své práci dokázal, ¾e tomu tak opravdu mù¾e být.
Od té doby bylo vyvinuto více algoritmù pro boosting klasifikace, nicménì vìt¹ina z nich se zakládá na iterativním uèení mno¾iny slabých klasifikátorù a jejich následném sdru¾ení do jednoho pøesného klasifikátoru.
Historicky nejvýznamnìj¹ím boostingovým algoritmem je zøejmì algoritmus \textit{AdaBoost}(adaptive boosting), vytvoøený Yoavem Freundem a Robertem Schapirem, který publikovali v práci \uv{A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting}\cite{boosting}.

\chapter{Bayesovský klasifikátor} \label{BAYES}
Bayesovský klasifikátor (naivní Bayesovský klasifikátor) je klasifikátor postavený na zjednodu¹eném Bayesoì teorému.
Zjednodu¹ení spoèívá v tom, ¾e existence nebo naopak neexistence jednoho tokenu (slova nebo ostatních pøíznakù -- viz \ref{TOKENIZACE}) není závislá na existenci nebo neexistenci jiného tokenu.
Tudí¾, i kdy¾ tokeny na sobì závisejí, Bayesovský klasifikátor s nimi pracuje jako se zcela nezávislými událostmi.
Tato vlastnost naivního Bayesovského filtrování je nevýhodou pro klasifikování pøirozeného jazyka pøedev¹ím proto, ¾e slova se v jazyce vyskytují ve slovních spojeních, je¾ stejnì jako samotná slova napomáhají klasifikaci.
Aèkoliv tato vlastnost Bayesovský klasifikátor omezuje, jeho vyu¾ití na reálných textech se osvìdèilo a je hojnì pou¾íván.
V posledních letech ale ji¾ existují jiné a lep¹í metody pro klasifikaci, napøíklad metoda \textit{support vector machines}, která je v této práci podrobnì popsána.

\section{Matematický model}
Bayesovský klasifikátor vyu¾ívá Bayesova teorému, který zní následovnì:
\begin{theorem}
  Mìjme dva náhodné jevy $A$ a $B$ s pravdìpodobnostmi $P(A)$ a $P(B)$, pøièem¾ $P(B) > 0$. Potom platí:
  \begin{equation}
    P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} \label{SF-BAYES-MAT-bayes_theorem}
  \end{equation}
\end{theorem}
\begin{proof}
  Dle podmínìných pravdìpodobností platí, ¾e pravdìpodobnost dvou událostí $A$ a $B$ -- $P(A \bigcap B)$ se rovná pravdìpodobnosti $A$ krát pravdìpodobnost $B$ za pøedpokladu, ¾e nastalo $A$ -- $P(B|A)$.
  \begin{equation}
    P(P(A \cap B)) = P(A) \cdot P(B|A)
  \end{equation}  
  Dále také platí, ¾e pravdìpodobnost $A$ a $B$ se rovná pravdìpodobnosti $B$ krát pravdìpodobnost $A$ za pøedpokladu ¾e nastalo $B$:
  \begin{equation}
    P(P(A \cap B)) = P(B) \cdot P(A|B)
  \end{equation}
  Z tìchto dvou vztahù vychází:
  \begin{equation}
    P(B) \cdot P(A|B) = P(A) \cdot P(B|A)
  \end{equation}
  Upravením této rovnice poté dostáváme:
  \begin{equation}
    P(A|B) = \frac{P(A) \cdot P(B|A)}{P(B)}
  \end{equation}
  Co¾ je Bayesùv teorém.
  \hfill \textbf{Q.E.D}.
\end{proof}

Pro klasifikaci textu není vhodné pou¾ít pøímo rovnici z Bayesova teorému, jeliko¾ by bylo tøeba zapamatovat si pro ka¾dý token tøi hodnoty a bylo by nutné provádìt velké mno¾ství výpoètù, nicménì je mo¾né rovnici upravit do tvaru, v nìm¾ si pro ka¾dý token vìty je tøeba pamatovat pouze jednu hodnotu pravdìpodobnosti a také není tøeba provádìt tolik výpoètù.
Upravená rovnice má následující tvar:

\begin{equation}
P = \frac{p_1 p_2 p_3 \ldots p_N}{p_1 p_2 p_3 \ldots p_N + (1 - p_1) (1 - p_2) (1 - p_3) \ldots (1 - p_N)} \label{equation_used} \\
\end{equation}
kde $P$ je pravdìpodobnost urèující míru nále¾itostí klasifikované zprávy do tøídy spam a $p_i, i=1 \ldots N$ udává tuto míru pro jednotlivé tokeny $i$.
Výsledné $P$ je potom porovnáno s urèitým prahem, který definuje, zda je oklasifikovaná zpráva spam nebo ham.


\subsection{Odvození rovnice pro klasifikaci} \label{SF-BAYES-MAT-bayes_theorem-deriv}
Nyní pøejdìme k odvození rovnice \ref{equation_used} z Bayesova teorému \ref{SF-BAYES-MAT-bayes_theorem}.
 
Mìjme $X$ a $Y$, pøedstavující dva tokeny, $S$ znamenající \uv{je to spam} a $\neg S$ znamenající \uv{je to ham} (=není to spam).
Pro zjednodu¹ení budeme pou¾ívat pøípad se dvìma tokeny.

$$
 P(S | X \cap Y) = \frac{P(X \cap Y | S) \cdot P(S)}{P(X \cap Y)}
$$

Nyní vyu¾ijeme toho, ¾e platí

$$
P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)
$$

Tudí¾:
\begin{equation}
 P(S | X \cap Y) = \frac{P(X \cap Y | S) \cdot P(S)}{(P(S)*P(X \cap Y | S) + P(\neg S) \cdot P(X \cap Y | \neg S))} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq1}
\end{equation}
Nyní aplikujeme onu naivitu, kterou jsme zmiòovali vý¹e \ref{BAYES}.

To znamená, ¾e budeme pøedpokládat, ¾e pravdìpodobnost $X$ je nezávislá na pravdìpodobnosti $Y$, tak¾e mù¾eme pou¾ít vztah:
  $$P(A \cap B) = P(A) \cdot P(B)$$
Z toho vyplývá, ¾e rovnici \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq1} mù¾eme upravit do tvaru
\begin{equation}
  P(S | X \cap Y) = \frac{P(X|S) \cdot P(Y|S) \cdot P(S)}{P(S) \cdot P(X|S) \cdot P(Y|S) + P(\neg S) \cdot P(X| \neg S) \cdot P(Y| \neg S)} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq2}
\end{equation}
Vyjdeme opìt z Bayesova teorému, který øíká, ¾e platí:
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$
Toho vyu¾ijeme a dosadíme do rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq2}, dostaneme tedy:
\begin{equation}
  P(S | X \cap Y) = \frac{\frac{P(S|X) \cdot P(X)}{P(S)} \cdot \frac{P(S|Y) \cdot P(Y)}{P(S)} \cdot P(S)}{P(S) \cdot \frac{P(S|X) \cdot P(X)}{P(S)} \cdot \frac{P(S|Y) \cdot P(Y)}{P(S)} + P(\neg S) \cdot \frac{P(\neg S|X) \cdot P(X)}{P(\neg S)} \cdot \frac{P(\neg S|Y) \cdot P(Y)}{P(\neg S)}} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq3}
\end{equation}
Z rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq2} mù¾eme odstranit $P(X)$ a $P(Y)$:
$$
  P(S | X \cap Y) = \frac{\frac{P(S|X)}{P(S)} \cdot \frac{P(S|Y)}{P(S)} \cdot P(S)}{P(S) \cdot \frac{P(S|X)}{P(S)} \cdot \frac{P(S|Y)}{P(S)} + P(\neg S) \cdot \frac{P(\neg S|X)}{P(\neg S)} \cdot \frac{P(\neg S|Y)}{P(\neg S)}}
$$
Po zjednodu¹ení dostaneme:
\begin{equation}
  P(S | X \cap Y) = \frac{\frac{P(S|X) \cdot P(S|Y)}{P(S)}}{\frac{P(S|X) \cdot P(S|Y)}{P(S)} + \frac{P(\neg S|X) \cdot P(\neg S|Y)}{P(\neg S)}} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq4}
\end{equation}
Nyní mù¾eme pøistoupit k závìreèné úpravì, která ov¹em pøedpokládá, ¾e zprávy v uèící mno¾inì jsou rovnomìrnì rozlo¾eny, tzn. ¾e zhruba 50\% nauèených zpráv je spam a zhruba 50\% je ham.
Potom  platí, ¾e $P(S) \approx P(\neg S)$, a tak dostaneme z rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq4} rovnici následující:
\begin{equation}
  P(S | X \cap Y) = \frac{P(S|X) \cdot P(S|Y)}{P(S|X) \cdot P(S|Y) + P(\neg S|X) \cdot P(\neg S|Y)} 
\end{equation}
kde $P(\neg A) = 1 - P(A)$ tudí¾ $P(\neg A|B)$ mù¾eme pøepsat na $1-P(A|B)$.
Z toho získáme rovnici \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq5} ekvivalentní s rovnicí \ref{equation_used}.
\begin{equation}
  P(S | X \cap Y) = \frac{P(S|X) \cdot P(S|Y)}{P(S|X) \cdot P(S|Y) + (1 - P(S|X)) \cdot (1- P(S|Y))} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq5}
\end{equation}
Jestli¾e není uèící mno¾ina vyvá¾ená (podobné mno¾ství spamu a hamu), je vhodné pro klasifikaci pou¾ít rovnici \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq4}, proto¾e pøi pou¾ití rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq5} mù¾e docházet ke zkreslení klasifikace.

\section{Princip Bayesovského klasifikátoru} \label{SF-BAYES-PRINCIP}
Bayesovský klasifikátor si do databáze pøi uèení ukládá pravdìpodobnosti zda jsou jednotlivé tokeny ze vstupních vìt spam, tzn. ukládá si hodnotu  $P(S|X)$.
Po nauèení dostateènì velkého objemu trénovacích dat potom vyu¾ívá tyto pravdìpodobnosti pro výpoèet, zda je klasifikovaný vstupní text spam, nebo ham.

Klasifikace tedy probíhá tak, ¾e vstupní text $X$ se rozdìlí na tokeny $X_i$.
Klasifikátor se podívá do databáze a zjistí pravdìpodobnost $P(S|X_i)$ se kterou jednotlivé tokeny nále¾í do tøídy spamu.
Jestli¾e klasifikátor pøi klasifikaci textu narazí na token, který dosud není v jeho databázi, pak je mu pøiøazena hodnota 0,5 (tzn. klasifikátor neumí urèit s jakou pravdìpodobností je tento token spam èi ham).
Pravdìpodobnost $P(X)$ ¾e vstupní text je spam je potom vypoèten pomocí dosazení $P(S|X_i)$ do rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq5}.

\section{Pøíklad klasifikace}
Uveïme si nyní jednoduchý pøíklad klasifikace textu:
mìjme vstupní vìtu \textit{'Aaaa, my stomach hurts.'}.
Tuto vìtu si klasifikátor rozdìlí podle zadaného pravidla na seznam tokenù (v tomto pøípadì pouzze slov).
$$['Aaaa', 'my', 'stomach', 'hurts']$$
V databázi klasifikátoru máme napøíklad:
$$\{'my':0.6, 'stomach':0.2, 'hurts':0.2\}$$
V¹imnìme si, ¾e v databázi tokenù klasifikátoru se nevyskytuje slovo \texttt{'Aaaa'}.
To znamená, ¾e klasifikátor se pøi svém uèení s tímto slovem doposud nesetkal, a tak mu pøiøadí pravdìpodobnost 0,5.
Jeho databáze tokenù pro klasifikaci vstupní vìty \textit{'Aaaa, my stomach hurts.'} bude tedy vypadat následovnì:
$$\{'my':0.6, 'stomach':0.2, 'hurts':0.2, 'Aaaa':0.5\}$$
Nyní u¾ má klasifikátor v¹e potøebné k tomu, aby vypoèítal pravdìpodobnost, zda je vstupní text spam èi nikoliv.
Bude postupovat podle rovnice \ref{equation_used}.
\begin{align*}
P &= \frac{p_1 p_2 p_3 \ldots p_N}{p_1 p_2 p_3 \ldots p_N + (1 - p_1) (1 - p_2) (1 - p_3) \ldots (1 - p_N)} \\
P &= \frac{0.2 \cdot 0.6 \cdot 0.2 \cdot 0.5}{0.2 \cdot 0.6 \cdot 0.2 \cdot 0.5 + (1 - 0.2) \cdot (1 - 0.6) \cdot (1 - 0.2) \cdot (1 - 0.5)} \\
P &= 0.0857
\end{align*}
Dle tohoto výpoètu se tedy pravdìpodobnost, ¾e vìta \textit{'Aaaa, my stomach hurts.'} je spam, rovná 0,0857.
Co¾ znamená, ¾e vìta je rozhodnì ham.

\chapter{SVM klasifikátor} \label{SVM}
Klasifikaèní metoda pomocí SVM (support vector machines) je spoleènì s vý¹e popsaným Bayesovským klasifikátorem dal¹í klasifikaèní metodou, kterou sev této práci zabýváme.
Jedná se o metodu uèení s uèitelem, která je v souèasné dobì nejvíce pou¾ívanou metodou v NLP.
Mimo NLP má v¹ak také velkou ¹kálu vyu¾ití, a» u¾ pro klasifikaèní úèely, nebo tøeba napøíklad pro úèely regresní analýzy (pro pøedvídání chování finanèních trhù, atd.).

V této práci nebude mo¾né popsat problematiku klasifikátoru support vector machines do úplných detailù, nicménì zde budou popsány základní principy jeho fungování v rozsahu potøebném pro pozdìj¹í implementaci této metody.
Pro podrobnìj¹í popis SVM doporuèuji knihu Vladimira N. Vapnika z roku 1998 s názvem \uv{Statistical Learning Theory} \cite{Vapnik1998} a èlánek \uv{Support-Vector Networks} \cite{Vapnik1995}.

\section{Obecné}
SVM klasifikátor je metoda dosahující velmi dobrých výsledkù ve velkém mno¾ství odvìtví a aplikací.
Poprvé byl tento algoritmus pøedstaven v roce 1995 ve èlánku Vladimira N. Vapnika a kolektivu s názvem \uv{Support-Vector Networks} \cite{Vapnik1995}.
Metoda SVM byla navrhnuta jako klasifikaèní algoritmus.
Aby bylo mo¾né metodu SVM vyu¾ít pro klasifikaci, musíme ji nejprve nauèit pomocí urèité sady dat, která je rozdìlena na trénovací a testovací data.
Ka¾dý jeden záznam v trénovacích a testovacích datech má definovanou jistou hodnotu (label), která urèuje, do které tøídy daný záznam spadá.
Ka¾dý záznam se skládá z nìkolika tokenù, pomocí kterých se SVM uèí a následnì klasifikuje.
Napøíklad klasifikujeme-li pomocí SVM text tìmito tokeny budou mimo jiné jednotlivá slova, délka textu, e-mailové adresy, Twitter tagy atd.

Nyní pøejdìme k matematickému popisu problematiky SVM.
Pøedpokládejme urèitý klasifikaèní problém a jistou datovou sadu, ve které je mno¾ství záznamù, které patøí buï do pozitivní (ham), nebo negativní (spam) tøídy. 
Trénovací sada $X$ pro tento problém obsahuje $l$ záznamù.
Jeden záznam v této sadì je tedy definován jako vektor $x_i \in \mathbb{R}^n$ kde $1 \leq i \leq l$.
Ka¾dý tento vektor $x_i$ se skládá z mno¾iny pøíznakù $[x_1, x_2,\ldots,x_n] \in x_i$, kde $x_1,\ldots,x_n$ jsou jednotlivé atributy daného záznamu.
Label $y$ ka¾dého záznamu z dat je definován jako $y \in \{1,-1\}$ v závislosti na tom, do které tøídy daný záznam spadá.
Trénovací sadu dat lze tedy zapsat jako:

\begin{equation}
  X = \{(x_i,y_i)\}_{i=1}^{l}
\end{equation}
\begin{equation}
  x \in \mathbb{R}^{n}, x_i = [x_1, x_2, \ldots, x_n], i \in 1, \ldots |X|
\end{equation}
\begin{equation}
  y \in \{1,-1\}
\end{equation}

Cílem SVM klasifikátoru je najít rozhodovací hranici, která rozdìluje pøíznaky v trénovacích datech tak, aby rozdìlení odpovídalo jednotlivým tøídám a souèasnì se sna¾í o maximalizaci vzdálenosti v¹ech bodù od rozhodovací hranice. 
Rozhodovací hranice je definována jako \emph{hyperplocha (nadrovina)}, co¾ je prostor s $n$ dimenzemi, který dìlí prostor s $n+1$ dimenzemi do dvou podprostorù.
Libovolnou hyperplochu lze definovat jako \emph{diskriminaèní funkci (discriminant function)}\footnote{diskriminaèní funkce je funkce, která optimálnì dìlí prostor pøíznakù. \cite{Seong-Wook}} v následující formì:

\begin{equation}
  f(x) = w^T x + b
\end{equation}


$b$ je takzvaný \emph{bias} a $w^T x$ definuje skalární souèin mezi váhovým vektorem $w$ a vektorem pøíznakù $x$.
Tento skalární souèin je definován jako:

\begin{equation}
  w^T x = \sum_{j=1}^{n}w_j x_j
\end{equation}

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=10cm,keepaspectratio]{fig/hyperplanes}
      \caption{Nìkolik hyperploch v prostoru $R^2$ s vektorem $ w^T x$} 
      \label{SVM-THEORY-HYPERPLANES}
    \end{center}
\end{figure}


\section{Nelineární SVM a jádra}
Jeliko¾ je u základního klasifikátoru SVM hyperplocha dìlící prostor pøíznakù lineární, pova¾ujeme tento klasifikátor za lineární.
Proto¾e je lineární dìlicí hyperplocha základního SVM klasifikátoru omezující, je mo¾né pou¾it nelineární dìlící hyperplochu, její¾ pomocí mù¾e klasifikátor dosahovat daleko lep¹ích výsledkù.
Problémem takto nelineárních klasifikátorù ale je zvy¹ování komplexicity výpoètù pøi uèení vìt¹ích poètù pøíznakù (vysoká dimenzionalita dat).
Pro vyøe¹ení vý¹e zmínìného problému byly klasické lineární SVM klasifikátory roz¹íøeny o podporu takzvaných \emph{jaderných funkcí (kernel functions)} za pomocí nich¾ jsou lineární klasifikátory schopny vytvoøit nelineární dìlící hyperplochy.
Pro vysvìtlení zpùsobu, jakým se této nelinearity dosáhne pøedpokládejme klasický lineární klasifikátor.

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=13cm,keepaspectratio]{fig/XtoFTransformation}
      \caption{Transformace vstupního prostoru $X$ do prostoru pøíznakù $F$ pomocí mapovací funkce $\varphi(x)$. $f(x)$ je znázornìní diskriminaèní funkce} 
      \label{SVM-KERNEL-TRANSFORMACE}
    \end{center}
\end{figure}

Nyní pøevedeme trénovací sadu $X$, také známou jako \emph{vstupní prostor (input space)} do vysoce dimenzionálního prostoru pøíznakù $F$ za pomocí nelineární mapovací funkce $\varphi : X \rightarrow F$ viz. obrázek \ref{SVM-KERNEL-TRANSFORMACE}.
Pro prostor pøíznakù tedy budeme mít následující diskriminaèní funkci:

\begin{equation}
  f(x) = w^T \varphi(x) + b
\end{equation}

Jeliko¾ je pøi výpoètu diskriminaèní funkce $f(x)$ tøeba vypoèíst mapovací funkci $\varphi(x)$, velmi výraznì roste nároènost jejího výpoètu s poètem dimenzí vstupù.
Mìjme kupøíkladu následující mapovací funkci:

\begin{equation}
  \varphi(x) = (x^2_1, \sqrt{2}x_1 x_2, x^2_2)^T
\end{equation}

Z této mapovací funkce jsme nyní schopni spoèítat diskriminaèní funkci v prostoru pøíznakù:

\begin{equation}
  f(x) = w_1 x^2_1 + \sqrt{2}w_2 x_1 x_2 + w_1 x^2_2 + b
\end{equation}

Je zcela zøejmé, ¾e tento zpùsob výpoètu je neúnosný.
Museli bychom toti¾ pro výpoèet transformovat celý vstupní prostor do prostoru pøíznakù, co¾ by kvadraticky zvý¹ilo èasovou a pamì»ovou nároènost klasifikátoru.
Tento zpùsob tedy není vhodný.
Existuje v¹ak øe¹ení, kterým lze spoèítat diskriminaèní funkci $f(x)$, ani¾ bychom museli znát, chápat a poèítat mapovací funkci do prostoru $F$.
Pro toto øe¹ení je tøeba si vyjádøit váhový vektor $w$ jako lineární kombinaci jednotlivých záznamù z tréninkové sady.

\begin{equation}
  w = \sum^{n}_{i=1}\alpha_i x_i
\end{equation}

Z toho plyne, ¾e diskriminaèní funkce vstupního prostoru $X$ je:

\begin{equation}
  f(x) = \sum^{n}_{i=1}\alpha_i x^T_i \cdot x + b
\end{equation}

a diskriminaèní funkce prostoru pøíznakù $F$ je:

\begin{equation}
  f(x) = \sum^{n}_{i=1}\alpha_i \varphi(x_i)^T \varphi(x) + b
\end{equation}

Existence tìchto dvou reprezentací diskriminaèní funkce pro vstupní prostor $X$ a pro prostor pøíznakù vzhledem k promìnné $\alpha_i$ bývá nazývána takzvanou \emph{duální reprezentací (dual representation)}. 

Kartézský souèin $\varphi(x_i)^T \varphi(x)$, kde $x_i, x \in X$ tedy pøedstavuje takzvanou \emph{jadernou funkci (kernel function)}.
Jaderná funkce je tedy definována jako:

\begin{equation}
  k(x,z) = \varphi(x_i)^T \varphi(x)
\end{equation}

Jeliko¾ jadernou funkci lze vypoèítat pouze pomocí pøíznakù ve vstupním prostoru, je mo¾né vypoèítat diskriminaèní funkci, ani¾ bychom znali potøebnou mapovací funkci.
Díky této vlastnosti jaderných funkcí nemusíme transformovat celý vstupní prostor do nového prostoru pøíznakù $F$, ale spoèítáme pouze jadernou funkci pro jednotlivé pøíznaky.
Tudí¾ nám poèet dimenzí prostoru $F$ neovlivní komplexnost výpoètu.
Vý¹e zmínìná operace, kdy vyu¾ijeme pouze kartézský souèin bodù v daném prostoru, bývá èasto v literatuøe nazývána jako takzvaný \emph{Kernel trick}.
Pro výpoèet diskriminaèní funkce $f(x) = w^T \varphi(x) + b$ tedy pou¾ijeme jadernou funkci $k(x,z) = \varphi(x)^T \varphi(z)$ a dostaneme $f(x) = k(x,z) + b$.

Nyní si nejprve si znázorníme na pøíkladu nároènost pøi pøevádìní v¹ech souøadnic do prostoru $F$.
Pøedpokládejme prostor $X \in \mathbb{R}^2$ (tzn. $x = (x_1, x_2)$) dále pak polynomiální mapovací funkci druhého øádu

\begin{equation}
\varphi(x) = (1, x_1, x_2, x^2_1, x^2_2, x_1 x_2)
\end{equation}

která pøevede vektor pøíznakù ze vstupního prostoru $X$ do prostoru $F$.
Abychom nyní pomocí této mapovací funkce vypoèítali jadernou funkci, musíme oba body pøevést do prostoru $F$ a provést skalární souèin v prostoru $F$.
Tudí¾:

\begin{equation}
  K(x, z) = \varphi(x)^T \varphi(z)
\end{equation}
\begin{equation}
  K(x, z) = 1 + x_1 z_1 + x_2 z_2 + x^2_1 z^2_1 + x^2_2 z^2_2 + x_1 z_1 x_2 z_2
\end{equation}

Nyní si pøedstavme, ¾e nemáme dvoudimenzionální prostor, ale velmi vysoce dimenzionální prostor.
Pøevádìní $x$ a $z$ do prostoru $F$ by bylo velmi výpoèetnì nároèné.

Nyní si na pøíkladu pøedvedeme pou¾ití vý¹e zmínìné metody \emph{kernel trick}, která nám umo¾ní vypoèíst jadernou funkci, ani¾ bychom potøebovali spoèítat transformace do prostoru pøíznakù $F$.
Pøedpokládejme jadernou funkci:

\begin{equation}
  K(x, z) = (1 + x^T z)^2
  \label{SVM-KERNEL-2POLYNOMIAL}
\end{equation}

kterou roznásobíme a dostaneme:

\begin{equation}
  K(x, z) = 1 + x^2_1 z^2_1 + x^2_2 z^2_2 + 2 x_1 z_1 + 2 x_2 z_2 + 2 x_1 z_1 x_2 z_2
\end{equation}

Co¾ je skuteènì skalární souèin definovaný v prostoru pøíznakù s mapovací funkcí 

\begin{equation}
\varphi(x) = (1, x^2_1, x^2_2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)
\end{equation}

Tím jsme dokázali, ¾e opravdu není tøeba pøevádìt celý prostor pøíznakù, ale staèí nám vypoèítat jednoduchou funkci $K(x, z) = (1 + x^T z)^2$, a tím dosáhnout daleko jednodu¹eji ký¾eného výsledku.

\subsection{Jádra} \label{SVM-KERNELS}
\label{SVM-THEORY-KERNELS}
Jádra, nebo také jaderné funkce, jsou tedy funkce, pomocí nich¾ je klasifikátor SVM  schopen vypoèítat diskriminaèní rovnici pro optimální rozdìlení prostoru pøíznakù.
V závislosti na tom jaké jádro je pøi trénování klasifikátoru pou¾ito, je klasifikátor lineární nebo nelineární a dosahuje rozdílných výsledkù na stejných datech.

V praxi je pou¾íváno nìkolik rùzných jader, nejèastìji \emph{RBF (radial basis function)}, \emph{polynomiální jaderná funkce} a nìkdy je pou¾ívána i základní \emph{lineární jaderná funkce}.

\subsubsection{Lineární jaderná funkce}
Lineární jaderná funkce je nejzákladnìj¹í ze v¹ech jaderných funkcí.
Poèítá toti¾ skalární souèin z bodù ve vstupním prostoru, tudí¾ je schopna pouze lineární klasifikace.
Lineární jaderná funkce je definována následovnì:

\begin{equation}
  K(x, z) =  x^T z
\end{equation}

\subsubsection{Polynomiální jaderná funkce}
Rovnice \ref{SVM-KERNEL-2POLYNOMIAL} popisuje tedy polynomiální jadernou funkci druhého øádu, nicménì v praxi jsou èasto pou¾ívány polynomiální jaderné funkce vy¹¹ích øádù $Q$, obecnì definované jako:

\begin{equation}
  K(x, z) = (x^T z + c)^Q
\end{equation}
kde $c \geq 0$ urèuje vliv vy¹¹ích a ni¾¹ích øádù polynomu.

Polynomiální jaderné funkce jsou v NLP pomìrnì hodnì oblíbené \cite{ACLShort}.
Pøi pou¾ití jaderné funkce vy¹¹ích øádù, ale mù¾e nastat takzvaný overfitting, kdy se SVM klasifikátor nauèí na velmi specifické pøíznaky, které nejsou pro korektní klasifikaci smìrodatné, co¾ zpùsobí markantní zhor¹ení schopností SVM klasifikátoru.
Z tohoto dùvodu je nejèastìji pou¾ívána polynomiální jaderná funkce 2. a 3. øádu, která není schopna tak dokonale kopírovat tvar kolem pøíznakù.

\subsubsection{Radiální jaderná funkce}
Radiální jaderná funkce, èasto oznaèovaná jako \emph{RBF jaderná funkce}, nebo \emph{RBF kernel} je dal¹í velmi oblíbenou jadernou funkcí, zalo¾enou na radiální bázové funkci \emph{RBF (radial basis function)}
Je definována jako:
 
\begin{equation}
  K(x, z) = exp(-\frac{||x-z||^2}{2\gamma^2})
\end{equation}

Obèas je také pou¾ívána jiná forma definice pro RBF jaderné funkce, nicménì základní my¹lenka, ¾e èím vzdálenìj¹í bod v prostoru pøíznakù, tím men¹í má vliv na rozhodování, zùstává ve v¹ech tìchto definicích zachována.
Promìnná $\gamma$, pro kterou musí platit $\gamma \geq 0$, definuje jak moc bude klasifikátor brát v úvahu vzdálenìj¹í body.
Nevhodným výbìrem této promìnné mù¾e podobnì jako v pøípadì polynomiální jaderné funkce dojít k overfittingu, proto je dùle¾ité zvolit její vhodnou hodnotu.

\subsubsection{Dal¹í jádra}
Tato tøi vý¹e zmínìná jádra/jaderné funkce samozøejmì nejsou jediná.
Kdykoliv je mo¾né vytvoøit novou jadernou funkci, která bude popisovat nìjaký jiný prostor.
Taková jaderná funkce, kterou vytvoøíme, v¹ak nemusí popisovat ¾ádný reálný prostor.
Existují tøi pøístupy, pomocí kterých se mù¾eme ujistit, ¾e námi vytvoøená jaderná funkce skuteènì nìjaký reálný prostor popisuje:
\begin{itemize}
\item \emph{Konstrukce} -- jádro zkonstruujeme z transformaèní funkce $\varphi(x)$.
\item \emph{Matematické podmínky jádra (Mercerovy podmínky)} -- pokud jaderná funkce $K(x, z)$ splòuje následující dvì podmínky, pak existuje prostor, který je danou jadernou funkcí popsán:
 \begin{itemize}
  \item $K(x, x^{'})$ musí být symetrická funkce (tzn. $K(x, x^{'}) = K(x^{'}, x)$,$\forall x, x^{'} \in X$)
  \item pro matici:
  	$\begin{bmatrix}
       K(x_1, x_1) & K(x_1, x_2) & \cdots  & K(x_1, x_N)  \\[0.3em]
       K(x_2, x_1) & K(x_2, x_2) & \cdots  & K(x_2, x_N)  \\[0.3em]
       \cdots      & \cdots      & \cdots  & \cdots       \\[0.3em]
       K(x_N, x_1) & K(x_N, x_2) & \cdots  & K(x_N, x_N)  \\[0.3em]
    \end{bmatrix}$ \\
    musí platit, ¾e je pozitivnì semi-definitní pro libovolné $x_1,\dots, x_N \in X$
 \end{itemize} 
\item \emph{Do we even care?} -- tento pøístup je velmi zvlá¹tní, nicménì bývá obèas také pou¾íván.
	Sopèívá ve vytvoøení libovolné jaderné funkce a následné aplikaci v klasifikátoru SVM a pokud klasifikace funguje, nehraje roli, ¾e dané jádro nepopisuje ¾ádný reálný prostor.
	Tento pøístup v¹ak rozhodnì není doporuèitelným.
\end{itemize}

\section{Vytvoøení rozhodovací hranice}
Jak ji¾ bylo vý¹e zmínìno, SVM klasifikátor se pøi uèení sna¾í vytvoøit optimální rozhodovací hranici, tedy hyperplochu, tak, aby dìlila prostor pøíznakù na dva podprostory, kde ka¾dý obsahuje objekty v¾dy pouze z jedné tøídy a v¾dy se sna¾í maximalizovat vzdálenost rozhodovací hranice ode v¹ech bodù v prostoru.


\begin{figure}[H]
    \begin{center}
      \includegraphics[width=10cm,keepaspectratio]{fig/margins}
      \caption{Ukázka mo¾ných marginù, vlevo maximální -- ¹iroký; vpravo tenký} 
      \label{SVM-THEORY-MARGINS}
    \end{center}
\end{figure}

Pøedpokládejme diskriminaèní funkci $f(x)$, která definuje urèitou hyperplochu, lineárnì rozdìlující prostor $X$ na pozitivní a negativní tøídy.
Nejbli¾¹í vektory $x \in X$ k této hyperplo¹e jsou oznaèeny $x_+$ pro pozitivní a $x_-$ pro negativní tøídy.
Definujeme takzvaný \emph{geometrický margin} hyperplochy $f$ na trénovacích datech $X$, co¾ je \emph{maximální ¹íøka (margin)} volného prostoru mezi rozdìlovací hyperplochou $f$ a nejbli¾¹ími body v prostoru $X$ (viz obrázek \ref{SVM-THEORY-MARGINS}).
Margin je definován jako:

\begin{equation}
  m_x(f) = \frac{1}{2} \widehat{w}^T (x_+ - x_-)
\end{equation}

kde vektor $\widehat{w}$ je jednotkový vektor vektoru $w$ a body $x_+$ a $x_-$ jsou od hyperplochy stejnì daleko, co¾ znamená, ¾e existuje nìjaká konstanta $\alpha > 0$, pro kterou platí:

\begin{equation}
  f(x_+) = w^T  x_+ = \alpha
\end{equation}
\begin{equation}
  f(x_-) = w^T  x_- = -\alpha
\end{equation}

Nyní nastavíme hodnotu promìnné $\alpha$ na $\alpha = 1$ a po úpravách získáme následující definici maximálního marginu:

\begin{equation}
  m_X(f) = \frac{1}{||w||}
\end{equation}

Abychom tedy dostali optimální rozdìlovací hranici s maximálním marginem, budeme muset maximalizovat hodnotu maximálního marginu.
Tato operace je ov¹em ekvivalentní s minimalizací $\frac{1}{2}||w||^2$.
Nalezení specifické diskriminaèní funkce s maximálním geometrickým marginem je tedy ekvivalentní s následujícím optimalizaèním problémem:

\begin{equation}
\begin{aligned}
& \underset{w,b}{\text{minimalizace}}
& & \frac{1}{2}||w||^2 \\
& \text{kde}
& & y_n(w^T x_i + b) \geq 1, & n = 1,\ldots,l
\end{aligned}
\end{equation}

Tato optimalizace pøedpokládá, ¾e trénovací mno¾ina $X$ je lineárnì separovatelná.
Podmínka $y_n(w^T x_n + b) \geq 1$ potom zaji¹»uje, ¾e diskriminaèní funkce oklasifikuje v¹echna data v trénovací mno¾inì korektnì.

Nicménì mù¾e nastat problém, ¾e trénovací mno¾ina $X$ nemusí být lineárnì separovatelná.
Aby byl klasifikátor schopný nauèit se i na takové trénovací mno¾inì, musíme jej upravit tak, aby mohl nekorektnì oklasifikovat nìjaké záznamy z trénovací mno¾iny a tím, i za cenu chyb, diskriminaèní funkci najít.
Toto opatøení mù¾e nìkdy pomoci nalézt ¹ir¹í margin, a tím zlep¹it výsledky klasifikátoru oproti pøedchozímu, u¾¹ímu marginu.
Tuto úpravu udìláme za pomocí takzvané \emph{chybové promìnné (slack variable)} $\xi_i$, kterou odeèteme od pravé strany optimalizaèní podmínky -- tzn. umo¾níme chybu.
Takto upravený problém nyní vypadá následovnì:

\begin{equation}
\begin{aligned}
& \underset{w,b}{\text{minimalizace}}
& & \frac{1}{2}||w||^2 \\
& \text{kde}
& & y_n(w^T x_i + b) \geq 1 - \xi_i, & n = 1,\ldots,l
\end{aligned}
\end{equation}

Pokud tedy platí, ¾e $y_i(w^T x_i + b) < 1$ respektive $\xi_i > 1$, pak je záznam $x_i \in X$ ¹patnì klasifikován.
Pokud ale platí, ¾e $0 \leq \xi_i \leq 1$, pak záznam $x_i \in X$ le¾í uvnitø marginu.
Záznam $x_i$ je tedy klasifikován korektnì, jestli¾e platí, ¾e $\xi_i \leq 0$.
Z tohoto vyplývá, ¾e suma v¹ech chybových promìnných reprezentuje míru chybovosti:

\begin{equation}
  \xi(X) = \sum^n_{i=1}\xi_i
\end{equation}
\begin{equation}
  X = \{(x_i,y_i)\}^l_{i=1}
\end{equation}

Abychom byli schopni minimalizovat míru chybovosti (penalizaci za ¹patnou klasifikaci a marginové chyby) vzhledem k maximalizaci marginu, zavedeme konstantu $C > 0$, kterou budeme míru chybovosti pøenásobovat.
Tato konstanta se nazývá \emph{konstanta soft-margin (soft-margin constant)}.
Na¹e optimalizaèní úloha tedy s touto soft-margin konstantou vypadá následovnì:

\begin{equation}
\label{SVM_C}
\begin{aligned}
& \underset{w,b}{\text{minimalizace}}
& & \frac{1}{2}||w||^2 + C \sum^{n}_{i=1}\xi_i \\
& \text{kde}
& & y_n(w^T x_i + b) \geq 1 - \xi_i, & n = 1,\ldots,l
\end{aligned}
\end{equation}

Nyní pro vyøe¹ení tohoto optimalizaèního problému pou¾ijeme metodu Lagrangeových multiplikátorù a získáme optimalizaèní problém:

\begin{equation}
\begin{aligned}
& \underset{\alpha}{\text{maximalizace}}
& & \sum^{n}_{i=1}\alpha_i - \frac{1}{2}\sum^{n}_{i=1}\sum^{n}_{j=1}y_iy_j\alpha_i\alpha_jx^T_i x_j \\
& \text{kde}
& & \sum^n_{i=0} y_i\alpha_i = 0, & 0 \leq \alpha_i \leq C
\end{aligned}
\end{equation}


Skalární souèin $x^T_i x_j$ v maximalizaèní rovnici mù¾eme nahradit libovolnou jadernou funkcí a dosáhnout tím nelineární transformace, a tedy i ¹irokého marginu ve vysoce dimenzionálních prostorech pøíznakù. (viz \ref{SVM-THEORY-KERNELS}).

Díky vyu¾ití Lagrangeovy funkce má výsledek øe¹ení optimalizaèního problému zajímavé vlastnosti.
Bylo napøíklad dokázáno, ¾e takto získané øe¹ení je v¾dy globální díky tomu, ¾e formulace problému je konvexní \cite{Burges1988}.

Zajímavou vlastností support vector machines je, ¾e ne v¹echny vektory z tréninkové mno¾iny se podílejí na výsledném øe¹ení.
Pou¾ijme rovnici pro výpoèet váhového vektoru $w$ získaného derivací pøi øe¹ení Lagrangeovy funkce, a to:

\begin{equation}
  w = \sum^{n}_{i=1}y_i \alpha_i x_i
\end{equation}

V¹echny vektory $x_i$, pro které tedy platí, ¾e  $\alpha_i > 0$, jsou vektory, které le¾í na pomezí marginu, uvnitø marginu, nebo jsou ¹patnì klasifikovány.
Tyto vektory jsou nazývány \emph{support vektory (support vectors)}.
Vektory, které v¹ak mají hodnotu $\alpha_i \leq 0$ nejsou vùbec do øe¹ení zahrnuty. 
Tudí¾ tyto vektory by mohly být zcela odstranìny z tréninkové sady a nemìlo by to ¾ádný vliv na výsledné øe¹ení.
Díky této vlastnosti jsou SVM ménì náchylné k overfittingu a také klasifikaèní model, který tyto vektory tvoøí, je díky tomu velmi malý a rychlý.


\chapter{Tokenizace a výbìr vhodných pøíznakù} \label{TOKENIZACE}
Poèínaje touto kapitolou se budeme zabývat postupy a problémy, které se vyskytly pøi implementaci vytvoøeného klasifikaèního programu.
Následující kapitoly jsou tedy na rozdíl od pøedchozích kapitol zamìøeny praktiètìji.

V této kapitole se popí¹eme tvorbou tokenù, které klasifikátory pou¾ívají pro trénovaní a klasifikaci textu.

Nejprve si ale definujme, co to tokenizace je:

\begin{definition}
Tokenizace je proces rozdìlování vstupního textu do vhodných tokenù (nejèastìji slov, frází, symbolù a jiných pøíznakù).
Seznam takto vytvoøených tokenù se potom pou¾ívá pro dal¹í zpracování (v pøípadì této práce jako vstup klasifikátorù).
\end{definition}

Pro potøeby implementovaného klasifikátoru rozdìlujeme tokeny na dvì skupiny:
\begin{itemize}
	\item Speciální pøíznaky -- v textu se mohou nacházet skryté znaky, které vìt¹inou nejsou klasifikátory bìhem klasifikace schopny zachytit a které mohou výrazným podílem napomoct pøesnosti klasifikace.
Ka¾dý druh klasifikovaného textu ale mù¾e mít citlivost k rùzným znakùm zcela jinou.
Proto je nutné dbát na to, jak vybrat optimální znaky pro daný vstupní text a pou¾itou klasifikaèní metodu.
	\item Textové tokeny -- mimo vý¹e zmínìných speciálních pøíznakù se v textu nacházejí pøíznaky jednotlivých slov, které jsou pro klasifikaci nejdùle¾itìj¹ími nositeli informace.
Proto také velmi zále¾í na výbìru, popøípadì úpravì, vstupních slov a zpùsobu jejich zpracování.
\end{itemize}

Nyní pøistoupíme k detailnìj¹ímu popisu vý¹e popsaných skupin tokenù tak, jak je k nim pøistupováno v praktické èásti této práce.
Následnì popí¹eme práci naivního Bayesovského klasifikátoru a klasifikátoru SVM s vytvoøenými tokeny.

\section{Speciální pøíznaky v textu} \label{TOKENIZACE-SPEC}
V této èásti práce budeme popisovat jednotlivé druhy speciálních pøíznakù, které jsou z textu extrahovány a které jsou následnì pøedávány klasifikátorùm, je¾ s nimi dále pracují.
Tyto speciální pøíznaky jsou rovnì¾ implementovány ve programu vytvoøeném jako souèást této práce.

Speciální pøíznaky v textu jsou mnohdy v textu skryté a klasickými pøístupy pro tokenizaci textu (viz \ref{Textove_tokeny}) je èasto nejsme schopni z textu získat.
Tyto speciální pøíznaky jsou nositeli dùle¾ité informace proto, ¾e mohou z textu napøíklad urèovat náladu pisatele, zobecòovat informace obsa¾né v textu atd., a tím umo¾nit pøesnìj¹í klasifikaci.
Vzhledem k tomu, ¾e takovýchto speciálních pøíznakù je jistì mo¾né definovat velké mno¾ství, je nereálné je popsat a implementovat v¹echny.
V implementovaném programu proto bylo zvoleno nìkolik typù tìchto speciálních pøíznakù, které byly implementovány a budou zde tedy popsány.

V následujícím popisu pøíznaky dìlíme do skupin podle typu informace, kterou se pomocí nich sna¾íme z textu z
ískat.
Tyto skupiny jsou potom dìleny na typy pøíznakù ze skupiny, které popisují jednotlivé varianty implementace této varianty, které jsou v programu vytvoøeném pro tuto práci pou¾ity.

\subsection{URL}
Skupina pøíznakù s názvem URL je skupinou, hledající v klasifikovaném textu internetové odkazy ("http://adresa.com", "ftp://adresa.cz", "adresa.cz/odkaz.html", atd.) a sna¾ící se z tìchto odkazù vytvoøit tokeny co nejvhodnìj¹í pro klasifikaci.
 
Skupina URL má v praktické èásti práce následující mo¾né varianty speciálních pøíznakù:

\begin{itemize}
  \item \emph{Celé URL} -- celé URL nalezené v textu je bráno jako pøíznak.
  \item \emph{Doména URL} -- doména daného URL je brána jako pøíznak (napøíklad: \emph{"http://healthland.time.com/2013/04/02/bird-flu-is-back-in-china-but-this-time-its-h7n9/"} $\longrightarrow$ \emph{"time.com"}).
  \item \emph{Existence URL ANO/NE} -- existuje-li v textu URL, pak se vytvoøí pro daný text pøíznak definující, ¾e se v textu URL vyskytovalo.
  Pokud se v textu ale URL nenachází vytvoøí se namísto pøíznaku \emph{ANO} pøíznak \emph{NE}, definující neexistenci URL v textu.
  \item \emph{Existence URL ANO} -- tento typ pøíznaku URL je variací na pøedchozí pøíznak \emph{Existence URL ANO/NE}.
  Pøíznak se vytvoøí pouze tehdy, pokud se v textu URL nachází.
  Jestli¾e se v textu URL nenachází, pøíznak vytvoøen není. 
  \item \emph{Nepou¾ívat URL} -- tato varianta zcela zru¹í pou¾ívání URL jako pøíznaku pro klasifikaci.
\end{itemize}

\subsection{E-mailové adresy}
Skupina pøíznakù E-mailové adresy sdru¾uje dohromady varianty pøíznakù, pracující s nalezenými e-mailovými adresami v textu.

Mo¾né varianty pøíznakù v této skupinì jsou:
\begin{itemize}
  \item \emph{Celý e-mail} -- celý e-mail nalezený v textu je brán jako pøíznak.
  \item \emph{Existence e-mailu ANO/NE} -- pøíznak definuje, zda se v textu nachází nìjaká e-mailová adresa.
  Pokud ano, je vytvoøen pøíznak \emph{ANO} a pokud ne, je vytvoøen pøíznak \emph{NE}.
  \item \emph{Existence emailu ANO} -- nachází-li se v textu e-mailová adresa, vytvoøí se pøíznak definující existenci této e-mailové adresy.
  Na rozdíl od pøedchozí varianty \emph{Existence e-mailu ANO/NE} se nevytváøí pøíznak NE v pøípadì nenalezení e-mailové adresy v textu.
  \item \emph{Nepou¾ívat e-mailové adresy} -- tato varianta zcela zru¹í pou¾ívání pøíznakù e-ailových adres jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Emotikony}
Dal¹í skupinou pøíznakù, které v této práci rozpoznáváme a pøedáváme klasifikátorùm, jsou pøíznaky emotikonù (smajlíkù).

Tuto skupinu pøíznakù pro potøeby na¹í implementace dìlíme na následující typy:
\begin{itemize}
  \item \emph{Existence jednotlivých emotikonù} -- pro ka¾dý nalezený emotikon je vytvoøen pøíznak.
  \item \emph{Existence emotikonù obecnì ANO/NE} -- jestli¾e se v textu nachází nìjaké emotikony, je vytvoøen pøíznak \emph{ANO}, jestli¾e se nenacházejí, je vytvoøen pøíznak \emph{NE}.
  \item \emph{Existence emotikonù obecnì ANO} -- obdobnì jako \emph{Existence emotikonù obecnì ANO/NE}, pouze s tím rozdílem, ¾e jestli¾e se v textu nenacházejí ¾ádné emotikony, ¾ádný pøíznak se nevytvoøí.
  \item \emph{Nálada emotikonù} -- v textu jsou vyhledávány emotikony a zji¹»uje se jejich nálada. 
  Typy emotikonù jsou rozdìleny do tøech tøíd -- smutné(":-(", ":'(", \ldots), veselé(":-)", ":-P", \ldots ) a ostatní ("o.O", ":-O", \ldots).
  Jestli¾e je v textu nalezen emotikon z vý¹e zmínìných tøíd, pak je vytvoøen odpovídající pøíznak \emph{SAD}, \emph{HAPPY}, nebo \emph{OTHER}.
  Nachází-li se v textu více typù emotikonù, je samozøejmì vytvoøeno více odpovídajících pøíznakù (tzn. napøíklad pokud se v textu nachází následující mno¾ina emotikonù: ":-)", ":-P", ":(", jsou pro daný text vytvoøeny dva pøíznaky \emph{HAPPY} a \emph{SAD}).
  \item \emph{Nepou¾ívat emotikony} -- tato varianta zcela zru¹í pou¾ívání emotikonù jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Tagy}
Takzvané tagy jsou textové znaèky (klíèová slova), které autoøi pøipisují k textu, aby daný text pøiøadili k nìjakému tématu a umo¾nili tak ostatním u¾ivatelùm snáze nalézt pøíspìvky k jimi hledanému tématu.
Tyto tagy bývají ve tvaru \#TAG (napøíklad "\#influenza", "\#sick", \ldots).
Pro potøeby této práce jsme do této skupiny zaøadili také tagy u¾ivatelù, pou¾ívané ve formátu @JMÉNO\_U®IVATELE, které odkazují na zmínìného u¾ivatele.

V této práci rozli¹ujeme v pøípadì této skupiny následující typy:
\begin{itemize}
  \item \emph{Celý tag} -- celý tag nalezený v textu je pou¾it jako pøíznak pro klasifikaci.
  \item \emph{Existence tagù ANO/NE} -- jestli¾e se v textu nacházejí nìjaké tagy, je vytvoøen pøíznak \emph{ANO}, jestli¾e se nenacházejí, je vytvoøen pøíznak \emph{NE}.
  \item \emph{Existence tagù ANO} -- obdobnì jako v pøípadì typu \emph{Existence tagù ANO/NE}, ale pokud ¾ádný tag nebyl nalezen, pøíznak NE není vytvoøen.
  \item \emph{Nepou¾ívat tagy} -- tato varianta zcela zru¹í pou¾ívání tagù jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Vìty}
V pøípadì této skupiny se jedná pouze o jediný typ pøíznaku.
V celém vstupním textu se spoèítá poèet napsaných vìt, ketrý je následnì pou¾it jako pøíznak pro klasifikaci.
Samozøejmì, stejnì jako ve vý¹e popsaných skupinách pøíznakù, i tento pøíznak je mo¾né zcela zru¹it a nepou¾ívat.

\subsection{Èasy}
V textu se velmi èasto nacházejí rùzné èasové údaje, které by mohly velkou mìrou pøispìt lep¹ím výsledkùm na¹eho klasifikátoru.
Proto byla vytvoøena skupina pøíznakù nazvaná \emph{Èasy}, která vytváøí pøíznaky z nalezených èasových informací v textu.

Obsahuje tyto typy:
\begin{itemize}
  \item \emph{Celý èas} -- jako pøíznaky pro klasifikaci se pou¾ijí v¹echny èasové informace nalezené v textu.
  \item \emph{Èas ve 24h formátu} -- èasové informace nalezené v textu jsou konvertovány do 24 hodinového formátu a následnì pou¾ity jako pøíznaky.
  \item \emph{Pouze hodiny ve 24h formátu} -- èasové informace jsou stejnì jako v pøípadì typu \emph{Èas ve 24h formátu} pøevedeny do 24h formátu, ale jako pøíznaky jsou pou¾ity pouze informace o hodinách.
  \item \emph{Nepou¾ívat èasy} -- tato varianta zcela zru¹í pou¾ívání èasù jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Data}
Podobnì jako skupina \emph{Èasy}, funguje i skupina \emph{Data}, která vytváøí pøíznaky z údajù o datu.
V následujícím textu budeme písmenem D oznaèovat dny, písmenem M mìsíce a písmenem Y roky (tzn. datum 15.2.1998 je ve formátu DMY).

Obsahuje tyto mo¾né typy pøíznakù:

\begin{itemize}
  \item \emph{Celé datum} -- jako pøíznak je v pøípadì tohoto typu pøíznakù pou¾ito datum nalezené v textu.
  \item \emph{Datum ve formátu DMY} --v pøípadì tohoto typu je datum nalezené v textu pøevedeno do formátu DMY a v tomto formátu je pou¾ito jako pøíznak (tzn. DMY).
  \item \emph{Datum ve formátu MY} -- podobnì jako v pøedchozím typu je datum pøevedeno do formátu DMY, ale jako pøíznak jsou pou¾ity pouze informace o mìsíci a roku (tzn. MY).
  \item \emph{Datum ve formátu Y} -- opìt podobné jako \emph{Datum ve formátu DMY} ale pro vytvoøení pøíznaku je pou¾it pouze rok z data nalezeného v textu (tzn. Y).
  \item \emph{Nepou¾ívat data} -- tato varianta zcela zru¹í pou¾ívání udajù o datu jako pøíznakù pro klasifikaci.
\end{itemize}

\section{Textové tokeny} \label{Textove_tokeny}
Pro tokenizaci textu obecnì existuje velmi mnoho pøístupù a zpùsobù, zde v¹ak popí¹eme pouze dva -- základní pøístup k tokenizaci textu a alternativní pøístup k tokenizaci textu.
Oba tyto pøístupy je mo¾né je¹tì roz¹íøit o vyu¾ití stematizátorù (pøevedení slov se kterými tokenizátor pracuje na koøeny).


\subsubsection{Základní pøístup k tokenizaci}
Základní tokenizace dìlí vstupní vìtu pouze na jednotlivá slova, která jsou posléze pou¾ita jako tokeny. 

To znamená:

Mìjme vstupní text \textit{'Damned headache. I have to sleep.'}.
Takovýto text se rozdìlí na slova, tzn. vznikne následující seznam slov:

$['Damned', 'headache', 'I', 'have', 'to', 'sleep']$.

Takto vytvoøená slova jsou tedy tokeny, s nimi¾ se klasifikátor uèí, nebo které klasifikuje.

\subsubsection{Základní pøístup k tokenizaci se stematizací}
Tento pøístup je velmi podobný pøedchozímu.
Li¹í se od nìj pouze tím, ¾e slova, která jsou vytvoøena rozdìlením pomocí stematizaèního algoritmu pøevede na koøeny (=stemateizace).
Tímto krokem se sní¾í poèet slov ve slovníku klasifikátoru.
To zpùsobí, ¾e u Bayesovského klasifikátoru dojde k výraznému zmen¹ení velikosti slovníku vytvoøeného pøi uèení a u klasifikátoru SVM se sní¾í poèet dimenzí pøíznakù a zjednodu¹í se tak nalezení optimální nadroviny.

To znamená:

Mìjme vstupní text \textit{'Damned headache. I have to sleep.'}.
Takovýto text se rozdìlí na slova, tzn. vznikne následující seznam slov:

$['Damned', 'headache', 'I', 'have', 'to', 'sleep']$.

Nyní v¹echna tato slova pøevedeme na tvar koøene (stematizace), èím¾ získáme seznam následujících slov:

$['Damn', 'headache', 'I', 'have', 'to', 'sleep']$.

Takto vytvoøené koøeny slov se ji¾ pou¾ijí jako tokeny pro uèení klasifikátorù, a nebo mù¾e být vstupní text rozdìlený na tato slova klasifikátory klasifikován do daných tøíd.


\subsubsection{Alternativní pøístup k tokenizaci}
Pøi vyu¾ití základního pøístupu k tokenizaci je problémem skuteènost, ¾e tokeny jsou zcela samostatné bloky, které se pou¾ité klasifikátor (a» u¾ Bayesovský klasifikátor, nebo klasifikátor SVM), uèí ani¾ by byly schopny z tìchto tokenù (slov) urèit kontext.
V¾dy» jak bylo ji¾ bylo vý¹e zmínìno, Bayesovský klasifikátor je oznaèován jako naivní právì proto, ¾e nebere v potaz závislosti jednotlivých tokenù na sobì (co¾ vychází z odvození rovnice \ref{equation_used} pou¾ívané pro klasifikaci -- viz. \ref{SF-BAYES-MAT-bayes_theorem-deriv}).
Proto jsem se sna¾il tuto nevýhodu kompenzovat tím, ¾e jsem pou¾il alternativní pøístup k tokenizaci zachovávající nejbli¾¹í kontext tokenù.
Ten na rozdíl od základní tokenizace nevytváøí tokeny pouze z jednotlivých slov, ale také z $N$ za sebou jdoucích slov, èím¾ bere v potaz jejich kontext ve vìtì.
Nevýhodou tohoto pøístupu je, ¾e pro velké uèící mno¾iny výraznì roste velikost slovníku u Bayesovského klasifikátoru a u SVM klasifikátoru se zvìt¹uje poèet dimenzí pøíznakù, co¾ vede k obtí¾nìj¹ímu nalezení optimální dìlící nadroviny.
Tato vlastnost ale opìt mù¾e být alespoò èásteènì kompenzována stematizací jednotlivých slov, a tím i sní¾ením celkového poètu rùzných tokenù.
Uka¾me si alternativní pøístup k tokenizaci na pøíkladu:

Mìjme opìt zprávu \textit{'Damned headache. I have to sleep.'}.
Prvním krokem pøi zpracování této zprávy je její rozdìlení na vìty, proto¾e kontext za sebou jdoucích slov funguje v¾dy v rámci jedné vìty.
Pro rozdìlení vstupního textu na jednotlivé vìty je s úspìchem pou¾ita knihovna NLTK (viz. \ref{IMP-NLTK}).
Vstupní text je nyní v tomto tvaru
$$['Damned\ headache.', 'I\ have\ to\ sleep']$$\\
Na rozdíl od základního pøístupu, který vytváøí tokeny pouze z jednotlivých slov, vytváøíme tokeny z libovolných za sebou jdoucích $N$-tic o maximální délce $N$ v¾dy tak, ¾e $N$-tice se skládají v¾dy jen ze slov dané vìty.
Hodnota $N$ je nastavena na optimální hodnotu urèenou experimentálnì za pomocí testù tak, aby stále je¹tì vylep¹ovala výslednou pøesnost klasifikátoru a pøitom aby velikost slovníku nepøesáhla pøijatelnou mez.
Pøed tím, ne¾ se samozøejmì ze slov vytvoøí tokeny, pøevedou se v¹echna slova na koøeny.
Tokeny pro vstupní text a $N = 3$ budou tedy pro první vìtu vypadat následovnì:
$$['Damn', 'headache', ('Damn', 'headache')]$$
a pro druhou vìtu:
$$['I', 'have', 'to', 'sleep', ['I', 'have'], ['have', 'to'], ['to', 'sleep'], ['I', 'have', 'to'], ['have', 'to', 'sleep']]$$
S tìmito tokeny pak klasifikátor pracuje stejnì jako s jakýmikoliv jinými tokeny.

\section{Pøíznaky a tokeny v Bayesovském klasifikátoru}

Bayesovský klasifikátor bere postupnì v¹echny tokeny a pøíznaky z trénovací mno¾iny a v závisosti na 
labelech trénovacích dat poèítá pravdìpodobnosti s jakými dané tokeny a pøíznaky nále¾ejí do tøídy spamu.
Takto vypoèítané pravdìpodobnosti pro jednotlivé tokeny a pøíznaky si ukládá do svého slovníku, pomocí nìho¾ následnì klasifikuje nové vstupní texty.

V Bayesovském klasifikátoru je velmi dùle¾ité vybrat pro danou trénovací mno¾inu dat vhodné pøíznaky, které se z textu budou extrahovat a pou¾ívat pro klasifikaci.
Proto je v této práci klasifikátor trénován se v¹emi mo¾nými kombinacemi pou¾itých pøíznakù a jsou zvoleny ty pøíznaky, pro které mají výsledky klasifikátoru na testovacích datech s tìmito daty nejvìt¹í korelaci.

\section{Pøíznaky a tokeny v SVM klasifikátoru}

Na rozdíl od Bayesovského klasifikátoru, problém volby vhodných pøíznakù a tokenù u SVM klasifikátoru odpadá.
Jeliko¾ si klasifikátor pøi uèení vytváøí váhový vektor, který urèuje míru dùle¾itosti jednotlivých pøíznakù v prostoru pøíznakù pro výsledek klasifikace, je jasné, ¾e klasifikátor doká¾e irelevantní pøíznaky ignorovat a øídit se hlavnì pøíznaky, které na výslednou klasifikaci budou mít nejlep¹í vliv.

Pokud ov¹em bude pou¾ito pøíli¹ velké mno¾ství pøíznakù, klasifikátoru výraznì naroste poèet dimenzí, co¾ se mù¾e markantnì projevit ve zhor¹ení klasifikaèních schopností SVM klasifikátoru.
Jeliko¾ speciálních pøíznaky generují jen malé mno¾ství tokenù, tento problém by nemìl nastat.
Naproti tomu ale generování $N$-tic textových tokenù tento problém zpùsobit mù¾e.
Proto je vhodné volit zvolit optimální velikost generovaných N-tic tak, aby se nezhor¹ovaly klasifikaèní schopnosti klasifikátoru.





\chapter{Vlastní implementace} \label{IMPLEMENTACE}
V této kapitole se budeme zabývat vlastní implementací obou klasifikaèních algoritmù z kapitol \ref{BAYES} a \ref{SVM}.
Kromì pou¾itých knihoven zde popí¹eme také architekturu programu implementovaného v praktické èásti této práce.
Také zde budou pøedstaveny problémy, ke kterým pøi implementaci do¹lo a rovnì¾ bude prezentováno, jakým zpùsobem byly vyøe¹eny.

Celý program je implementován v programovacím jazyce Python 2.7.
Jazyk Python byl zvolen hlavnì z toho dùvodu, ¾e pro nìj existuje mnoho knihoven, které mohly být v této práci pou¾ity a za jejich¾ pomoci se implementace výraznì zjednodu¹ila.
Takto byla vyu¾ita zajména knihovna pro zpracování pøirozené øeèi \texttt{NLTK}, knihovna \texttt{NumPy} (Numerical python), která umo¾òuje v prostøedí jazyka Python velmi jednodu¹e pracovat se slo¾itìj¹í matematikou a knihovna \texttt{CVXOPT} vyu¾itá k implementaci SVM klasifikátoru pro øe¹ení úloh kvadratického programování.

Tyto knihovny budou mimo jiné také podrobnìji popsány dále v této kapitole.

\section{Pou¾ité knihovny} \label{IMPL_LIB}
\subsection{NLTK} \label{IMP-NLTK}
\texttt{NLTK}, neboli Natural Language ToolKit je balík knihoven pro skriptovací jazyk Python 2.7, urèený pro symbolické a statistické zpracovávání pøirozené øeèi.
Nabízí jednoduché rozhraní pro velké mno¾ství rùzných nástrojù pro zpracování textu, ale také velké mno¾ství ukázkových dat, která lze pou¾ít jako testovací a trénovací data pøi vyvíjení softwaru pracujícího s pøirozeným jazykem.
Díky vynikající dokumentaci je \texttt{NLTK} skvìlou knihovnou výraznì zjednodu¹ující implementaci softwaru pracujícího nìjakým zpùsobem s pøirozeným jazykem.

V praktické èásti této práce je knihovna \texttt{NLTK} pou¾ívána jako souèást pøedzpracování vstupního textu urèeného buï k uèení klasifikátorù, nebo ji¾ pøímo pro klasifikaci.
Je pou¾ita k tìmto dvìma operacím s textem:
\begin{enumerate}
 \item \emph{Rozdìlení vìt} -- prvním modulem z této knihovny, který je v této práci pou¾it, je modul \texttt{punkt} (\texttt{nltk/tokenize/punkt}), jen¾ rozdìluje vstupní text na seznam vìt.
 \item \emph{Pøevedení slov na jejich koøeny} -- druhým modulem z této knihovny je modul \texttt{snowball} (\texttt{nltk.stem.snowball}), jeho¾ úkolem je z jednotlivých slov vstupního textu udìlat koøen slov (více viz \ref{Textove_tokeny}).
\end{enumerate}

\subsection{NumPy} \label{IMP-NUMPY}
\texttt{NumPy} je základním balíkem knihoven pro numerické výpoèty v programovacím jazyce Python.
Tento balík knihoven umo¾òuje jednoduchou práci s mnohadimenzionálními poli a dal¹ími objekty od nich odvozenými (masked arrays, matice $\ldots$).
Mimo to v sobì knihobna zahrnuje velké mno¾ství rychlých matematických operací nad poli, zahrnujících diskrétní Fourierovy transformace, tøídìní, základní lineární algebru a mnoho dal¹ích.
Vyu¾ití této knihovny je pro programy øe¹ící vìt¹í mno¾ství slo¾itìj¹ích matematických výpoètù prakticky nutností, nebo» výraznì zrychlí výpoèty a programátorovi zjednodu¹í práci s matematickými daty.
Mnoho dal¹ích knihoven øe¹ících nìjaké rozsáhlej¹í matematické úkony je zalo¾eno právì na knihovnì \texttt{NumPy}.
Mezi tyto dal¹í knihovny patøí i balík knihoven \texttt{CVXOPT} (viz \ref{IMP-CVXOPT}), pou¾itý pro implementaci SVM klasifikátoru v praktické èásti této práce.

\subsection{CVXOPT} \label{IMP-CVXOPT}
\texttt{CVXOPT} je volnì dostupný balík knihoven pro øe¹ení konvexních optimalizaèních problémù (mimo jiné do této skupiny problémù patøí i kvadratické programování, je¾ je pou¾ito pro implementaci SVM klasifikátoru) v programovacím jazyce Python 2.7.
Hlavním úkolem tohoto balíku knihoven je zjednodu¹it vývoj softwaru, jen¾ potøebuje pro svùj bìh øe¹it konvexní optimalizaèní úlohy, ani¾ by bylo potøeba implementovat slo¾ité optimalizaèní algoritmy.


\section{Architektura a implementace programu}
V této èásti kapitoly struènì pøedstavíme architekturu, tedy návrh struktury implementovaného programu a implementaci klasifikaèních algoritmù z popsaných v teoretické èásti této práce.

\subsection{Architektura programu}
\subsubsection{Fyzická architektura}
Fyzická architektura programu pøedstavuje popis fyzického rozdìlení zdrojových souborù programu.
Toto rozdìlení zde bude popsáno, aby se ètenáø orientoval v následujícím textu z implementace programu z praktické èásti této práce.

V koøenovém adresáøi tohoto projektu se nachází spou¹tìcí skript \texttt{control.py}, pomocí nìho¾ je program ovládán a který nabízí ve¹keré operace, které je mo¾né s programem provést.
Výpis nápovìdy k programu bude vypsán v Pøíloze XXXXX. 

Nad koøenovým adresáøem se nachází adresáø \texttt{/src/} obsahující tøi podadresáøe, ka¾dý obsahující jednu logickou èást programu:

\begin{enumerate}
 \item Adresáø \texttt{common} -- v tomto adresáøi se nacházejí ve¹keré operace, které jsou pro oba implementované klasifikátory spoleèné, a to extrakce textových tokenù a tokenù speciálních pøíznakù z textu.
 \item Adresáø \texttt{bayes} -- v tomto adresáøi se nachází kompletní implementace naivního Bayesovského klasifikátoru.
 \item Adresáø \texttt{svm} -- v tomto adresáøi se nachází kompletní implementace SVM klasifikátoru.
\end{enumerate}

Mimo èást se zdrojovými kódy se potom nachází adresáø \texttt{data}, který obsahuje ve¹kerá ruènì anotovaná data a natrénované klasifikaèní modely pro oba klasifikátory, které umo¾òují klasifikátor pou¾ívat bez toho, ani¾ by se musel poka¾dé znovu spou¹tìt trénovací algoritmus na mno¾inì trénovacích dat.

\subsubsection{Architektura tøíd}
V této èásti dokumentu bude popsána architektura tøíd s jejich krátkým popisem, která umo¾ní rychle pochopit architekturu a funkènost programu.

Program, implementovaný jako praktická èást této práce, je, jak ji¾ bylo vý¹e zmínìno (\ref{IMPLEMENTACE}), implementován  v programovacím jazyce Python, jen¾ umo¾òuje vyu¾ití objektovì orientovaného pøístupu k vývoji aplikací.
Díky této vlastnosti Pythonu bylo mo¾né implementovat program objektovì a vyu¾ít tak výhod tohoto pøístupu.

Jak Bayesovský klasifikátor, tak klasifikátor zalo¾ený na support vector machines, jsou v implementaèní èásti této práce vytvoøeny tak, aby bylo mo¾né je pou¾ívat jako modulù programovacího jazyka Python -- zcela nezávisle na sobì.
To znamená, ¾e lze oba klasifikátory importovat jako modul do novì implementované aplikace a pou¾ít pro klasifikaci libovolného vstupního textu.
Této vlastnosti bylo vyu¾ito v projektu M-Eco, v nìm¾ byl Bayesovský klasifikátor pou¾it ve skriptu vkládajícím nová data do databáze, kde filtroval relevantní data od nerelevantní a relevantních data byla následnì vkládána do databáze.
V projektu jsou tedy implementovány dvì základní tøídy \texttt{SVM} a \texttt{BayesianClassifier}.
Obì tyto tøídy obsahují metody pro trénování klasifikátoru z trénovacích dat a metody umo¾òující po pøedchozím nauèení klasifikátoru klasifikovat data do daných tøíd.

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=15cm,keepaspectratio]{fig/Features-diag}
      \caption{Diagram tøíd dìdících z tøídy Features} 
      \label{features-diag}
    \end{center}
\end{figure}

Jeliko¾ jak Bayesovský klasifikátor, tak klasifikátor zalo¾ený na support vector machines pracují se vstupním textem prakticky stejnì, byla vytvoøena tøída \texttt{Entry} a tøídy dìdící z rodièovské tøídy \texttt{Feature} (viz obr. \ref{features-diag}), nacházející se v adresáøi \texttt{common}.
Tyto tøídy pou¾ívají oba klasifikátory pro práci s ve¹kerým vstupním textem, tedy jak pro klasifikaci, tak pro uèení.
Instance tøídy \texttt{Entry} se vytváøí pro ka¾dý text vstupující do klasifikátoru.
Jejím úkolem je text rozdìlit na tokeny tak, aby samotné klasifikátory ji¾ s textem nemusely nijak pracovat, ale aby dostaly pouze seznam textových a speciálních tokenù, je¾ vstupní text obsahoval.
Tato tøída tedy zodpovídá za vyhledávání jednotlivých speciálních tokenù (viz \ref{TOKENIZACE-SPEC}), které potom jako instance tøíd dìdících ze tøídy \texttt{Features} spoleènì s $N$-ticemi jednotlivých textových tokenù (viz \ref{Textove_tokeny}) pøedává klasifikátorùm.

Aby bylo mo¾né oba klasifikátory testovat a posléze i vzájemnì porovnávat, byly pro ka¾dý klasifikátor vytvoøeny tøídy, je¾ mají na starost testování.
Tyto tøídy se nazývají \texttt{SVMTest} pro klasifikátor zalo¾ený na support vector machines a 
\texttt{BayesianTest} pro Bayesovský klasifikátor.
Obì tyto tøídy obsahují metody pro spou¹tìní testù klasifikátorù a jejich následné vyhodnocení.

Jeliko¾ oba implementované klasifikátory vyu¾ívají pro klasifikaci diametrálnì odli¹ných pøístupeù, má ka¾dý klasifikátor potøebu jinak nakládat s tokeny získanými z instancí tøídy \texttt{Entry}.
Proto ka¾dý klasifikátor obsluhuje nìkolik tøíd, je¾ pøi klasifikaci nebo uèení pou¾ívá.

V pøípadì Bayesovského klasifikátoru je takovouto dal¹í pou¾ívanou tøídou pouze jediná tøída s názvem \texttt{WordDictionary}, která se stará o slovník tokenù Bayesovského klasifikátoru.
Tento slovník tokenù obsahuje pravdìpodobnostní hodnoty jednotlivých tokenù nalezených bìhem uèení a pravdìpodobnost jejich nále¾itosti do urèité tøídy. 
Tato tøída také zabezpeèuje ve¹keré operace provádìné s tímto slovníkem tokenù, jako napøíklad jeho ulo¾ení do souboru a opìtovné naètení, díky èemu¾ je mo¾né Bayesovský klasifikátor, stejnì jako klasifikátor zalo¾ený na metodì support vector machines pou¾ívat, ani¾ bychom je museli bezprostøednì pøed klasifikací uèit, co¾ mù¾e být velmi zdlouhavý proces.
Mimo operací exportu a importu slovníku také tato tøída umo¾òuje exportovat daný slovník do souboru v jazyce XML.

Podobnì jako Bayesovský klasifikátor, potøebuje i klasifikátor zalo¾ený na metodì support vector machines tøídu, je¾ pracuje se vstupním textem a tokeny z nìj vytvoøenými a pøipraví tokeny do podoby, ve které je klasifikátor schopen s nimi pracovat.
Tato tøída se u SVM klasifikátoru jmenuje \texttt{Data}.
Její funkcí je vytvoøit prostor pøíznakù ze vstupních dat a data následnì rozdìlit na bloky trénovacích a testovacích dat.
Dal¹í specifickou tøídou, ji¾ klasifikátor zalo¾ený na SVM pou¾ívá, je tøída \texttt{Kernel} a její podtøídy, které klasifikátor potøebuje pro výpoèet jaderné funkce pro klasifikaci.
Tuto tøídu dìdí v implementovaném programu následující tøi specifické jaderné funkce:

\begin{itemize}
  \item Lineární jaderná funkce
  \item Polynomiální jaderná funkce
  \item Radiální bázová jaderná funkce (RBF)
\end{itemize}

Poslední tøídou která je v souvislosti s klasifikátorem zalo¾eným na SVM spojena, je tøída \texttt{Annealing}, je¾ øe¹í obtí¾ný úkol vhodného výbìru parametrù u klasifikátoru a jím pou¾ívané jaderné funkce.
Více o tomto výbìru vhodných parametrù bude popsáno v èásti o implementaci SVM klasifikátoru (viz \ref{IMPLEMENTATION-SVM}).

Tímto jsme velmi obecnì pro¹li v¹echny tøídy pou¾ité v implementaèní èásti této práce.
Pro podrobnìj¹í popis tøíd je vygenerována dokumentace pomocí dokumentaèního nástroje \texttt{epydoc}.
Dokumentace je pøilo¾ena na CD, je¾ je souèástí této práce.

\section{Implementace}
V této èásti kapitoly se budeme podrobnìji zabývat postupem implementace jednotlivých vybraných èásti programu z implementaèní èásti této práce.
Pokusíme se zde rozebrat problémy, je¾ pøi implementaci nastaly a jejich øe¹ení.
Tato sekce bude pro vìt¹í pøehlednost rozdìlena na dvì èásti: implementaci Bayesovského klasifikátoru a klasifikátoru zalo¾eného na SVM, ve kterých budeme rozebírat odpovídající implementaèní problémy.


\subsection{Implementace Bayesovského klasifikátoru}
Pøi implementaci Bayesovského klasifikátoru nastaly pouze dva vìt¹í problémy, a to potøeba ukládání dat slovníku Bayesovského klasifikátoru pro pozdìj¹í vyu¾ití a otázka jakým zpùsobem vytvoøit klasifikaèní algoritmus, aby textové tokeny i tokeny speciálních pøíznakù ovlivòovaly výsledek klasifikace stejnou mìrou.

\subsubsection{Ukládání slovníku Bayesovského klasifikátoru}
Prvním problémem, který bylo tøeba vyøe¹it, bylo ukládání slovníku, obsahujícího pravdìpodobnostní hodnoty jednotlivých tokenù, na nì¾ klasifikátor pøi svém uèení narazil, do souboru tak, aby se klasifikátor nemusel pøed ka¾dou klasifikací znovu uèit v¹echna data z trénovací sady, co¾ by bylo velmi zdlouhavé.
Pro tuto potøebu jsem zvolil knihovní funkci programovacího jazyku Python s názvem \textit{pickle}, která umo¾òuje provést takzvanou serializaci libovolné datové struktury programovacího jazyka do souboru a zpìt.

\subsubsection{Typy tokenù a jejich vliv na klasifikaci}
Druhým problémem, který nastal, bylo jakým zpùsobem by mìly být zpracovávány textové tokeny a tokeny speciálních pøíznakù.
Jeliko¾ poèet textových tokenù mù¾e dalece pøevy¹ovat poèet tokenù speciálních pøíznakù, nebylo by vhodné, aby se s tìmito dvìma typy tokenù zacházelo stejným zpùsobem.
Je toti¾ vysoce pravdìpodobné, ¾e pokud bude textových tokenù mnohem více tokenù speciálních pøíznakù, budou mít tokeny speciálních pøíznakù velmi malý dopad na výslednou klasifikaci.
Proto se v implementovaném programu oba typy tokenù klasifikují samostatnì pomocí dvou klasifikátorù a výsledná klasifikace se urèí jako aritmetický prùmìr výsledkù tìchto klasifikátorù.


\subsection{Implementace SVM klasifikátoru}\label{IMPLEMENTATION-SVM}
Podobnì jako pøi implementaci Bayesovského klasifikátoru nastly i pøi implementaci klasifikátoru, zalo¾eného na support vector machines urèité problémy, z nich¾ zmínímenásledující dva tøi:

\begin{itemize}
\item Jaké parametry pøedat balíèku kvadratického programování.
\item Jak optimálnì zvolit parametr $C$ (viz. \ref{SVM_C}) a parametry jaderných funkcí.
\item Jak optimalizovat výpoèet skalárního souèinu v prostoru pøíznakù klasifikátoru SVM.
\end{itemize}

\subsubsection{Parametry kvadratického programování}

Jak ji¾ bylo zmínìno vý¹e v této kapitole o pou¾itých knihovnách (viz \ref{IMPL_LIB}), pro øe¹ení optimalizaèního problému, vycházejícího z matematického popisu klasifikátoru SVM (viz \ref{SVM}), byl pou¾it balík knihoven pro programovací jazyk Python s názvem \texttt{CVXOPT}, pøesnìji modul \texttt{qp} z tohoto balíku knihoven, umo¾òující øe¹it optimalizaèní problémy pomocí kvadratického programování.

Optimalizaèní problém, který potøebujeme øe¹it, je popsán na konci kapitoly \ref{SVM} a vypadá následovnì:

\begin{equation}
\label{IMP_SVM_MAX}
\begin{aligned}
& \underset{\alpha}{\text{maximalizace}}
& & \sum^{n}_{i=1}\alpha_i - \frac{1}{2}\sum^{n}_{i=1}\sum^{n}_{j=1}y_iy_j\alpha_i\alpha_jx^T_i x_j \\
& \text{kde}
& & \sum^n_{i=0} y_i\alpha_i = 0, & 0 \leq \alpha_i \leq C
\end{aligned}
\end{equation}

Zde se nám ji¾ ukazuje první problém, který je tøeba vyøe¹it, a sice, ¾e optimalizaèní algoritmus kvadratického programování v balíku \texttt{CVXOPT.qp} je implemntován tak, aby øe¹il minimalizaèní problémy. 
Musíme tedy pøevést problém \ref{IMP_SVM_MAX} na ekvivalentní minimalizaèní problém:

\begin{equation}
\label{IMP_SVM_MIN}
\begin{aligned}
& \underset{\alpha}{\text{minimalizace}}
& & \frac{1}{2}\sum^{n}_{i=1}\sum^{n}_{j=1}y_iy_j\alpha_i\alpha_jx^T_i x_j - \sum^{n}_{i=1}\alpha_i\\
& \text{kde}
& & \sum^n_{i=0} y_i\alpha_i = 0, & 0 \leq \alpha_i \leq C
\end{aligned}
\end{equation}

V dokumentaci balíku \texttt{CVXOPT} je definován formát kvadratických problémù, které je modul \texttt{qp} schopen øe¹it.
Vypadá takto:

\begin{equation}
\label{IMP_SVM_CVXOPT}
\begin{aligned}
& \underset{\alpha}{\text{minimalizace}}
& & \frac{1}{2} \alpha^T P \alpha + q^T \alpha\\
& \text{kde}
& & G\alpha \leq h \\
& & & A\alpha = b
\end{aligned}
\end{equation}

Museli jsme tedy urèit, jaké hodnoty dosadit za parametry $P$ (kvadratické koeficienty), $q$ (lineární koeficienty), $G$ a $h$ (podmínky nerovnosti), $A$ a $b$ (podmínky rovnosti).

Za promìnnou $P$ jsme dosadili matici 

\begin{equation}P = 
\begin{bmatrix}
y_1y_1 \; x^T_1x_1 & y_1y_2 \; x^T_1x_2 & y_1y_3 \; x^T_1x_3 & \cdots & y_1y_N \; x^T_1x_N \\
y_2y_1 \; x^T_2x_1 & y_2y_2 \; x^T_2x_2 & y_2y_3 \; x^T_2x_3 & \cdots & y_2y_N \; x^T_2x_N \\
\vdots            & \vdots           & \vdots          & \vdots & \vdots          \\
y_Ny_1 \; x^T_Nx_1 & y_Ny_2 \; x^T_Nx_2 & y_Ny_3 \; x^T_Nx_3 & \cdots & y_Ny_N \; x^T_Nx_N
\end{bmatrix}
\end{equation}

kde platí, ¾e $x_N \in X$ a $y_N \in Y$.
Tato matice v¹ak platí pouze v pøípadì, ¾e nepou¾íváme jaderných funkcí pro vytvoøení nelineární diskriminaèní funkce.
Jestli¾e v¹ak pou¾íváme jadernou funkci $K(x_1, x_2)$ poèítající skalární souèin v prostoru pøíznakù (viz \ref{SVM-KERNELS}), potom musíme nahradit skalární souèin $x^T_mx_n$ za tuto jadernou funkci, do ní¾ dosadíme jednotlivé vektory.
Matice $P$ pøi pou¾ití jaderných funkcí bude tedy vypadat následovnì:

\begin{equation}P = 
\begin{bmatrix}
y_1y_1 \; K(x_1, x_1) & y_1y_2 \; K(x_1, x_2) & y_1y_3 \; K(x_1, x_3) & \cdots & y_1y_N \; K(x_1, x_N) \\
y_2y_1 \; K(x_2, x_1) & y_2y_2 \; K(x_2, x_2) & y_2y_3 \; K(x_2, x_3) & \cdots & y_2y_N \; K(x_2, x_N) \\
\vdots                & \vdots                & \vdots                & \vdots & \vdots                \\
y_Ny_1 \; K(x_N, x_1) & y_Ny_2 \; K(x_N, x_2) & y_Ny_3 \; K(x_N, x_3) & \cdots & y_Ny_N \; K(x_N, x_N)
\end{bmatrix}
\end{equation}

Dal¹ím parametrem, který musíme kvadratickému programování poskytnout, je parametr lineárních koeficientù $q$.
Tím bude pouze vektor stejné délky, jako je poèet vstupních dat obsahující hodnoty $-1$, tedy:

\begin{equation}
q = (-1, -1, \ldots, -1), \hspace{5em} |q| = |X|
\end{equation}

Nyní se dostáváme k parametrùm $G$, $h$ a $A$, $b$.
Tyto parametry, jak bylo ji¾ vý¹e zmínìno, definují podmínky (\emph{constrains}) optimalizaèního procesu. 
Jak vyplývá z podmínek rovnice \ref{IMP_SVM_MIN}, musíme zadat jednu podmínku rovnosti, a sice:

\begin{equation}
\sum^n_{i=0} y_i\alpha_i = 0
\end{equation}

To je provedeno velmi jednoduchým zpùsobem, tak ¾e promìnná $A$ bude obsahovat vektor $Y$ (labely trénovacích dat) a promìnná $b$ bude hodnota 0, tedy:

\begin{equation}
A = (y_1, y_2, \ldots, y_N), \hspace{5em} y_i \in Y; i \in |Y|
\end{equation}
\begin{equation}
b = 0
\end{equation}

V programu je mo¾né zvolit, zda chce u¾ivatel pou¾ívat promìnnou $C$ -- tedy zda umo¾ní poru¹ovat margin (viz \ref{SVM-THEORY-MARGINS}).
Pokud se promìnná nepou¾ívá, pak má optimalizaèní problém následující tvar:

\begin{equation}
\label{IMP_SVM_MIN_BEZ_C}
\begin{aligned}
& \underset{\alpha}{\text{minimalizace}}
& & \frac{1}{2}\sum^{n}_{i=1}\sum^{n}_{j=1}y_iy_j\alpha_i\alpha_jx^T_i x_j - \sum^{n}_{i=1}\alpha_i\\
& \text{kde}
& & \sum^n_{i=0} y_i\alpha_i = 0, & 0 \leq \alpha_i \leq \infty
\end{aligned}
\end{equation}

Podíváme-li se podrobnìji na omezení podmínky $\sum^n_{i=0} y_i\alpha_i = 0$, vidíme, ¾e $\alpha$ mù¾e nabývat libovolné hodnoty od 0 do nekoneèna, tudí¾ parametry podmínky nerovnosti budou následující:

\begin{equation}
\label{IMP-G}
G = \begin{bmatrix}
-1 & 0 & \cdots & 0\\
0 & -1 & \cdots & 0\\
\vdots & \vdots & \ddots & 0\\
0 & 0 & \cdots & -1 \\
\end{bmatrix}
\end{equation}
\begin{equation}
\label{IMP-h}
h = (0, 0, \ldots, 0), \hspace{5em} |h| = |X|
\end{equation}

Promìnné $G$ a $h$ (\ref{IMP-G} a \ref{IMP-h}) po vlo¾ení do kvadratického programování popisují právì podmínku $0 \leq \alpha_i \leq \infty$.

Jestli¾e ale u¾ivatel zadá, ¾e chce pou¾ívat promìnnou $C$, situace se tro¹ku komplikuje.
Minimalizaèní úloha, kterou potøebujeme vyøe¹it, se nepatrnì zmìní do následující podoby:

\begin{equation}
\label{IMP_SVM_MIN_S_C}
\begin{aligned}
& \underset{\alpha}{\text{minimalizace}}
& & \frac{1}{2}\sum^{n}_{i=1}\sum^{n}_{j=1}y_iy_j\alpha_i\alpha_jx^T_i x_j - \sum^{n}_{i=1}\alpha_i\\
& \text{kde}
& & \sum^n_{i=0} y_i\alpha_i = 0, & 0 \leq \alpha_i \leq C
\end{aligned}
\end{equation}

Podmínka, je¾ v pøedchozím pøípadì (bez $C$) nebyla shora nijak omezená, je najednou limitována hodnotou promìnné $C$.
Parametr $G$ a $h$ se tedy oproti pøedchozímu pøípadu bude muset roz¹íøit tak, aby pojal i pravou èast podmínky.
Bude tedy vypadat takto:

\begin{equation}
\label{IMP-G}
G = \begin{bmatrix}
-1 & 0 & \cdots & 0\\
0 & -1 & \cdots & 0\\
\vdots & \vdots & \ddots & 0\\
0 & 0 & \cdots & -1 \\
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & 0\\
0 & 0 & \cdots & 1 \\
\end{bmatrix}
\end{equation}
\begin{equation}
\label{IMP-h}
h = (0, 0, \ldots, 0, C, C, \ldots, C), \hspace{5em} |h| = 2 \cdot |X|
\end{equation}

Pokud tyto parametry tedy poskytneme modulu kvadratického programování, získáme výsledné hodnoty Lagrangeových multiplikátorù, které definují support vektory, je¾ se z trénovací mno¾iny uèením vytvoøily. 
S tìmito Lagrangeovými multiplikátory pak dále pracujeme a vytvoøíme z nich klasifikaèní model (viz \ref{SVM-THEORY-MARGINS}).


\subsubsection{Volba vhodných volných parametrù klasifikátoru a jaderných funkcí}
Vhodný výbìr volných parametrù klasifikátoru a jaderných funkcí je velmi dùle¾itou souèástí pøi implementaci klasifikátoru zalo¾eného na metodì SVM.
Nejsou-li vybrány vhodné parametry, mù¾e klasifikátor dosahovat vskutku výraznì hor¹ích výsledkù, ne¾ jsou-li vybrány tyto parametry optimálnì.
V implementovaném programu bylo tøeba optimálnì urèit parametr $C$ (míru chybovosti vzhledem k maximalizaci marginu -- viz \ref{SVM_C}) a parametr pou¾ité jaderné funkce (urèující vlastnosti pou¾ité jaderné funkce -- viz \ref{SVM-KERNELS}), jen¾ budeme nadále nazývat $\gamma$.

Nastává ov¹em problém, jakým zpùsobem tyto volné parametry volit tak, aby byly výsledky klasifikace co nejpøesnìj¹í.
Ve své knize \uv{The Nature of Statistical Learning Theory} \cite{vapnik2000nature} Vladimir Vapnik doporuèil nastavovat volné parametry v závislosti na znalosti mno¾iny trénovacích dat.

Jestli¾e takovými znalostmi o datech nedisponujeme, mù¾eme vyu¾ít velmi èasto pou¾ívanou metodu hledání hodnot optimálních parametrù v møí¾ce.
Tato metoda je sice funguje, ale aby byla dostateènì pøesná, je tøeba ji poèítat pro dostateènou hustotu bodù v møí¾ce, co¾ mù¾e být velmi výpoèetnì nároèné.

V této práci byla pro hledání optimálních metod pou¾ita metoda simulovaného ¾íhání (\emph{simulated annealing}).
Simulované ¾íhání je pravdìpodobnostní optimalizaèní metoda pro nalezení globální minimální nebo maximální energie, odpovídající souøadnicím ve stavovém prostoru.
Tato metoda je inspirovaná fyzikou procesu ¾íhání oceli, pøi kterém se materiál pøedehøeje na urèitou teplotu a postupnì se nechá ochlazovat, èím¾ se odstraní napìtí v materiálu, který se takto homogenizuje.
U optimalizaèního procesu simulovaného ¾íhání se nastaví poèáteèní souøadnice ve stavovém prostoru a poèáteèní teplota, která se postupnì po krocích sni¾uje a¾ do úplného vychladnutí.

Pøi ka¾dém kroku se vygenerují náhodné souøadnice ve stavovém prostoru (stav) a spoèítá se k nim odpovídající energie.
Jestli¾e je tato energie ni¾¹í, ne¾ energie pøedchozího stavu, pak je tento stav pou¾it pro dal¹í iteraci.
Pokud v¹ak energie ni¾¹í nebyla, spoèítá se pravdìpodobnost skoku do daného stavu s vy¹¹í energií.
Tato pravdìpodobnost závisí na teplotì -- èím vy¹¹í teplota, tím více umo¾òuje algoritmus skoèit do stavu s vy¹¹ími hodnotami energie.
Tímto se významnì eliminuje riziko uváznutí v lokálním minimu.
Ekvivalentnì lze algoritmus pou¾ít pro maximalizaci energie.

\paragraph{Popis algoritmu simulovaného ¾íhání}
Pro co nejjasnìj¹í pøedvedení pou¾itého algoritmu simulovaného ¾íhání pøedstavíme jeho pseudokód:

\begin{algorithm}
\caption{Pseudokód algoritmu simulovaného ¾íhání -- minimalizace energie}
\label{IMP-ANNEALING-PSEUDOCODE}
\begin{algorithmic}[1]
\STATE $state \leftarrow STATE0$
\STATE $energy \leftarrow calculate\_energy(s)$
\STATE $state\_best \leftarrow state$, $energy\_best \leftarrow energy$
\STATE $temp \leftarrow INIT\_TEMP$
\WHILE{$temp > STOP\_TEMP$}
	\STATE $temp \leftarrow temp \cdot COOLING\_FACTOR$
	\STATE $state\_new \leftarrow generate\_neighbors(state)$
	\STATE $energy\_new \leftarrow calculate\_energy(state\_new)$
	\IF{$P(energy, energy\_new, temp) > random()$}
		 \STATE $state \leftarrow state\_new$, $energy \leftarrow energy\_new$
	\ENDIF
	\IF{$energy\_new < energy\_best$}
		 \STATE $state\_best \leftarrow state\_new$, $energy\_best \leftarrow energy\_new$
	\ENDIF
\ENDWHILE
\RETURN $state\_best$
\end{algorithmic}
\end{algorithm}

Promìnné $STATE0$ -- poèáteèní stav (za stav budeme nadále pova¾ovat bod v prostoru souøadnic volných parametrù SVM klasifikátoru), $INIT\_TEMP$ -- poèáteèní teplota, $STOP\_TEMP$ -- koneèná teplota a $COOLING\_FACTOR$ -- faktor chládnutí jsou parametry, které jsou pøi spu¹tìní algoritmu simulovaného ¾íhání zadány.
Funkce $calculate\_energy(stav)$ potom poèítá energii stavu $stav$ a funkce $generate_neighbor(stav)$ vygeneruje mo¾ný sousední stav ke stavu $stav$.
V pseudokódu je také pou¾ita funkce $random()$, která generuje náhodná èísla v rozmezí $<0,1>$.

Na následujících øádcích se podrobnìji podíváme na nìkteré zajímavé souèásti tohoto algoritmu.


\subparagraph{Výpoèet energie stavu}
Abychom byli schopni najít minimální energii systému v souøadnicích, musíme být schopni vypoèítat energii libovolného vygenerovaného stavu.
Pro tuto optimalizaci volných parametrù klasifikátoru SVM jsme vycházeli ze dvou po¾adavkù, které na SVM klasifikátor máme, a sice:

\begin{itemize}
\item sna¾íme se maximalizovat pøesnost klasifikátoru
\item sna¾íme se minimalizovat poèet support vektorù klasifikátoru.
\end{itemize}

Výsledná energie potom bude záviset na míøe naplnìní tìchto dvou vý¹e zmínìných po¾adavkù.

Vzhledem k tomu, ¾e implementovaná metoda simulovaného ¾íhání je minimalizaèní, musíme pøevést reprezentaci tìchto po¾adavkù tak, aby vytvoøily metriku umo¾òující vyu¾ití v procesu minimalizace.

V pøípadì prvního po¾adavku (snaha o maximalizaci pøesnosti klasifikátoru), je zøejmé, ¾e maximalizaci je tøeba pøevést na minimalizaci.
Místo o maximalizaci pøesnosti klasifikátoru tedy usilujeme o minimalizaci chybovosti klasifikátoru.
Takto tedy dostaneme vzorec pro výpoèet energie pøesnosti $E_p$ klasifikátoru:

\begin{equation}
E_p = 1 - (\frac{spravne\_klasifikovane\_vstupy}{vsechny\_vstupy}), \hspace{5em} E_p \in <0,1>
\end{equation}

Pomocí tohoto vzorce vypoèteme energii pøesnosti klasifikátoru, kterou se budeme sna¾it minimalizovat.

Pokud jde o druhý po¾adavek (snaha o minimalizaci poètu support vektorù klasifikátoru), tento po¾adavek ji¾ má minimalizaèní charakter, a proto jej nemusíme nijak pøevádìt.
Pro výpoèet energie tototo po¾adavku $E_{SV}$ dostaneme následující vzorec:

\begin{equation}
E_{SV} = \frac{pocet\_pouzitych\_SV}{vsechny\_SV}, \hspace{5em} E_{SV} \in <0,1>
\end{equation}

Výpoèet celkové energie $E$ stavu se tudí¾ bude rovnat souètu energie pøesnosti a energie support vektorù, tedy:

\begin{equation}
E =  E_p + E_{SV} , \hspace{5em} E \in <0,2>
\end{equation}

\subparagraph{Generování mo¾ného sousedního stavu}
Pro generování mo¾ného sousedního stavu byla pou¾ita metoda navrhnutá v èlánku \uv{Parameter determination of support vector machine and feature selection using simulated annealing approach} \cite{annealing}.
Tato metoda je zalo¾ena na vygenerování smìrového vektoru s poèátkem v aktuálním stavu.
Na tomto vektoru je pak náhodnì zvolen mo¾ný sousední stav. 

\subparagraph{Pravdìpodobnost pøijetí nového stavu}
Pøi vy¹¹ích teplotách na poèátku bìhu algoritmu je mo¾né, ¾e algoritmus simulovaného ¾íhání pou¾ije jako svùj následující stav stav, který má vy¹¹í energii, ne¾ stav pùvodní.
Tímto postupem se zamezí uváznutí v lokálních minimech a umo¾ní se tak nalezení globálního minima.
S postupným sni¾ováním teploty se pravdìpodobnost skoku na takovéto stavy s vy¹¹í energií sni¾uje.

Pro výpoèet této pravdìpodobnosti je pou¾it následující vzorec:

\begin{equation}
P_{skoku} = e^{\frac{\Delta E}{T}}
\end{equation}

kde $\Delta E$ je rozdíl mezi energií sousedního stavu a pùvodního stavu a $T$ je aktuální teplota v iteraci simulovaného ¾íhání.
Pro urèení, zda se má do nového stavu skoèit, èi ne je vygenerováno náhodné èíslo $X \in <0,1>$.
Pokud je $P_{skoku} > X$, pak je pro dal¹í iteraci pou¾it sousední stav jako stav základní.


\subsubsection{Optimalizace skalárních souèinù v prostoru pøíznakù} \label{IMP-GRAM}
Abychom docílili zrychlení uèení SVM klasifikátoru, pøistoupili jsme k výpoètu takzvané Gramovy matice.
Gramova matice je matice $G$ skalárních souèinù vstupního vektoru, tedy $ G_{i,j} = x_i^Tx_j$ kde $x_i, x_j \in X$.
Tím, ¾e vypoèteme tuto matici pøed samotným uèením klasifikátoru, zamezíme zbyteènému nìkolikanásobnému výpoètu kartézských souèinù mezi stejnými vektory vstupních trénovacích dat, èím¾ se výraznì zrychlí výpoèet.

Gramova matice pro lineární SVM klasifikátor vypadá takto:

\begin{equation}
\label{IMP-G}
G_{linear} = \begin{bmatrix}
x_1^Tx_1 & x_1^Tx_2 & x_1^Tx_3 & \cdots & x_1^Tx_N\\
x_2^Tx_1 & x_2^Tx_2 & x_2^Tx_3 & \cdots & x_2^Tx_N\\
\vdots   & \vdots   & \vdots   & \vdots & \vdots  \\
x_N^Tx_1 & x_N^Tx_2 & & x_N^Tx_3 \cdots & x_N^Tx_N\\
\end{bmatrix}
\end{equation}

Gramovu matice lze samozøejmì pou¾ít i vpøípadì pou¾ití jaderných funkcí.
V takovém pøípadì je pro výpoèet Gramovy matice $G$ pou¾ita, namísto standardního kartézského souèinu v prostoru pøíznakù $X$, jaderná funkce, která ale vlastnì pøedstavuje kartézský souèin v prostoru $F$, do kterého lze prostor $X$ pøevést pomocí urèité transformaèní funkce $\varphi$ (\ref{SVM}).

Gramova matice pro SVM klasifikátor vyu¾ívající jaderných metod vypadá následovnì:

\begin{equation}
\label{IMP-G}
G_{nonlinear} = \begin{bmatrix}
K(x_1, x_1) & K(x_1, x_2) & K(x_1, x_3) & \cdots & K(x_1, x_N)\\
K(x_2, x_1) & K(x_2, x_2) & K(x_2, x_3) & \cdots & K(x_2, x_N)\\
\vdots      & \vdots      & \vdots      & \vdots & \vdots     \\
K(x_N, x_1) & K(x_N, x_2) & K(x_N, x_3) & \cdots & K(x_N, x_N)\\
\end{bmatrix}
\end{equation}

V praktické èásti této práce je Gramova matice implementována ve tvaru $G_{nonlinear}$ a v závislosti na pou¾ité jaderné funkci (jaderná funkce mù¾e být i obyèejným kartézským souèinem v prostoru pøíznakù $X$) je matice pøedpoèítána pro pozdìj¹í pou¾ití pøi trénování klasifikátoru.


\chapter{Testy} \label{TESTY}

%./control.py common -d ../data/tweets/annotated.db -n 5 -c 100000 -t 1 -k RBF -a 0.5 -b 0.5
%##################################
%########## svm restults ##########
%##################################
%True positive = 93.0
%True negative = 92.8
%False positive = 7.2
%False negative = 7.2
%Precision = 0.928143712575
%Recall = 0.928143712575
%Accuracy = 0.928071928072
%F-measure = 0.928143712575
%##################################
%
%##################################

%./control.py common -d ../data/tweets/annotated.db -n 5 -c 100000 -t 2 -k RBF -a 0.5 -b 0.5
%##################################
%########## svm restults ##########
%##################################
%True positive = 94.0
%True negative = 91.4
%False positive = 8.6
%False negative = 8.6
%Precision = 0.916179337232
%Recall = 0.916179337232
%Accuracy = 0.915103652517
%F-measure = 0.916179337232
%##################################

%./control.py common -d ../data/tweets/annotated.db -n 5 -c 100000 -t 3 -k RBF -a 0.5 -b 0.5 > out.txt
%##################################
%########## svm restults ##########
%##################################
%True positive = 93.0
%True negative = 91.8
%False positive = 8.2
%False negative = 8.2
%Precision = 0.918972332016
%Recall = 0.918972332016
%Accuracy = 0.918489065606
%F-measure = 0.918972332016
%##################################


%./control.py common -d ../data/articles/annotated.db -n 5 -c 1000 -t 1 -k RBF -a 0.5 -b 0.5
%##################################
%########## svm restults ##########
%##################################
%True positive = 28.4
%True negative = 32.4
%False positive = 7.6
%False negative = 4.6
%Precision = 0.860606060606
%Recall = 0.860606060606
%Accuracy = 0.868571428571
%F-measure = 0.860606060606
%##################################


\section{Data}\label{TEST-DATA}
Jak bylo ji¾ zmínìno v úvodu (\ref{UVOD-MOTIVACE}), tato práce vznikla s cílem vytvoøit pro projekt M-Eco \footnote{http://www.meco-project.eu/}, ve kterém participovala výzkumná skupina NLP, pracující pøi Fakultì informaèních technologii VUT v Brnì.
Úkolem tohoto klasifikátoru mìla být klasifikace vstupního textu do tøíd a následné urèení, zda bude vstupní text zapsán do databáze a nebo nebude.
K tomuto úèelu byla vytvoøena datová sada pro trénování a testování Bayesoského klasifikátoru a SVM klasifikátoru.
Zdrojem dat, ze kterých byla vytvoøena datová sada, byla databáze projektu M-eco.

Pro potøeby této práce byly v¹ak vytvoøeny datové sady dvì, aby bylo mo¾né implementované klasifikátory na rùzných typech textù.
Jedna datová sada byla vytvoøea pro anglické tweety a druhá pro anglické èlánky.

Pro datovou sadu obsahující anglické tweety je datová sada rozdìlena do dvou tøíd -- \textbf{zabývající se osobními zku¹enostmi pisatelù s nemocemi (ale také zku¹enostmi z pisatelova okolí)}, nebo \textbf{nezabývající se tìmito zku¹enostmi}.
Tedy napøíklad tweet \textit{'I have a huge headache'} nebo \textit{'My dad feels sick'} je relevantní, naproti tomu \textit{'Canadians should expect to see more severe cases of swine flu.'} je nerelevantní. 

Pro datovou sadu obsahující anglické èlánky je potom datová sada, podobnì jako v pøípadì tweetù, ruènì anotována tak, ¾e jsou vstupní èlánky rozdìleny do následujících dvou tøíd -- \textbf{zabývající se skuteènou nákazou ve svìtì}, nebo \textbf{nezabývající se skuteènou nákazou ve svìtì}.

Jak vychází z odvození klasifikaèní rovnice, pou¾ité v Bayesovském klasifikátoru (viz \ref{SF-BAYES-MAT-bayes_theorem-deriv}), mìlo by rozlo¾ení relevantních a nerelevantních dat v trénovací a testovací mno¾inì být zhruba v pomìru 1:1.
Celkovì bylo ruènì anotováno pøibli¾nì 4500 anglických tweetù a pøibli¾nì 500 anglických èlánkù.
V databázi anotovaných anglických tweetù se nachází 502 relevantních a zbytek nerelevantních záznamù.
Pro databázi ruènì anotovaných anglických èlánku je tento pomìr 194 relevantních k 269 nerelevantním záznamùm.
Vzhledem k tomu, ¾e v testu chceme porovnávat dva klasifikátory, z nich¾ jeden má vlastnost limitující jeho uèící sadu, budeme pro oba klasifikátory pou¾ívat identickou trénovací sadu -- tzn. pro tweety bude datová sada pro testy obsahovat 502 relevantních a 502 nerelevantních záznamù a pro èlánky bude datová sada obsahovat 194 relevantních a 194 nerelevantních záznamù.

Pro testování byla pou¾ívána metoda \emph{$n$-fold cross-validation}.
Tato metoda rozdìluje vstupní datovou sadu na $n$ stejnì velkých dílù.
Jeden z tìchto dílù je pak pou¾it jako testovací sada a zbylých $n-1$ dílù je pou¾ito jako trénovací datová sada.
Metoda sestává z $n$ iterací, pøi èem¾ v ka¾dé iteraci se jako testovací datová sada pou¾ije jiná z $n$ èástí rozdìlených dat.
Jako celkový výsledek \emph{$n$-fold cross-validation} se pou¾ijí prùmìrné výsledky testù jednotlivých bìhù.

Pøi testování v implementaèní èásti této práce byla pou¾ita metoda \emph{5-fold cross-validation}, co¾ znamená, ¾e data byla rozdìlena na 5 stejných dílù a nad tìmito díly se provádìly testy.
Èíslo 5 bylo zvoleno kvùli poèítaèi, na kterém testy probíhaly.
Validaèní proces byl toti¾ paralelizován -- ka¾dá iterace v implementovaném programu vyu¾ívá právì jedno jádro a výsledky v¹ech jader jsou po dobìhnutí v¹ech iterací zprùmìrovány do jednoho výsledku.

\section{Testovací metriky} \label{TEST-METRIKY}
Aby bylo mo¾né nìjakým zpùsobem objektivnì porovnat rùzné výsledky klasifikátorù a také posléze porovat klasifikátory mezi sebou, je tøeba zavést vhodné metriky, popisující výsledky klasifikátoru.
V následující èásti práce budou tyto pou¾ité metriky vysvìtleny.

\subsection{Korelace}
Korelace je statistická metoda, která definuje vzájemný vztah mezi velièinami $X$ a $Y$.
Míru korelace urèuje koeficient korelace, nabývající hodnot $<-1, 1>$.
Jestli¾e korelaèní koeficient nabývá hodnoty -1, pak to znaèí, ¾e velièiny $X$ a $Y$ jsou na sobì zcela nezávislé.
Naopak nabývá-li korelaèní koeficient hodnoty 1, pak jsou na sobì velièiny pøímo závislé.

Korelaèní koeficient se vypoèítá jako 

\begin{equation}
  K(X,Y) = \frac{E(XY) - E(X)E(Y)}{\sqrt{E(X^2 - E(X)^2)} \sqrt{(E(Y^2) - E(Y)^2)}}
\end{equation}

kde $X$ jsou klasifikátorem automaticky spoèítané pravdìpodobnosti a $Y$ jsou pravdìpodobnosti zadané u¾ivatelem pøi anotaci ($0.01 = spam$ a $0.99 = ham$).

Pøi testování klasifikátoru se poèítá korelace mezi u¾ivatelem zadanou hodnotou vstupního textu (pøi anotaci testovací mno¾iny) a výsledkem klasifikátoru.
Èím více se tedy korelaèní koeficient blí¾í hodnotì 1, tím lep¹í výsledek klasifikátoru pøedstavuje.

Korelaci je tedy mo¾né poèítat pouze u Bayesovského klasifikátoru, který pøiøazuje ka¾dému klasifikovanému vstupu urèitou pravdìpodobnost (soft-klasifikace).
U SVM klasifikátoru hodnota, která by pøiøazovala pravdìpodobnost nále¾itosti do urèité tøídy neexistuje (hard-klasifikace), a proto není mo¾né spoèítat korelaci.

\subsection{Výsledek klasifikace}\label{TEST-VYS_KLAS}
Abychom byli schopni pøesnì zjistit jak klasifikátor testovací mno¾inu oklasifikoval, rozdìlíme oklasifikované vstupní texty do ètyø skupin:

\begin{itemize}
 \item \textit{True positive} -- poèet vstupních textù, které byly klasifikátorem správnì zaøazeny do relevantní tøídy.
 \item \textit{True negative} -- poèet vstupních textù, které byly klasifikátorem správnì zaøazeny do nerelevantní tøídy.
 \item \textit{False positive} -- poèet vstupních textù, které byly klasifikátorem ¹patnì zaøazeny do relevantní tøídy.
 \item \textit{False negative} -- poèet vstupních textù, které byly klasifikátorem ¹patnì zaøazeny do nerelevantní tøídy.
\end{itemize}

Toto rozdìlení je výsle;dkem porovnání pøedpokládaného výstupu klasifikátoru s reálným výstupem implementovaného klasifikátoru.
Je zøejmé, ¾e èím ménì záznamù se nachází ve False negative a False positive, tím lépe klasifikátor funguje.

\subsection{Precision, recall, accuracy, F1-measure}
Dal¹ími metrikami, pomocí nich¾ budeme moci porovnávat výsledky klasifikátorù, jsou funkce precision(pøesnost), recall(odezva), accuracy(pøesnost\footnote{'accuracy' a 'precision' se do èeského jazyka pøekládají stejnì, tzn. jako pøesnost, nicménì jde o jiné metriky pro porovnávání klasifikátorù. Z toho dùvodu budeme metriky nazývat anglickými názvy.}) a F1-measure.
Pro v¹echny vý¹e zmínìné metody porovnání klasifikátorù se pøedpokládá, ¾e jsme schopni vypoèítat hodnoty \textit{True positive}, \textit{True negative}, \textit{False positive} a \textit{False negative} (viz \ref{TEST-VYS_KLAS}).

\subsubsection{Accuracy}
Nejjednodu¹¹í metrikou pro porovnávání pøesnosti klasifikátorù je metrika accuracy.
Tato metrika definuje pomìr, jakým jsou ve výsledcích zastoupeny správnì klasifikované vstupy.

Lze ji jednodu¹e vypoèítat následovnì:
$$
  Accuracy = \frac{true\_positive + true\_negative}{true\_positive + true\_negative + false\_positive + false\_negative}
$$

Nevýhodou této metriky v¹ak je, ¾e nebere v potaz poèty záznamù v jednotlivých tøídách.

\subsubsection{Precision, recall}
$$
  Precision = \frac{true\_positive}{true\_positive + false\_positive}
$$
$$
  Recall = \frac{true\_positive}{true\_positive + false\_negative}
$$
Precision tedy mù¾eme definovat jako pomìr správnì oklasifikovaných relevantních záznamù vùèi v¹em oklasifikovaným relevantním záznamùm.
Recall je potom pomìr správnì oklasifikovaných relevantních záznamù vùèi skuteènì relevantním záznamùm.

\subsubsection{F1-measure}
$$
  F1measure = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
$$
F1-measure je jedna z metrik nejèastìji pou¾ívaných pro hodnocení klasifikátorù.
De facto jde o vá¾ený prùmìr precision a recall.
F1-measure nabývá hodnot $<0,1>$, kde 0 je nejhor¹í skóre popisující výsledek klasifikátoru a 1 je nejlep¹í.

\section{Testy Bayesovského klasifikátoru}
Nyní pøistupme k pøedstavení výsledkù testù Bayesovského klasifikátoru.
V této kapitole budeme hledat optimální nastavení klasifikátoru pro klasifikaci jednotlivých typù textu (anglické tweety a anglické èlánky).
V testech se budeme zamìøovat pøedev¹ím na porovnávání rùzných zpùsobù tokenizace, a to zvlá¹» textovými tokeny a zvlá¹» speciálními pøíznaky v textu.

Pro testování klasifikace anglických tweetù je pou¾ita datová sada manuálnì anotovaných tweetù a pro testování klasifikace anglických èlánkù je pou¾ita datová sada manuálnì anotovaných èlánkù (viz. \ref{TEST-DATA}).

\subsection{Testy textové tokenizace}
Pro zji¹tìní ideální délky $n$-tic textových tokenù jsme aplikovali testy pro postupnì se zvy¹ující $n$, a to, dokud se zlep¹ovala hodnota korelaèního koeficientu (korelace) a F1-measure (viz \ref{TEST-METRIKY}).

\subsubsection{Tweety}
%{'emoticon': 3, 'sentence': 1, 'url': 0, 'tag': 1, 'time': 3, 'date': 4, 'email': 2} best
Jak ji¾ bylo vý¹e v této kapitole zmínìno, pro trénování anglických tweetù je pou¾ívána datová sada obsahující 502 relevantních a 502 nerelevantních tweetù.

Následující tabulka øíká, jak se klasifikátor choval pøi zmìnì nastavení maximální délky za sebou jdoucích $n$-tic textových tokenù:

\begin{center}
\begin{tabular}{lcccc}
\toprule
n    & 1 & 2 & 3 & 4 \\
\midrule
True positive  & 83.2   & 90.0   & 90.0   & 90.0   \\
True negative  & 93.8   & 91.2   & 91.2   & 91.2   \\
False positive &  6.2   &  8.8   &  8.8   &  8.8   \\
False negative & 16.8   & 10.0   & 10.0   & 10.0   \\
Korelace       & 0.8188 & 0.8363 & 0.8363 & 0.8363 \\
Precision      & 0.9306 & 0.9109 & 0.9109 & 0.9109 \\
Recall         & 0.832  & 0.9    & 0.9    & 0.9    \\
Accuracy       & 0.885  & 0.906  & 0.906  & 0.906  \\
F1-measure     & 0.8786 & 0.9054 & 0.9054 & 0.9054 \\
\bottomrule
\end{tabular}
\end{center}


\subsubsection{Èlánky}
%{'emoticon': 4, 'sentence': 1, 'url': 4, 'tag': 3, 'time': 3, 'date': 1, 'email': 0} best
Obdobnì jako v pøedchozím pøípadì tweetù budeme testovat anglické èlánky.
V databázi ruènì anotovaných anglických èlánkù se nachází 194 relevantních a 269 nerelevantních záznamù.
Jak ji¾ bylo v úvodu této kapitoly zmínìno pro trénování a testování budeme pou¾ívat vyvá¾enou sadu 194 relevantních a 194 nerelevantních èlánkù.

Pou¾ijeme stejný test jako v pøípadì testu textové tokenizace tweetù.
Budeme se sna¾it urèit optimální délku $n$-tic textových tokenù.
Výsledky jednotlivých testù jsou zaznamenány v následující tabulce:

\begin{center}
\begin{tabular}{lcccc}
\toprule
n    & 1 & 2 & 3 & 4 \\
\midrule
True positive  & 21.0   & 21.4   & 21.4   & 21.4   \\
True negative  & 25.6   & 29.4   & 29.4   & 29.4   \\
False positive & 11.4   & 7.6    & 7.6    & 7.6    \\
False negative & 16.0   & 15.6   & 15.6   & 15.6   \\
Korelace       & 0.3198 & 0.4286 & 0.4286 & 0.4286 \\
Precision      & 0.6481 & 0.7379 & 0.7379 & 0.7379 \\
Recall         & 0.5676 & 0.5784 & 0.5784 & 0.5784 \\
Accuracy       & 0.6297 & 0.6865 & 0.6865 & 0.6865 \\
F1-measure     & 0.6052 & 0.6485 & 0.6485 & 0.6485 \\
\bottomrule
\end{tabular}
\end{center}


\subsection{Testy speciálních pøíznakù}
Abychom mohli otestovat vliv jednotlivých typù speciálních pøíznakù na klasifikaci Bayesovského klasifikátoru, spustíme nejprve test klasifikátoru vyu¾ívajícího pouze textových tokenù.
Takto získáme kontrolní výsledek, který bude následné porovnán s výsledky klasifikace s vyu¾itím jednotlivých speciálních pøíznakù.
V implementovaném programu byla vytvoøena metoda, která vliv v¹ech mo¾ných speciálních pøíznakù na klasifikaci porovná a najde optimální kombinaci.

Nyní pøedstavíme jaký vliv má výbìr optimálních speciálních pøíznakù na klasifikaèní schopnosti Bayesovského klasifikátoru.
Pro ka¾dou datovou sadu bude prezentován výsledek bez speciálních pøíznakù a poté výsledek s vhodnì vybranými speciálními pøíznaky.

\subsubsection{Tweety}
Následující tabulka popisuje, jak se zlep¹í klasifikaèní schopnosti Bayesovského klasifikátoru  pøi výbìruím výbìru vhodných speciálních pøíznakù pøi trénování a testování na datové sadì tweetù.

\begin{center}
\begin{tabular}{lcccc}
\toprule
               & Bez spec. pøíznakù & S optim. spec. pøíznaky  \\
\midrule
True positive  & 83.8               & 83.6   \\
True negative  & 92.6               & 93.4   \\
False positive &  7.4               & 6.6    \\
False negative & 16.2               & 16.4   \\
Korelace       & 0.8035             & 0.8114 \\
Precision      & 0.9189             & 0.9268 \\
Recall         & 0.838              & 0.836  \\
Accuracy       & 0.882              & 0.885  \\
F1-measure     & 0.8765             & 0.8791 \\
\bottomrule
\end{tabular}
\end{center}

\subsubsection{Èlánky}
Obdobnì jako pro klasifikaci tweetù, následující tabulka popisuje zlep¹ení klasifikaèní schopnosti klasifikátoru díky vhodnému výbìru speciálních pøíznakù.
V tomto pøípadì ale testy klasifikátoru probìhly nad datovou sadou anglických èlánkù.

\begin{center}
\begin{tabular}{lcccc}
\toprule
               & Bez spec. pøíznakù & S optim. spec. pøíznaky  \\
\midrule
True positive  & 21.8               & 21.0   \\
True negative  & 25.6               & 25.6   \\
False positive & 11.4               & 11.4   \\
False negative & 16.0               & 16.0   \\
Korelace       & 0.2873             & 0.3198 \\
Precision      & 0.6481             & 0.6481 \\
Recall         & 0.5676             & 0.5676 \\
Accuracy       & 0.6297             & 0.6297 \\
F1-measure     & 0.6052             & 0.6052 \\
\bottomrule
\end{tabular}
\end{center}


\subsection{Interpretace výsledkù testù}
Z výsledkù testù zabývajících se délkou textových tokenù vyplývá, ¾e jak v pøípadì klasifikace èlákù, tak v pøípadì klasifikace tweetù je nejlep¹ím øe¹ením vyu¾ít alternativní metodu tokenizaci textu s maximální délkou $n$-tic rovnou dvìma.
Takovéto nastavení klasifikátoru výraznì zlep¹ilo klasifikaèní schopnosti Bayesovského klasifikátoru oproti ostatním variantám.
Je-li délka $n$-tic vìt¹í ne¾ dva, klasifikátoru to u¾ v klasifikaci nepomáhá a toto nastavení pouze zvy¹uje velikost slovníku klasifikátoru, podle kterého potom klasifikátor klasifikuje vstupní texty.
Dùvodem pro to, ¾e umo¾nìní vyu¾ití del¹ích $n$-tic ji¾ nezlep¹uje klasifikaèní schopnosti Bayesovského klasifikátoru je zøejmì ten, ¾e tøi za sebou jdoucí slova jsou ji¾ dosti specifickým pøíznakem, který se v textu nevyskytuje pøíli¹ èasto, a tudí¾ nepøispívají klasifikátoru ke zlep¹ení klasifikaèních schopností.
Kdyby se výraznì zvìt¹ila trénovací a testovací datová sada, je mo¾né, ¾e by i del¹í $n$-tice zlep¹ovaly klasifikaèní schopnosti Bayesovského klasifikátoru.

Testy zabývající se tokeny speciálních pøíznakù získávaných z textu ukázaly, ¾e v pøípadì tweetù tyto tokeny speciálních pøíznakù mírnì zlep¹ují klasifikaèní schopnosti klasifikátoru.
V pøípadì klasifikace èlánkù ov¹em tak dobrý úspìch nemìly.
Zlep¹ení klasifikaèních schopností Bayesovského klasifikátoru bylo zcela minimální.

\section{Testy SVM klasifikátoru}
Stejnì jako pøi testování Bayesovského klasifikátoru, je SVM klasifikátor testován na dvou testovacích sadách anglických tweetù a anglických èlánkù.
Na rozdíl od Bayesovského klasifikátoru ale nemusíme volit jaké speciální pøíznaky budou pro klasifikaci vyu¾ity.
Klasifikátoru umo¾níme si nalézt optimální øe¹ení ze v¹ech tokenù (textových tokenù, i tokenù speciálních pøíznakù), které implementovaný program umo¾òuje vytvoøit.
Na druhou stranu stejnì jako v Bayesovském klasifikátoru budeme hledat optimální délku $n$-tic textových tokenù, která by mìla být taková, aby zlep¹ovala klasifikaèní schopnosti klasifikátoru.
Jestli¾e toti¾ bude zvolena pøíli¹ velká délka tìchto maximálních $n$-tic, zbyteènì se zvìt¹í dimenzionalita pøíznakù a klasifikaèní schopnosti klasifikátoru se sní¾í.

\subsection{Tweety}
\subsubsection{Testy textové tokenizace}
\subsubsection{Testy pøíznakù}
\subsection{Èlánky}
\subsubsection{Testy textové tokenizace}
\subsubsection{Testy pøíznakù}
\subsection{Interpretace výsledkù testù}
\section{Porovnání Bayesovského klasifikátoru a SVM klasifikátoru}
\subsection{Shrnutí výsledkù testù}


\chapter{Závìr} 


\nocite{automatic_sumarization}
\nocite{anonymisation}
\nocite{nlg}
\nocite{clustering}
\nocite{Manning}
\nocite{SRM}
\nocite{boosting_bayesian}
\nocite{Malik2007thesis}
%=========================================================================
