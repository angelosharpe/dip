%=========================================================================
% (c) Michal Bidlo, Bohuslav Køena, 2008

\chapter{Úvod}
Klasifikace dokumentù podle tématu je jednou z úloh oboru zpracovávání pøirozeného jazyka (Natural Language Processing - NLP).
Zpracování pøirozeného jazyka je oborem aplikace výpoèetních modelù pro øe¹ení úkolù, které se urèitým zpùsobem vztahují k textu libovolného pøirozeného jazyka.
Historie zpracovávání pøirozeného jazyka zaèíná v padesátých letech 20. století, kdy Alan Turing ve svém èlánku \uv{Computing Machinery and Intelligence} \cite{turing_50} poprvé publikoval takzvaný \uv{Turingùv test}.
Od té doby doznal obor zpracovávání pøirozeného jazyka velkých zmìn a v souèasné dobì se v nìm nejvìt¹í mìrou pou¾ívají statistické metody a metody strojového uèení (machine learning).

Významnou úlohou øe¹enou ve zpracovávání pøirozeného jazyka je klasifikace textu.
Tato úloha spadá do vý¹e zmínìné skupiny statistických metod a metod strojového uèení.
Metody klasifikace textu lze pou¾ít pro velké mno¾ství úloh, které se týkají zpracovávání pøirozeného jazyka, a to pøedev¹ím díky jejich flexibilitì a ji¾ velmi dobøe zpracované teorii.

V této práci se nejprve budeme zabývat obecnými pøístupy pou¾ívanými pro øe¹ení úloh zpracovávání pøirozeného jazyka a následnì pøedstavíme dvì zvolené klasifikaèní metody.
Jednu probabilistickou a jednu neprobabilistickou.

Tou první, probabilistickou metodu, která je v této práci podrobnì popsána, je metoda nazývaná \textit{Bayesovský klasifikátor}, vyu¾ívající ke klasifikaci textu Bayesova teorému.
Bayesovský klasifikátor, nebo také Bayesovský filtr, jak je obèas nazýván, je velmi èasto pou¾íván e-mailovými klienty pro zji¹»ování, zda je e-mailová zpráva nevy¾ádanou po¹tou, nebo ne.

Druhou zvolenou metodou je neporobabilistická klasifikaèní metoda \textit{SVM}(support vector machines).
Je¾ pomocí namapování vstupního textu do $N$-dimenzionálního prostoru a následným rozdìlením tohoto prostoru na dva poloprostory doká¾e vstupní text klasifikovat.

Obì metody jsou v této práci podrobnì rozebrány jak z teoretického, tak praktického hlediska.

Souèástí této práce je mimo toretické èásti prezentované v tomto dokumentu, také praktická, implementaèní èást, ve které je vytvoøen program a implementovány obì vý¹e zmínìné klasifikaèní metody, které jsou testovány a porovnány.
Souèástí praktické èásti je také mno¾ina trénovacích a testovacích dat, které jsou potom pou¾ívány v testech.
V této práci bude také popsána implementaèní èást, ve ní¾ budou specifikovány ve¹keré problémy a pou¾ité postupy pøi implementaci praktické èásti.

Na závìr budou prezentovány výsledky získané z porovnání dvou vý¹e zmínìných implementovaných klasifikátorù.

\section{Zadání práce a motivace} \label{UVOD-MOTIVACE}
Zadáním této práce bylo seznámit se s problematikou klasifikace dokumentù dle tématu a zvolit si dvì metody, kterými se budu podrobnìji zabývat.
Následnì tyto dvì metody analyzovat a implementovat program, který pomocí tìchto metod bude klasifikovat vstupní dokumenty do urèitých tøíd.
U obou metod jsem mìl dle zadání dbát na výbìr vhodných pøíznakù z textu.

Pro potøeby této práce jsem anotoval velkou sadu dokumentù, které budu následnì pou¾ívat pro trénování a testování mnou vytvoøeného programu.
Na tìchto datech následnì porovnám dvì zvolené metody klasifikace pomocí standardních metrik pro hodnocení klasifikátorù.

Toto téma diplomové práce jsem si zvolil, jeliko¾  jsem témìø dva roky pracoval na projektu M-eco\footnote{http://www.meco-project.eu/} ve skupinì NLP na VUT FIT, kde jsem se zabýval klasifikací tweetù\footnote{krátkých zpráv o maximální délce 140 znakù, je¾ u¾ivatelé sociální sítì Twitter(http://twitter.com/) pí¹í na své profily}.
Chtìl bych této práce vyu¾ít k tomu, abych prohloubil své znalosti o problematice klasifikace dokumentù a hloubìj¹ímu porozumìní klasifikaèních metod, zejména pak klasifikátoru zalo¾eného na SVM, jeliko¾ metoda SVM je velmi dobøe variabilní a lze ji pou¾ít pro øe¹ení velkého mno¾stí rùzných problémù.

Bayesovský klasifikátor implementovaný této práci byl v reálu dlouho pou¾íván na filtrování tweetù pøidávaných do databáze M-eco.

\section{Návaznost práce na semstrální projekt} \label{UVOD-NAVAZNOST}
Tato práce úzce navazuje odevzdaný dokument semestrálního projektu, jen¾ byl vytvoøen pro stejnojmenný pøedmìt.

Semestrální projekt obsahoval úvodní kapitolu o problematice zpracování pøirozeného jazyka a popis problematiky naivního Bayesovského klasifikátoru, jen¾ byl pro tuto práci také implementován (\ref{BAYES}).
Byla vytvoøena malá testovací sada, pro trénování a testování tohoto klasifikátoru.
Nad touto datovou sadou a klasifikátorem byly následnì provedeny testy v nich¾ byl zhodnocen vliv tokenizace (\ref{TOKENIZACE}) na klasifikaèní schopnosti implementovaného Bayesovského klasifikátoru.
V odevzdané práci je¹tì nebyly implementovány speciální pøíznaky (\ref{TOKENIZACE-SPEC}).



\chapter{Zpracování pøirozené øeèi}
V této kapitole se budeme struènì zabývat historií (\ref{NLP-H}) oboru zpracovávání pøirozeného jazyka, poté si pøiblí¾íme hlavní úkoly, které se v oboru zpracovávání pøirozeného jazyka øe¹í (\ref{NLP-HU}) a nakonec se podrobnìji podíváme na klasifikaèní úlohy(\ref{NLP-KU}), které jsou hlavním tématem této práce.

\section{Historie} \label{NLP-H}
Obor zpracovávání pøirozeného jazyka je se vyvíjí soubì¾nì s historií vývoje výpoèetní techniky.
Po roce 1945, kdy spoleèensko politická situace ve svìtì umo¾nila zmìnu hlavních dosavadních smìrù výzkumu v oblasti výpoèetní techniky od pùvodnì pøevá¾nì vojenského vyu¾ití (napø. kryptografie, kryptoanalýza, atd.) k dal¹ím vìdním oborùm.
Díky tomu se zaèal rozvíjet také obor zpracovávání pøirozeného jazyka.

Jak ji¾ bylo zmínìno v úvodu, jedním z prvních významných mílníkù v historii zpracovávání pøirozeného jazyka byl èlánek Alana Turinga s názvem \uv{Computing Machinery and Intelligence} \cite{turing_50}, v nìm¾ Turing publikoval takzvaný \textit{Turingùv test} jako¾to kritérium inteligence.
Aby poèítaèový program pro¹el \textit{Turingovým testem}, nesmí nestranný soudce poznat z obsahu konverzace mezi programem a èlovìkem (konverzace probíhá v reálném èase), která strana je která.

Vìdci zabývající se zpracováváním pøirozeného jazyka logicky hledali pomoc v lingvistice.
V roce 1957  publikoval americký lingvista Noam Chomsky èlánek \uv{Syntactic structures}, který znamenal zcela nový pohled na lingvistiku.
Formuloval v nìm teorii takzvané \textit{Transformaèní gramatiky}.
V ní¾ tvrdil, ¾e ka¾dá vìta libovolného jazyka má dvì úrovnì reprezentace
\begin{itemize}
 \item \textit{hloubkovou} -- reprezentující sémantické vztahy ve vìtì, kterou lze pøevést na povrchovou úroveò
 \item \textit{povrchovou} -- reprezentující fonémickou formu vìty
\end{itemize}
Hluboké úrovnì v¹ech jazykù podle Chomského vykazují znaènou podobnost, která se vytrácí v úrovních povrchových.
Témìø v¹echny výzkumy v oboru zpracovávání pøirozeného jazyka byly po roce 1957 silnì ovlivnìny touto publikací.

Jednou z prvních øe¹ených úloh v oboru zpracovávání pøirozené øeèi bylo vytvoøení automatických pøekladaèù, tedy programù, které bez lidského pèispìní doká¾ou pøelo¾it vstupní text z jednoho pøirozeného jazyka do druhého.
Uspokojivé øe¹ení tohoto úkolu v¹ak v té dobì nebylo nalezeno a ani v roce 1966 je¹tì výzkumníci nebyli nikterak blízko k vyvinutí takového softwaru.
V roce 1966 proto vydal americký vládní výbor \uv{The Automatic Language Processing Advisory Committee} (ALPAC) zprávu shrnující dosavadní výsledky výzkumu a konstatující, ¾e \uv{Nebyl vyvinut ¾ádný strojový pøekladaè obecných vìdeckých textù a ¾ádný takový pøekladaè nebude vyvinut ani v blízké budoucnosti}.
Takto formulovaný závìr zprávy zapøíèinil výrazné ¹krty v rozpoètech vìdeckých týmù, zabývajících se výzkumem zpracovávání pøirozeného jazyka a pøekladaèù.

V 60. letech byl americkým matematikem nìmeckého pùvodu Josephem Weizenbaumem vytvoøen program \uv{ELIZA}, co¾ byl jednoduchý robot, urèený pro konverzaci s èlovìkem (chatbot).
Konverzace s tímto robotem se zakládala na opakování výrokù, které u¾ivatel zadal a na kladení velmi jednoduchých otázek, zalo¾ených na klíèových slovech, nalezených v pøedchozí konverzaci.
Program se v diskusi choval jako psychoterapeut, reagující v¾dy bezprostøednì na pøedchozí výrok èlovìka, tedy \uv{pacienta}.
Bìhem sedmdesátých let bylo vyvinuto mnoho dal¹ích programù pro konverzaci s u¾ivatelem podobných programu \textit{ELIZA}, napøíklad \textit{PARRY} nebo \textit{Jabberwacky}.

Do 80. let 20. století pou¾ívala drtivá vìt¹ina systémù pro zpracovávání pøirozeného jazyka velmi slo¾itá ruènì zadávaná pravidla.
V 80. letech v¹ak byly poprvé pøedstaveny metody strojového uèení, které díky stále rostoucímu výkonu výpoèetní techniky umo¾òovaly generovat tato pravidla automaticky a dosahovat tak pøi zpracovávání pøirozeného jazyka stále lep¹ích výsledkù.
S nástupem strojového uèení se odvìtví zpracovávání pøirozeného jazyka zaèalo zamìøovat na statistické metody, je¾ k øe¹ení problémù pøistupovaly jinak ne¾ metody dosavadní.
Jejich hlavním principem byla práce s pravdìpodobnostními modely a váhami jednotlivých rozhodnutí.
Tímto zpùsobem bylo mo¾né efektivnìji øe¹it vìt¹inu NLP úkolù.
Vzhledem k tomu ¾e obor zpracovávání pøirozeného jazyka pracuje s daty vytvoøenými èlovìkem, je¾ mohou obsahovat rùzné chyby, pracují statistické metody o znaènì spolehlivìji ne¾ døívìj¹í ruènì psaná pravidla.

V souèasné dobì je výzkum zpracovávání pøirozeného jazyka orientován pøedev¹ím na vytvoøení autonomních a semiautonomních uèících agoritmù, tedy algoritmù, schopných uèit se z dat, která pøedtím nebyla ruènì anotována, a nebo z kombinace anotovaných a neanotovaných dat.
%%%%%%%%%
%%%%%%%%
%%%%%%%%
%%%%%%%%%%

\section{Hlavní úkoly øe¹ené v NLP} \label{NLP-HU}
Obor zpracovávání pøirozeného jazyka je velmi rozsáhlý a existuje v nìm mimo klasifikace textu velké mno¾ství dal¹ích úloh k øe¹ení.
V následující èásti této práce se podíváme na hlavní úkoly, kterými se oblast NLP zabývá.

\subsection{Automatická sumarizace}
Úkolem automatické sumariace je redukovat vstupní text nebo sadu textù do nìkolika slov, nebo krátkého odstavce, popisujícího sémantický obsah vstupního textu.
Základním principem automatické sumarizace je zpracovávání slov, frází a vìt ze vstupního souboru, respektive vstupních souborù.
Z vý¹e zmínìných dat program pro automatickou sumarizaci vygeneruje vnitøní sémantický popis tìchto dat.
Ze sémantického popisu potom mù¾e vygenerovat buï sadu vhodných slov, popisujících daný soubor, nebo je mo¾né pou¾ít metody pro generování pøirozeného jazyka a vytvoøit tak vìtný popis vstupního textu.

\subsection{Generování pøirozeného jazyka} \label{NLP-HU-generovani}
Generování pøirozeného jazyka má vytvoøit výstup v pøirozeném jazyce z interní reprezentace v poèítaèi.
De facto je generátor pøirozeného jazyka pøekladaè, pøekládající data z jednoho jazyka do druhého (z vnitøní reprezentace poèítaèe do pøirozeného jazyka).
Tato úloha je opakem úlohy porozumìní pøirozenému jazyku (\ref{NLP-HU-porozumeni}).

\subsection{Porozumìní pøirozenému jazyku} \label{NLP-HU-porozumeni}
Programy pro porozumìní pøirozenému jazyku mají pøevést vstupní text do podoby zpracovatelné poèítaèem, tedy porozumìt textu a vygenerovat vnitøní reprezentaci onìch vstupních dat.
Proces porozumìní pøirozenému jazyku je sice opakem úlohy generování pøirozeného jazyka (\ref{NLP-HU-generovani}), ale je znaènì slo¾itìj¹í, a to nejen kvùli rozmanitosti vstupního jazyka a tudí¾ mo¾nosti výskytu neznámých slov, ale také kvùli nutnosti zvolení vhodných syntaktických a sémantických schémat aplikovaných ve vstupním textu.

\subsection{Odpovídání na otázky}
V oblasti odpovídání na otázky se programátoøi pokou¹ejí vytvoøit program, který by dokázal korektnì odpovìdìt na u¾ivatelem zadanou vstupní otázku formulovanou v pøirozeném jazyce.
V roce 2011 vytvoøila spoleènost IBM poèítaè s názvem \textit{Watson} specializovaný na odpovídání na otázky, který následnì vyhrál americkou vìdomostní soutì¾ \textit{Jeopardy!}, kdy¾ porazil dva nejlep¹í hráèe této soutì¾e v její historii.

\subsection{Odstraòování víceznaènosti slov v textu}
Odstraòování víceznaènosti slov v textu je významnou úlohou napomáhající správnému porozumìní textu.
Víceznaèná slova jsou v textu identifikována a poté je vyhledáván jejich správný význam v kontextu vstupního textu.
Po zji¹tìní významu slova je slovo nahrazeno nevýceznaèným synonymem.
Tato úloha je velmi dùle¾itá napøíklad pro zlep¹ování kvality a pøesnosti internetových vyhledávaèù.
Velmi zajímavé øe¹ení tohoto problému publikovala Rada Mihalcea v \uv{Using Wikipedia for Automatic Word Sense Disambiguation} \cite{wp_word_sense}, která øe¹ila identifikaci víceznaèných slov za pomoci textového obsahu internetové encyklopedie \textit{Wikipedia}\footnote{http://wikipedia.org/}.

\subsection{Strojový pøeklad}
Posledním úkolem oboru zpracovávání pøirozeného jazyka, kterým se zde budeme zabývat, je strojový pøeklad.
Jak bylo vý¹e zmínìno v kapitole o historii zpracovávání pøirozeného jazyka, je strojový pøeklad jednou ze zásadních úloh, kterými se obor zpracovávání pøirozeného jazyka zabývá.
Z øady rozmanitých pøístupù k øe¹ení strojového pøekladu zmíníme nejstar¹í metodu, zalo¾enou na sadì pravidel, pomocí nich¾ je pøeklad uskuteèòován a také statistické pøekladaèe, jako napøíklad \textit{Google translator}, vyu¾ívající pro pøeklad statistických modelù.


\subsection{Klasifikaèní úlohy} \label{NLP-KU}
Nyní se podíváme na následující tøi klasifikaèní úlohy:
\begin{itemize}
 \item Anonymizace \ref{NLP-KU-anonym}
 \item Klasifikace tématu \ref{NLP-KU-klas_tem}
 \item Filtrování spamu \ref{NLP-KU-spam_filt}
\end{itemize}

\subsubsection{Anonymizace}\label{NLP-KU-anonym}
Klasifikaèní úloha anonymizace není pøi zpracovávání pøirozeného jazyka pøíli¹ èasto øe¹ená.
Jejím cílem je odstranìní citlivých referencí (napøíklad osobních informací -- rodného èísla, e-mailové adresy, atd.) z tìla daného textu, co¾ umo¾òuje následnì anonymizovaný text vyu¾ít napøíklad pro výzkumné úèely.
Anonymizace na rozdíl od spam filteringu vy¾aduje daleko jemnìj¹í pøístup ke klasifikaci, jeliko¾ pracuje nikoliv s celým dokumentem, ale pouze s jeho èástmi, nìkdy dokonce jen nìkolika slovy, nebo vìtami.
Pro anonymizaci jsou nejèastìji pou¾ívány tyto 3 postupy: 
\begin{itemize}
 \item Odstranìní -- odstranìní v¹ech citlivých informací z dokumentu a jejich nahrazení výplní
 \item Pseudoanonymizace -- nahrazení v¹ech citlivých informací náhodnými hodnotami stejného typu
 \item Kategorizace -- nahrazení v¹ech citlivých informací kategorií do které spadají.
\end{itemize}
Zpùsoby u¾ití anonymizace viz obr. \ref{NLP-KU-anonym-img}.
\begin{figure}[h]
    \begin{center}
      \includegraphics[width=15cm,keepaspectratio]{fig/anonymization}
      \caption{Zpùsoby pou¾ití anonymizace.} 
      \label{NLP-KU-anonym-img}
    \end{center}
\end{figure}

\subsubsection{Klasifikace tématu}\label{NLP-KU-klas_tem}
Klasifikace tématu je úloha pøiøazování názvù témat ke vstupním textovým dokumentùm.
Typicky daný vstupní text pokrývá vìt¹í mno¹ství témat.
Metody pro klasifikaci tématu mohou být zalo¾eny napøíklad na skrytých Markovových modelech.
Výstupem klasifikátoru tématu pro vstupní text bývá velmi èasto kromì seznamu tøíd témat, kterých se zøejme vstupní text týká, také seznam pravdìpodobností definující míru nále¾itosti do jednotlivých tìchto tøíd.

\subsubsection{Filtrování spamu}\label{NLP-KU-spam_filt}
Zájem o klasifikaèní problém filtrování spamu v posledních letech velmi výraznì vzrostl, a to pøedev¹ím kvùli mno¾ství nevy¾ádané po¹ty (spamu), kterou u¾ivatelé dostávají do svých e-mailových schránek.
Jsou dvì mo¾nosti jak úloha filtrování spamu mù¾e fungovat.
Buï jsou zprávy filtrovány na základì obsahu (a» u¾ textového, nebo jiného) nebo na základì metainformací v hlavièce zprávy.
Metody zabývající se filtrováním na základì obsahu e-mailu velmi èasto vyu¾ívají toho, ¾e vìt¹ina zpráv obsahuje nìjakou textovou informaci.
Podle ní potom klasifikují, zda je zpráva nevy¾ádanou po¹tou.
Klasifikátory urèující, zda je zpráva spam nebo ne mají pro klasifikaci e-mailù velmi èasto asymetrické ohodnocení pøi klasifikaci.
Chybné oznaèení vy¾ádané po¹ty jako spamu, vedoucí k následnému odstranìní zprávy, je toti¾ vìt¹í problém, ne¾ oznaèení spamu jako vy¾ádané po¹ty.

V tomto èlánku se budeme zabývat právì pøedev¹ím metodami filtrování spamu, a to nejen pro pou¾ítí pøi jeho odfiltrovávání z e-mailové po¹ty, ale také pro oznaèování relevantních textových vstupù vzhledem k danému tématu.

\chapter{Klasifikace dokumentù}
Jedním z úkolù této práce je získat pøehled o klasifikaci textových dokumentù, seznámit se s metodami klasifikace a aplikovat je na vytvoøenou datovou sadu.
\section{Definice klasifikace} \label{CLASS-DEF}

\begin{definition}
 Klasifikace je èinnost, která rozdìluje objekty do tøíd (kategorií) podle jejich spoleèných vlastností.
\end{definition}

Tøída v kontextu zpracovávání pøirozeného jazyka je mno¾ina objektù, vyznaèujících se urèitou spoleènou vlastností nebo vlastnostmi, která/é danou tøídu popisuje/í.
Klasifikace je potom èinnost, která pøiøøzuje objekty do daných tøíd.
Nadále se budeme zabývat zejména klasifikací textu.

Obecnì mìjme tedy prostor objektù $X$ a mno¾inu tøíd $Y$.
Operaci klasifikace potom odpovídá funkci $f$:

\begin{equation}
f: X \rightarrow Y
\end{equation}

Klasifikaèní funkce $f$ tedy pøiøadí jeden objekt z mno¾iny objektù právì do jedné tøídy.
Mù¾e ov¹em nastat situace, kdy jeden objekt odpovídá kritériím pro zaøazení do více tøíd.
Napøíklad budeme klasifikovat textovou zprávu, je¾ mù¾e z hlediska obsahu zapadnout do více tøíd, napøíklad do tøídy osobních zku¹eností (pisatel pí¹e o vlastní zku¹enosti) a do tøídy relevantní k tématu zdraví (\textit{'Bolí mì hlava.}).
V tomto pøípadì musíme modifikovat klasifikaèní funkci $f$ následovnì:

\begin{equation}
f: X \rightarrow 2^Y
\end{equation}

kde $2^Y$ oznaèuje potenèní mno¾inu v¹ech tøíd.
Tato forma klasifikace se také oznaèuje jako \textit{multi-label klasifikace}.
V \textit{multi-label} klasifikaci se ke ka¾dé tøídì mù¾e pøiøadit reálné èíslo $p \in <0,1>$, které definuje míru nale¾itosti klasifikovaného objektu do pøiøazených tøíd.
Tomuto se také øíká \textit{soft klasifikace}.
Naproti tomu, kdy¾ je pøi klasifikaci pøiøazen objekt do tøídy \uv{napevno} (ano, patøí tam / ne, nepatøí tam), pak tento zpùsob klasifikace nazýváme \textit{hard klasifikace}.

\section{Klasifikaèní pøístupy}
Existují dva hlavní pøístupy pøi øe¹ení klasifikaèních úloh:
\begin{itemize}
 \item Probabilistické
 \item Neprobabilistické
\end{itemize}

\subsection{Probabilistické klasifikátory}
Jedním ze zpùsobù jak vytvoøit funkèní klasifikátor je vyu¾ít teorii pravdìpodobnosti.
Klasifikátory fungující na bázi pravdìpodobnostních výpoètù urèují pravdìpodobnosti, se kterými daný objekt spadá do nìkteré tøídy.
Do které tøídy respektive kterých tøíd objekt zapadá, je následnì urèeno pomocí pøedem definovaného prahu (threshold).

Probabilistické metody jsou napøíklad metody zalo¾ené na Bayesovì teorému.
Bayesùv teorém a jeho pou¾ití pro klasifikaci je podrobnì popsán v kapitole \ref{SF-BAYES}.

\subsection{Neprobabilistické klasifikátory}
Kromì probabilistických metod exustují také neprobabilistické metody pro klasifikaèní úlohy.
Neprobabilistické pøístupy se vìt¹inou sna¾í pøímo vymodelovat klasifikaèní funkci a nesna¾í se klasifikaci zjistit za pomocí pravdìpodobnostních výpoètù.
Asi nejznámìj¹í neprobabilistickou metodou je metoda SVM (support vector machine), která se sna¾í nalézt takovou nadrovinu v prostoru pøíznakù, která bude rozdìlovat trénovací data.
Ideální nadrovina rozdìluje data z trénovací mno¾iny tak, ¾e body v prostoru le¾í v opaèných poloprostorech a vzdálenosti v¹ech bodù od roviny jsou co nejvìt¹í.

\section{Klasifikace a clustering}\label{CLASS-APPROACHES-CLUSTERING}
Je¹tì relativnì nedávno byla klasifikace chápána jako podmno¾ina úlohy nazývané clustering.
Je sice pravda, ¾e klasifikace a clustering k sobì mají velmi blízko, a také mnoho technik pou¾ívaných v klasifikaci je mo¾né pou¾ít i v clusteringu, nicménì rozdíl mezi tìmito dvìma úlohami je v tom, ¾e pøi klasifikaci známe pøedem tøídy, do kterých budeme vstup klasifikovat.
U clusteringu tomu tak není.
Clustering dìlí vstupní text podle významných spoleèných pøíznakù ve vstupních datech.
Z definice klasifikace (viz \ref{CLASS-DEF}) víme, ¾e klasifikaèní úloha je definována klasifikaèní funkcí $f$,  která vstupnímu vzorku $x$ podle jeho pøíznakù pøiøadí oznaèení tøídy $y$, do ní¾ vzorek spadá.
Klasifikaèní úlohy v NLP se sna¾í tuto funkci $f$ co nejpøesnìjí aproximovat, aby simulovaly její výsledek.
Naroti tomu clustering nemá podobnou funkci jako klasifikace, která by urèovala, jak má vypadat výsledek.
Výsledná struktura tøíd clusteringu je vytvoøena za bìhu metody na základì znakù podobnosti vzorkù z mno¾iny vstupù.

\subsection{Strojové uèení}
Jeliko¾ pro vytvoøení funkèního klasifikátoru je nutné, aby klasifikár byl nauèen pomocí správné testovací mno¾iny, byla problematika strojového uèení s tématem klasifikace velmi úzce spojena.
Strojové uèení je jedním z podtémat oboru umìlé inteligence, zabývající se vytvoøením algoritmù, umo¾òujících poèítaèovým programùm se uèit.
Algoritmy strojového uèení se dìlí do následujících tøí kategorií:
\begin{itemize}
 \item \textit{Uèení s uèitelem (supervised learning)} -- tento zpùsob uèení je nejjednodu¹¹í. Problematiènost tohoto pøístupu spoèívá v tom, ¾e ve¹kerá data, která se program nauèí, musí být ruènì anotována èlovìkem, z èeho¾ vyplývá jeho velká èasová nároènost.
 \item \textit{Uèení bez uèitele (unsupervised learning)} -- pøi uèení bez uèitele je uèenému programu pøedána sada trénovacích dat, ve které si uèící se program sám hledá význaèné vlastnosti, na jejich¾ základì poté vytvoøí vlastní klasifikaèní funkci. Jedním z mo¾ných pøístupù k øe¹ení této metody je metoda clustering (viz \ref{CLASS-APPROACHES-CLUSTERING}).
 \item \textit{Pøístup na pomezí mezi uèení s uèitelem a uèení bez uèitele (semi-supervised learning)} -- poslední metodou strojového uèení, kterou zde zmíníme, je metoda zalo¾ená na uèení pomocí vstupních trénovacích dat, kterých následnì metoda vyu¾ije k automatickému vytvoøení dal¹ích trénovacích dat.
\end{itemize}

\subsection{Boosting}
V roce 1988 vyslovil Michael Kearns ve své práci s názvem \textit{Thoughts on hypothesis boosting} otázku, zda lze z mno¾iny slabých klasifikátorù (takových, které mají nízké hodnoty korelace testovacích a výstupních dat) vytvoøit jeden klasifikátor, který by mìl výraznì lep¹í výsledky a ve své práci dokázal, ¾e tomu tak opravdu mù¾e být.
Od té doby bylo vyvynuto více algoritmù pro boosting klasifikace, nicménì vìt¹ina z nich se zakládá na iterativním uèení mno¾iny slabých klasifikátorù a jejich následném sdru¾ení do jednoho pøesného klasifikátoru.
Historicky nejvýznam¹j¹ím boostingovým algoritmem je zøejmì algoritmus \textit{AdaBoost}(adaptive boosting), vytvoøený Yoavem Freundem a Robertem Schapirem, který publikovali v práci \textit{A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting}\cite{boosting}.

\chapter{Bayesovský klasifikátor} \label{BAYES}
Bayesovský klasifikátor (naivní Bayesovský klasifikátor) je klasifikátor postavený na zjednodu¹eném Bayesovském teorému.
Zjednodu¹ení plyne z toho, ¾e výskyt nebo naopak neexistence jednoho tokenu (slova) není závislá na existenci nebo neexistenci jiného tokenu(slova).
Tudí¾ i kdy¾ tokeny na sobì závisejí, Bayesovský klasifikátor s nimi pracuje jako se zcela nezávislými událostmi.
Tato vlastnost naivního Bayesovského filtrování je nevýhodou pro klasifikování pøirozenáho jazyka pøedev¹ím proto, ¾e slova se v jazyce vyskytují ve slovních spojeních, je¾ stejnì jako samotná slova napomáhají klasifikaci.
Aèkoliv tato vlastnost Bayesovský klasifikátor omezuje, jeho vyu¾ití na reálných textech se osvìdèilo a je hojnì pou¾íván.
V posledních letech ale ji¾ existují jiné a lep¹í metody pro klasifikaci, napøíklad metoda \textit{random forest}, kterou bych se v pozdìj¹ích verzích této práce rád podrobnìji zabýval.

\section{Matematický model}
Bayesovský klasifikátor vyu¾ívá Bayesova teorému, který zní následovnì:
\begin{theorem}
  Mìjme dva náhodné jevy $A$ a $B$ s pravdìpodobnostmi $P(A)$ a $P(B)$, pøièem¾ $P(B) > 0$. Potom platí:
  \begin{equation}
    P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} \label{SF-BAYES-MAT-bayes_theorem}
  \end{equation}
\end{theorem}
\begin{proof}
  Dle podmínìných pravdìpodobností platí, ¾e pravdìpodobnost dvou událostí $A$ a $B$, $P(A \bigcap B)$ se rovná pravdìpodobnosti $A$ krát pravdìpodobnost $B$ za pøedpokladu, ¾e nastalo $A$, $P(B|A)$.
  \begin{equation}
    P(P(A \cap B)) = P(A) \cdot P(B|A)
  \end{equation}  
  Dále také platí, ¾e pravdìpodobnost $A$ a $B$ se rovná pravdìpodobnosti $B$ krát pravdìpodobnost $A$ za pøedpokladu ¾e nastalo $B$:
  \begin{equation}
    P(P(A \cap B)) = P(B) \cdot P(A|B)
  \end{equation}
  Z tìchto dvou vztahù vychází:
  \begin{equation}
    P(B) \cdot P(A|B) = P(A) \cdot P(B|A)
  \end{equation}
  Upravením této rovnice poté dostáváme:
  \begin{equation}
    P(A|B) = \frac{P(A) \cdot P(B|A)}{P(B)}
  \end{equation}
  Co¾ je Bayesùv teorém.
  \hfill \textbf{Q.E.D}.
\end{proof}


Pro klasifikaci textu není vhodné pou¾ít pøímo rovnici z Bayesova teorému, jeliko¾ by bylo tøeba zapamatovat si pro ka¾dý token tøi hodnoty a bylo by nutné provádìt velké mno¾ství výpoètù, nicménì je mo¾né rovnici upravit do tvaru, v nìm¾ si pro ka¾dý token (slovo) vìty je tøeba pamatovat pouze jednu hodnotu pravdìpodobnosti a také není tøeba provádìt tolik výpoètù.
Upravená rovnice má nasledující tvar:

\begin{equation}
P = \frac{p_1 p_2 p_3 \ldots p_N}{p_1 p_2 p_3 \ldots p_N + (1 - p_1) (1 - p_2) (1 - p_3) \ldots (1 - p_N)} \label{equation_used} \\
\end{equation}
kde $P$ je pravdìpodobnost urèující míru nále¾itostí klasifikované zprávy do tøídy spam a $p_i, i=1 \ldots N$ udává tuto míru pro jednotlivé tokeny $i$
Výsledné $P$ je potom porovnáno s urèitým prahem, který definuje, zda je oklasifikovaná zpráva spam nebo ham.


\subsection{Odvození rovnice pro klasifikaci} \label{SF-BAYES-MAT-bayes_theorem-deriv}
Nyní pøejdìme k odvození rovnice \ref{equation_used} z Bayesova teorému \ref{SF-BAYES-MAT-bayes_theorem}.
 
Mìjme $X$ a $Y$, které znamená, ¾e tokeny $x$ a $y$ jsou pøítomny a $S$ znamenající \uv{je to spam} a $\neg S$ znamenající \uv{je to ham} (není to spam).
Pro zjednodu¹ení budeme pou¾ívat pøípad se dvìma slovy.
$$
 P(S | X \cap Y) = \frac{P(X \cap Y | S) \cdot P(S)}{P(X \cap Y)}
$$
Nyní vyu¾ijeme toho, ¾e platí $$P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)$$. Tudí¾:
\begin{equation}
 P(S | X \cap Y) = \frac{P(X \cap Y | S) \cdot P(S)}{(P(S)*P(X \cap Y | S) + P(\neg S) \cdot P(X \cap Y | \neg S))} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq1}
\end{equation}
Nyní aplikujeme onu naivitu, kterou jsme zmiòovali vý¹e \ref{SF-BAYES}.
To znamená, ¾e budeme pøedpokládat, ¾e pravdìpodobnost $X$ je nezávislá na pravdìpodobnosti $Y$, tak¾e mù¾eme pou¾ít vztah:
  $$P(A \cap B) = P(A) \cdot P(B)$$
Z toho vyplývá ¾e rovnici \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq1} mù¾eme upravit do tvaru
\begin{equation}
  P(S | X \cap Y) = \frac{P(X|S) \cdot P(Y|S) \cdot P(S)}{P(S) \cdot P(X|S) \cdot P(Y|S) + P(\neg S) \cdot P(X| \neg S) \cdot P(Y| \neg S)} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq2}
\end{equation}
Vyjdeme opìt z Bayesova teorému, který øíká, ¾e platí:
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$
Toho vyu¾ijeme a dosadíme do rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq2}, dostaneme tedy:
\begin{equation}
  P(S | X \cap Y) = \frac{\frac{P(S|X) \cdot P(X)}{P(S)} \cdot \frac{P(S|Y) \cdot P(Y)}{P(S)} \cdot P(S)}{P(S) \cdot \frac{P(S|X) \cdot P(X)}{P(S)} \cdot \frac{P(S|Y) \cdot P(Y)}{P(S)} + P(\neg S) \cdot \frac{P(\neg S|X) \cdot P(X)}{P(\neg S)} \cdot \frac{P(\neg S|Y) \cdot P(Y)}{P(\neg S)}} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq3}
\end{equation}
Z rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq2} mù¾eme odstranit $P(X)$ a $P(Y)$:
$$
  P(S | X \cap Y) = \frac{\frac{P(S|X)}{P(S)} \cdot \frac{P(S|Y)}{P(S)} \cdot P(S)}{P(S) \cdot \frac{P(S|X)}{P(S)} \cdot \frac{P(S|Y)}{P(S)} + P(\neg S) \cdot \frac{P(\neg S|X)}{P(\neg S)} \cdot \frac{P(\neg S|Y)}{P(\neg S)}}
$$
Po zjednodu¹ení dostaneme:
\begin{equation}
  P(S | X \cap Y) = \frac{\frac{P(S|X) \cdot P(S|Y)}{P(S)}}{\frac{P(S|X) \cdot P(S|Y)}{P(S)} + \frac{P(\neg S|X) \cdot P(\neg S|Y)}{P(\neg S)}} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq4}
\end{equation}
Nyní mù¾eme pøistoupit k závìreèné úpravì, která ov¹em pøedpokládá, ¾e zprávy v uèící mno¾inì jsou rovnomìrnì rozlo¾eny, tzn. ¾e zhruba 50\% nauèených zpráv jsou spam a zhruba 50\% je ham.
Potom  platí, ¾e $P(S) \approx P(\neg S)$ a tak dostaneme z rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq4} rovnici následující
\begin{equation}
  P(S | X \cap Y) = \frac{P(S|X) \cdot P(S|Y)}{P(S|X) \cdot P(S|Y) + P(\neg S|X) \cdot P(\neg S|Y)} 
\end{equation}
kde $P(\neg A) = 1 - P(A)$ tudí¾ $P(\neg A|B)$ mù¾eme pøepsat na $1-P(A|B)$.
Z toho získáme rovnici \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq5} ekvivalentní s rovnicí \ref{equation_used}.
\begin{equation}
  P(S | X \cap Y) = \frac{P(S|X) \cdot P(S|Y)}{P(S|X) \cdot P(S|Y) + (1 - P(S|X)) \cdot (1- P(S|Y))} \label{SF-BAYES-MAT-bayes_theorem-deriv-eq5}
\end{equation}
Jestli¾e není uèící mno¾ina vyvá¾ená (stejnì spamu a hamu), je vhodné pro klasifikaci pou¾ít rovnici \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq4}, proto¾e pøi pou¾ití rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq5} mù¾e docházet ke zkreslení klasifikace.

\section{Princip Bayesovského klasifikátoru} \label{SF-BAYES-PRINCIP}
Bayesovský klasifikátor si do databáze pøi uèení ukládá pravdìpodobnost zda jsou jednotlivé tokeny (slova a features) ze vstupních vìt spam, tzn. ukládá si hodnotu  $P(S|X)$.
Po nauèení dostateènì velkého objemu trénovacích dat potom vyu¾ívá tyto pravdìpodobnosti pro výpoèet, zda je klasifikovaný vstupní text spam, nebo ham.

Klasifikace tedy probíhá tak, ¾e vstupní text $X$ se rozdìlí na tokeny $X_i$.
Klasifikátor se podívá do databáze a zjistí pravdìpodobnosti $P(S|X_i)$ vy¾adující míru pravdìpodobnosti, ¾e jednotlivé tokeny nále¾í do tøídy spamu.
Jestli¾e klasifikátor pøi klasifikaci textu narazí na token, který dosud není v jeho databázi, pak je mu pøiøazena hodnota 0.5 (tzn. klasifikátor neumí urèit s jakou pravdìpodobností je tento token spam èi ham).
Pravdìpodobnost $P(X)$ ¾e vstupní text je spam je potom vypoèten pomocí dosazení $P(S|X_i)$ do rovnice \ref{SF-BAYES-MAT-bayes_theorem-deriv-eq5}.

\section{pøíklad klasifikace}
Uveïme si nyní jednoduchý pøíklad klasifikace textu:
mìjme vstupní vìtu \textit{'Aaaa, my stomach hurts.'}.
Tuto vìtu si klasifikátor rozdìlí podle zadaného pravidla na seznam tokenù (nejèastìji slov).
$$['Aaaa', 'my', 'stomach', 'hurts']$$
V databázi klasifikátoru máme napøíklad:
$$\{'my':0.6, 'stomach':0.2, 'hurts':0.2\}$$
V¹imnìme si, ¾e v databázi tokenù klasifikátoru se nevyskytuje slovo \texttt{'Aaaa'}.
To znamená, ¾e klasifikátor se pøi svém uèení s tímto slovem doposud nesetkal, a tak mu pøiøadí pravdìpodobnost 0.5.
Jeho databáze tokenù pro klasifikaci vstupní vìty \textit{'Aaaa, my stomach hurts.'} bude tedy vypadat následovnì:
$$\{'my':0.6, 'stomach':0.2, 'hurts':0.2, 'Aaaa':0.5\}$$.
Nyní u¾ má klasifikátor v¹e potøebné k tomu, aby vypoèítal pravdìpodobnost, zda je vstupní text spam èi nikoliv.
Bude postupovat podle rovnice \ref{equation_used}.
\begin{align*}
P &= \frac{p_1 p_2 p_3 \ldots p_N}{p_1 p_2 p_3 \ldots p_N + (1 - p_1) (1 - p_2) (1 - p_3) \ldots (1 - p_N)} \\
P &= \frac{0.2 \cdot 0.6 \cdot 0.2 \cdot 0.5}{0.2 \cdot 0.6 \cdot 0.2 \cdot 0.5 + (1 - 0.2) \cdot (1 - 0.6) \cdot (1 - 0.2) \cdot (1 - 0.5)} \\
P &= 0.0857
\end{align*}
Dle tohoto výpoètu se tedy pravdìpodobnost, ¾e vìta \textit{'Aaaa, my stomach hurts.'} je spam rovná 0.0857.
Co¾ znamená, ¾e vìta je rozhodnì ham.

\chapter{SVM klasifikátor} \label{SVM}
Klasifikaèní metoda pomocí SVM (support vector machines) je spoleènì s vý¹e popsaným Bayesovským klasifikátorem klasifikaèní metodou, kterou se zabývá tato práce.
Tato metoda uèení s uèitelem je v souèasné dobì nejvíce pou¾ívanou metodou v NLP.
Mimo NLP má v¹ak také velkou ¹kálu vyu¾ití, a» u¾ pro klasifikaèní úèely, nebo tøeba napøíklad pro úèely regresní analýzy (pro pøedvídání chování finanèních trhù, atd.).

V této práci nebude mo¾né popsat problematiku support vector machines do úplných detailù, nicménì zde budou popsány základy, které jsou potøebné pro pochopení problematiky a pro její pozdìj¹í implementaci.
Pro podrobnìj¹í popis svm doporuèuji knihu Vladimira N. Vapnika a z roku 1998 s názvem \emph{Statistical Learning Theory} \cite{Vapnik1998} a èlánek \emph{Support-Vector Networks} \cite{Vapnik1995}.

\section{Obecné}
SVM je klasifikátor dosahující velmi dobrých výsledkù ve velkém mno¾ství odvìtví a aplikací.
Poprvé byl tento algoritmus pøedstaven v roce 1995 ve èlánku Vladimira N. Vapnika a kolektivu s názvem \emph{Support-Vector Networks} \cite{Vapnik1995}.
SVM bylo navrhnuto jako klasifikaèní algoritmus.
Aby bylo SVM schopné klasifikovat, musíme jej nejprve nauèit pomocí urèité sady dat, která je rozdìlena na trénovací a testovací data.
Ka¾dý jeden záznam v trénovacích a testovacích datech má definovanou jistou hodnotu (label), která urèuje, do které tøídy daný záznam spadá.
Ka¾dý záznam se skládá z nìkolika pøíznakù (features), pomocí kterých se SVM uèí a následnì klasifikuje.
Napøíklad klasifikujeme-li pomocí SVM text tyto pøíznaky budou mimo jiné jednotlivá slova, délka textu, emailové adresy, Twitter tagy atd.

Nyní pøejdìme k matematickému popisu problematiky SVM.
Pøedpokládejme urèitý klasifikaèní problém a jistou datovou sadu ve které je mno¾ství záznamù, které patøí buï do pozitivní, nebo negativní tøídy. 
Trénovací sada $X$ pro tento problém obsahuje $l$ záznamù.
Jeden záznam v této sadì je tedy definován jako vektor $x_i \in \mathbb{R}^n$ kde $1 \leq i \leq l$.
Ka¾dý tento vektor $x_i$ se skládá z mno¾iny pøíznakù $[x_1, x_2,\ldots,x_n] \in x_i$, kde $x_1,\ldots,x_n$ jsou jednotlivé atributy daného záznamu.
Label $y$ ka¾dého záznamu z dat je definován jako $y \in \{1,-1\}$ v závislosti na tom, do které tøídy daný záznam spadá.
Trénovací sadu dat lze tedy zapsat jako:

\begin{equation}
  X = \{(x_i,y_i)\}_{i=1}^{l}
\end{equation}
\begin{equation}
  x \in \mathbb{R}^{n}, x_i = [x_1, x_2, \ldots, x_n], i \in 1, \ldots |X|
\end{equation}
\begin{equation}
  y \in \{1,-1\}
\end{equation}

Cílem SVM klasifikátoru je najít rozhodovací hranici, která rozdìluje pøíznaky v trénovacích datech tak, aby rozdìlení odpovídalo jednotlivým tøídám a souèasnì se sna¾í o maximalizaci vzdálenosti v¹ech bodù od rozhodovací hranice. 
Rozhodovací hranice je definována jako \emph{hyperplocha (nadrovina)}, co¾ je prostor s $n$ dimenzemi, který dìlí prostor s $n+1$ dimenzemi do dvou podprostorù.
Libovolná hyperplocha lze definovat jako \emph{diskriminaèní funkci (discriminant function)}\footnote{diskriminaèní funkce je funkce, která optimálnì dìlí prostor pøíznakù. \cite{Seong-Wook}} v následující formì:

\begin{equation}
  f(x) = w^T x + b
\end{equation}


$b$ je takzvaný \emph{bias} a $w^T x$  definuje skalární souèin mezi váhovým vektorem $w$ a vektorem pøíznakù $x$.
Tento skalární souèin je definován jako:

\begin{equation}
  w^T x = \sum_{j=1}^{n}w_j x_j
\end{equation}

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=10cm,keepaspectratio]{fig/hyperplanes}
      \caption{Nìkolik hyperploch v prostoru $R^2$ s vektorem $ w^T x$} 
      \label{SVM-THEORY-HYPERPLANES}
    \end{center}
\end{figure}


\section{Nelineární SVM a jádra}
Jeliko¾ je u základního klasifikátoru SVM hyperplocha dìlící prostor pøíznakù lineární, pova¾ujeme tento klasifikátor za lineární.
Nicménì pøi pou¾ití nelineární dìlící hyperplochy mù¾e klasifikátor dosahovat je¹tì daleko lep¹ích výsledkù.
Problémem takto nelineárních klasifikátorù ale je zvy¹ování komplexicity výpoètù pøi uèení vìt¹ích poètù pøíznakù (vysoká dimenzionalita dat).
Pro vyøe¹ení vý¹e zmínìného problému byly klasické lineární SVM klasifikátory roz¹íøeny o podporu takzvaných \emph{jaderných funkcí (kernel functions)} za pomocí nich¾ jsou lineární klasifikátory schopny vytvoøit nelineární dìlící hyperplochy.
Pro vysvìtlení zpùsobu, jakým se této nelinearity dosáhne pøedpokládejme klasický lineární klasifikátor.

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=13cm,keepaspectratio]{fig/XtoFTransformation}
      \caption{Transformace vstupního prostoru $X$ do prostoru pøíznakù $F$ pomocí mapovací funkce $\varphi(x)$. $f(x)$ je znázornìní diskriminaèní funkce} 
      \label{SVM-KERNEL-TRANSFORMACE}
    \end{center}
\end{figure}

Nyní pøevedeme trénovací sadu $X$, také známou jako \emph{vstupní prostor (input space)} do vysoce dimenzionálního prostoru pøíznakù $F$ za pomocí nelineární mapovací funkce $\varphi : X \rightarrow F$ viz. obrázek \ref{SVM-KERNEL-TRANSFORMACE}.
Pro prostor pøíznakù tedy budeme mít následující diskriminaèní funkci:

\begin{equation}
  f(x) = w^T \varphi(x) + b
\end{equation}

Jeliko¾ je pøi výpoètu diskriminaèní funkce $f(x)$ tøeba vypoèíst mapovací funkci $\varphi(x)$ 
 tak velmi výraznì roste nároènost jejího výpoètu s poètem dimenzí vstupù.
Mìjme kupøíkladu následující mapovací funkci:

\begin{equation}
  \varphi(x) = (x^2_1, \sqrt{2}x_1 x_2, x^2_2)^T
\end{equation}

Z této mapovací funkce jsme nyní schopni spoèítat diskriminaèní funkci v prostoru pøíznakù:

\begin{equation}
  f(x) = w_1 x^2_1 + \sqrt{2}w_2 x_1 x_2 + w_1 x^2_2 + b
\end{equation}

Je zcela zøejmé, ¾e tento zpùsob výpoètu je neúnosný.
Museli bychom toti¾ pro výpoèet transformovat celý vstupní prostor do prostoru pøíznakù, co¾ by kvadraticky zvý¹ilo èasovou a pamì»ovou nároènost klasifikátoru.
Tento zpùsob tedy není vhodný.
Existuje v¹ak øe¹ení, kterým lze spoèítat diskriminaèní funkci $f(x)$ ani¾ bychom museli znát, chápat a poèítat mapovací funkci do prostoru $F$.
Pro toto øe¹ení je tøeba si vyjádøit váhový vektor $w$ jako lineární kombinaci jednotlivých záznamù z tréninkové sady.

\begin{equation}
  w = \sum^{n}_{i=1}\alpha_i x_i
\end{equation}

Z tohoto vychází, ¾e diskriminaèní funkce vstupního prostoru $X$ je:

\begin{equation}
  f(x) = \sum^{n}_{i=1}\alpha_i x^T_i \cdot x + b
\end{equation}

a diskriminaèní funkce prostoru pøíznakù $F$ je:

\begin{equation}
  f(x) = \sum^{n}_{i=1}\alpha_i \varphi(x_i)^T \varphi(x) + b
\end{equation}

Tyto dvì reprezentace diskriminaèní funkce pro vstupní prostor $X$ a pro prostor pøíznakù vzhledem k promìnné $\alpha_i$ bývá nazývána takzvanou \emph{duální reprezentací (dual representation)}. 

Kartézský souèin $\varphi(x_i)^T \varphi(x)$, kde $x_i, x \in X$ tedy pøedstavuje takzvanou \emph{jadernou funkci (kernel function)}.
Jaderná funkce je tedy definována jako:

\begin{equation}
  k(x,z) = \varphi(x_i)^T \varphi(x)
\end{equation}

Jeliko¾ jadernou funkci lze vypoèítat pouze pomocí pøíznakù ve vstupním prostoru, je mo¾né vypoèítat diskriminaèní funkci ani¾ bychom znali potøebnou mapovací funkci.
Díky této vlastnosti jaderných funkcí nemusíme transformovat celý vstupní prostor do nového prostoru pøíznakù $F$, ale spoèítáme pouze jadernou funkci pro jednotlivé pøíznaky.
Tudí¾ nám poèet dimenzí prostoru $F$ neovlivní komplexnost výpoètu.
Vý¹e zmínìná operace, kdy vyu¾ijeme pouze kartézský souèin bodù v daném prostoru bývá èasto v literatuøe nazývána jako takzvaný \emph{Kernel trick}.
Pro výpoèet diskriminaèní funkce $f(x) = w^T \varphi(x) + b$ tedy pou¾ijeme jadernou funkci $k(x,z) = \varphi(x)^T \varphi(z)$ a dostaneme $f(x) = k(x,z) + b$.

Nyní si nejprve si znázorníme na pøíkladu nároènost pøi pøevádìní v¹ech souøadnic do prostoru $F$.
Pøedpokládejme prostor $X \in \mathbb{R}^2$ (tzn. $x = (x_1, x_2)$) dále pak polynomiální mapovací funkci druhého øádu

\begin{equation}
\varphi(x) = (1, x_1, x_2, x^2_1, x^2_2, x_1 x_2)
\end{equation}

která pøevede vektor pøíznakù ze vstupního prostoru $X$ do prostoru $F$.
Abychom nyní pomocí této mapovací funkce vypoèítali jadernou funkci, musíme oba body pøevést do prostoru $F$ a provést skalární souèin v prostoru $F$.
Tudí¾:

\begin{equation}
  K(x, z) = \varphi(x)^T \varphi(z)
\end{equation}
\begin{equation}
  K(x, z) = 1 + x_1 z_1 + x_2 z_2 + x^2_1 z^2_1 + x^2_2 z^2_2 + x_1 z_1 x_2 z_2
\end{equation}

Nyní si pøedstavme, ¾e nemáme dvou dimenzionální prostor, ale velmi vysoce dimenzionální prostor.
Pøevádìní $x$ a $z$ do prostoru $F$ by bylo velmi výpoèetnì nároèné.

Nyní si na pøíkladu pøedvedeme pou¾ití vý¹e zmínìného kernel tricku, který nám umo¾ní vypoèíst jadernou funkci, ani¾ bychom potøebovali spoèítat transformace do prostoru pøíznakù $F$.
Pøedpokládejme jadernou funkci:

\begin{equation}
  K(x, z) = (1 + x^T z)^2
  \label{SVM-KERNEL-2POLYNOMIAL}
\end{equation}

kterou roznásobíme a dostaneme:

\begin{equation}
  K(x, z) = 1 + x^2_1 z^2_1 + x^2_2 z^2_2 + 2 x_1 z_1 + 2 x_2 z_2 + 2 x_1 z_1 x_2 z_2
\end{equation}

Co¾ je skuteènì skalární souèin definovaný v prostoru pøíznakù s mapovací funkcí 

\begin{equation}
\varphi(x) = (1, x^2_1, x^2_2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)
\end{equation}

Tímto jsme dokázali, ¾e opravdu není tøeba pøevádìt celý prostor pøíznakù, ale staèí nám vypoèítat jednoduchou funkci $K(x, z) = (1 + x^T z)^2$ a tím dosáhnout daleko jednodu¹eji ký¾eného výsledku.

\subsection{Jádra}
\label{SVM-THEORY-KERNELS}
Jádra, nebo také jaderné funkce, jsou tedy funkce, pomocí nich¾ je klasifikátor SVM  schopen vypoèítat diskriminaèní rovnici pro optimální rozdìlení prostoru pøíznakù.
V závislosti na tom jaké jádro je pøi trénování klasifikátoru pou¾ito je klasifikátor lineární nebo nelineární a dosahuje rozdílných výsledkù na rùzných datech.

V praxi se pou¾ívá nìkolik rùzných jader, nejvíce RBF (radial basis function), polynomiální jaderná funkce a nìkdy se pou¾ívá i základní lineární jaderná funkce.

\subsubsection{Lineární jaderná funkce}
Lineární jaderná funkce je nejzákladnìj¹í ze v¹ech jaderných funkcí.
Poèítá toti¾ skalární souèin z bodù ve vstupním prostoru, tudí¾ je schopna pouze lineární klasifikace.
Lineární jaderná funkce je definována následovnì:

\begin{equation}
  K(x, z) =  x^T z
\end{equation}

\subsubsection{Polynomiální jaderná funkce}
Rovnice \ref{SVM-KERNEL-2POLYNOMIAL} popisuje tedy polynomiální jadernou funkci druhého øádu, nicménì praxi se èasto pou¾ívají polynomiální jaderné funkce vy¹¹ích øádù $Q$, obecnì definované jako:

\begin{equation}
  K(x, z) = (x^T z + c)^Q
\end{equation}
kde $c \geq 0$ urèuje vliv vy¹¹ích a ni¾¹ích øádù polynomu.

Polynomiální jaderné funkce jsou v NLP pomìrnì hodnì oblíbené \cite{ACLShort}.
Pøi pou¾ití jaderné funkce vy¹¹ích øádù, ale mù¾e nastat takzvaný overfitting, kdy se SVM nauèí na velmi specifické pøíznaky, které nejsou pro korektní klasifikaci smìrodatné, co¾ zpùsobí markantní zhor¹ení schopností SVM klasifikátoru.
Z tohoto dùvodu je nejèastìji pou¾ívána polynomiální jaderná funkce 2. a 3. øádu, která není schopna tak dokonale kopírovat tvar kolem pøíznakù.

\subsubsection{Radiální jaderná funkce}
Radiální jaderná funkce, èasto oznaèována jako RBF jaderná funkce, nebo RBF kernel je dal¹í velmi oblíbenou jadernou funkcí zalo¾enou na radiální bázové funkci (radial basis function).
Je definována jako:
 
\begin{equation}
  K(x, z) = exp(-\frac{||x-z||^2}{2\gamma^2})
\end{equation}

Obèas se také pou¾ívá jiná forma definice pro RBF jaderné funkce, nicménì základní my¹lenka, ¾e èím vzdálenìj¹í bod v prostoru pøíznakù, tím men¹í má vliv na rozhodování, zùstává ve v¹ech tìchto definicích zachována.
Promìnná $\gamma$, pro kterou musí platit $\gamma \geq 0$, definuje jak moc bude klasifikátor brát v úvahu vzdálenìj¹í body.
Nevhodným výbìrem této promìnné mù¾e opìt, podobnì jako v pøípade polynomiální jaderné funkce k overfittingu, proto je dùle¾ité zvolit její vhodnou hodnotu.

\subsubsection{Dal¹í jádra}
Tyto tøi vý¹e zmínìná jádra/jaderné funkce samozøejmì nejsou jediná.
Kdykoliv je mo¾né vytvoøit novou jadernou funkci, která bude popisovat nìjaký jiný prostor.
Taková jaderná funkce, kterou vytvoøíme v¹ak nemusí popisovat ¾ádný prostor.
Existují tøi pøístupy pomocí kterých se mù¾eme ujistit, ¾e námi vytvoøená jaderná funkce skuteènì nìjaký prostor popisuje:
\begin{itemize}
\item \emph{Konstrukce} -- jádro zkonstruujeme z transformaèní funkce $\varphi(x)$.
\item \emph{Matematické podmínky jádra (Mercerovy podmínky)} -- Pokud jaderná funkce $K(x, z)$ splòuje následující dvì podmínky, pak existuje prostor, který je danou jadernou funkcí popsán:
 \begin{itemize}
  \item $K(x, x^{'})$ musí být symetrická funkce (tzn. $K(x, x^{'}) = K(x^{'}, x)$,$\forall x, x^{'} \in X$)
  \item Pro matici:
  	$\begin{bmatrix}
       K(x_1, x_1) & K(x_1, x_2) & \cdots  & K(x_1, x_N)  \\[0.3em]
       K(x_2, x_1) & K(x_2, x_2) & \cdots  & K(x_2, x_N)  \\[0.3em]
       \cdots      & \cdots      & \cdots  & \cdots       \\[0.3em]
       K(x_N, x_1) & K(x_N, x_2) & \cdots  & K(x_N, x_N)  \\[0.3em]
    \end{bmatrix}$ musí platit, ¾e je pozitivnì semi-definitní pro libovolné $x_1,\dots, x_N \in X$
 \end{itemize} 
\item \emph{Do we even care?} -- tento pøístup je velmi zvlá¹tní, nicménì obèas také pou¾ívaný.
	Vytvoøí se prostì libovolná jaderná funkce, aplikuje se v SVM a pokud funguje, tak nás nezajímá, ¾e dané jádro neexistuje.
	Tento pøístup v¹ak rozhodnì není doporuèitelným.
\end{itemize}

\section{Vytvoøení rozhodovací hranice}
Jak ji¾ bylo vý¹e zmínìno, SVM klasifikátor se pøi uèení sna¾í vytvoøit optimální rozhodovací hranici (hyperplocha) tak, aby dìlila prostor pøíznakù na dva podprostory, kde ka¾dý obsahuje objekty v¾dy pouze z jedné tøídy a v¾dy se sna¾í maximalizovat vzdálenost rozhodovací hranice ode v¹ech bodù v prostoru.


\begin{figure}[H]
    \begin{center}
      \includegraphics[width=10cm,keepaspectratio]{fig/margins}
      \caption{Ukázka mo¾ných marginù, vlevo maximální, ¹iroký; vpravo tenký} 
      \label{SVM-THEORY-MARGINS}
    \end{center}
\end{figure}

Pøedpokládejme diskriminaèní funkci $f(x)$, která definuje urèitou hyperplochu, která lineárnì rozdìluje prostor $X$ na pozitivní a negativní tøídy.
Nejbli¾¹í vektory $x \in X$ k této hyperplo¹e jsou oznaèeny $x_+$ pro pozitivní a $x_-$ pro negativní tøídy.
Definujeme takzvaný \emph{geometrický margin} hyperplochy $f$ na trénovacích datech $X$, co¾ je \emph{maximální ¹íøka (margin)} volného prostoru mezi rozdìlovací hyperplochou $f$ a nejbli¾¹ími body v prostoru $X$ (viz obrázek \ref{SVM-THEORY-MARGINS}).
Je definován jako:

\begin{equation}
  m_x(f) = \frac{1}{2} \widehat{w}^T (x_+ - x_-)
\end{equation}

kde vektor $\widehat{w}$ je jednotkový vektor vektoru $w$ a body $x_+$ a $x_-$ jsou od hyperplochy stejnì daleko, co¾ znamená, ¾e existuje nìjaká konstanta $\alpha > 0$, pro kterou platí:

\begin{equation}
  f(x_+) = w^T  x_+ = \alpha
\end{equation}
\begin{equation}
  f(x_-) = w^T  x_- = -\alpha
\end{equation}

Nyní nastavíme hodnotu promìnné $\alpha$ na $\alpha = 1$ a po úpravách získáme následující definici maximálního marginu:

\begin{equation}
  m_X(f) = \frac{1}{||w||}
\end{equation}

Abychom tedy dostali optimální rozdìlovací hranici s maximálním marginem, budeme muset maximalizovat hodnotu maximálního marginu.
Tato operace je ov¹em ekvivalentní s minimalizací $\frac{1}{2}||w||^2$.
Nalezení specifické diskriminaèní funkce s maximálním geometrickým marginem je tedy ekvivalentní s následujícím optimalizaèním problémem:

\begin{equation}
\begin{aligned}
& \underset{w,b}{\text{minimalizace}}
& & \frac{1}{2}||w||^2 \\
& \text{kde}
& & y_n(w^T x_i + b) \geq 1, & n = 1,\ldots,l
\end{aligned}
\end{equation}

Tato optimalizace pøedpokládá, ¾e trénovací mno¾ina $X$ je lineárnì separovatelná.
Podmínka $y_n(w^T x_n + b) \geq 1$ potom zaji¹»uje, ¾e diskriminaèní funkce oklasifikuje v¹echna data v trénovací mno¾inì korektnì.

Nicménì mù¾e nastat problém, ¾e trénovací mno¾ina $X$ nemusí být lineárnì separovatelná.
Aby klasifikátor byl schopný se nauèit i na takové trénovací mno¾inì, musíme jej upravit tak, aby byl mohl nekorektnì oklasifikovat nìjaké záznamy z trénovací mno¾iny a tím, i za cenu chyb, diskriminaèní funkci najít.
Toto opatøení mù¾e nìkdy pomoci nalézt ¹ir¹í margin a tím zlep¹it výsledky klasifikátoru oproti pøedchozímu, u¾¹ímu, marginu.
Tuto úpravu udìláme za pomocí takzvané \emph{chybové promìnné (slack variable)} $\xi_i$, kterou odeèteme od pravé strany optimalizaèní podmínky -- tzn. umo¾níme chybu.
Takto upravený problém nyní vypadá následovnì:

\begin{equation}
\begin{aligned}
& \underset{w,b}{\text{minimalizace}}
& & \frac{1}{2}||w||^2 \\
& \text{kde}
& & y_n(w^T x_i + b) \geq 1 - \xi_i, & n = 1,\ldots,l
\end{aligned}
\end{equation}

Pokud tedy platí, ¾e $y_i(w^T x_i + b) < 1$ respektive $\xi_i > 1$, pak je záznam $x_i \in X$ ¹patnì klasifikována.
Pokud ale platí, ¾e $0 \leq \xi_i \leq 1$, pak $x_i \in X$ le¾í uvnitø marginu.
$x_i$ je tedy klasifikováno korektnì, jestli¾e platí, ¾e $\xi_i \leq 0$.
Z tohoto vyplývá, ¾e suma v¹ech chybových promìnných reprezentuje míru chybovosti:

\begin{equation}
  \xi(X) = \sum^n_{i=1}\xi_i
\end{equation}
\begin{equation}
  X = \{(x_i,y_i)\}^l_{i=1}
\end{equation}

Abychom byli schopni  minimalizovat míru chybovosti vzhledem k maximalizaci marginu (penalizace, za ¹patnou klasifikaci a marginové chyby) zavedeme konstantu $C > 0$, kterou budeme pøenásobovat míru chybovosti.
Tato konstanta se nazývá \emph{konstanta soft-margin (soft-margin constant)}.
Na¹e optimalizaèní úloha tedy s touto soft-margin konstantou vypadá následovnì:

\begin{equation}
\begin{aligned}
& \underset{w,b}{\text{minimalizace}}
& & \frac{1}{2}||w||^2 + C \sum^{n}_{i=1}\xi_i \\
& \text{kde}
& & y_n(w^T x_i + b) \geq 1 - \xi_i, & n = 1,\ldots,l
\end{aligned}
\end{equation}

Nyní pro vyøe¹ení tohoto optimalizaèního problému pou¾ijeme metodu Lagrangeových multiplikátorù a získáme optimalizaèní problém:

\begin{equation}
\begin{aligned}
& \underset{\alpha}{\text{maximalizace}}
& & \frac{1}{2}\sum^{n}_{i=1}\sum^{n}_{j=1}y_iy_j\alpha_i\alpha_jx^T_i x_j \\
& \text{kde}
& & \sum^n_{i=0} y_i\alpha_i = 0, & 0 \leq \alpha_i \leq C
\end{aligned}
\end{equation}


Skalární souèin $x^T_i x_j$ v maximalizaèní rovnici mù¾eme nahradit libovolnou jadernou funkcí a dosáhnout tím nelineární transfomrace a dosáhnout tak ¹irokého marginu ve vysoce dimenzionálních prostorech pøíznakù. (viz \ref{SVM-THEORY-KERNELS}).

Díky vyu¾ití Lagrangeovy teorie pro øe¹ení, má výsledné øe¹ení nalezené po vyøe¹ení optimalizaèního problému zajímavé vlastnosti.
Bylo napøíklad dokázáno, ¾e získané øe¹ení je v¾dy globální díky tomu, ¾e formulace problému je konvexní \cite{Burges1988}.

Zajímavou vlastností support vector machines je, ¾e ne v¹echny vektory z tréninkové mno¾iny se podílejí na výsledném øe¹ení.
Pou¾ijme rovnici pro výpoèet váhového vektoru $w$ získaného derivací pøi øe¹ení Lagran¾ánu, a to:

\begin{equation}
  w = \sum^{n}_{i=1}y_i \alpha_i x_i
\end{equation}

V¹echny vektory $x_i$ pro které tedy platí, ¾e  $\alpha_i > 0$ jsou vektory které le¾í na pomezí marginu, uvnitø marginu, nebo jsou ¹patnì klasifikovány.
Tyto vektory jsou nazývány \emph{support vektory (support vectors)}.
Vektory, které v¹ak mají hodnotu $\alpha_i \leq 0$ nejsou vùbec do øe¹ení zahrnuty. 
Tudí¾ tyto vektory by mohly být zcela odstranìny z tréninkové sady a nemìlo by to ¾ádný vliv na výsledné øe¹ení.
Díky této vlastnosti jsou SVM ménì náchylné na overfitting a také klasifikaèní model, který tyto vektory tvoøí je díky tomu velmi malý a rychlý.


\chapter{Tokenizace a výbìr vhodných pøíznakù} \label{TOKENIZACE}
V této kapitole se budeme zabývat tvorbou tokenù, které klasifikátory pou¾ívají pro trénovaní a klasifikaci textu.

Nejprve si ale definujme, co to tokenizace je:

\begin{definition}
Tokenizace je proces rozdìlování vstupního textu do vhodných tokenù (nejèastìji slov, frází, symbolù a jiných významových elementù).
Seznam takto vytvoøených tokenù se potom pou¾ívá pro dal¹í zpracování (v pøípadì této práce, jako vstup klasifikátorù).
\end{definition}

V této práci rozdìlujeme tokeny na dva typy:
\begin{itemize}
	\item Speciální pøíznaky -- V textu se mohou nacházet skryté znaky, které vìt¹inou nejsou klasifikátory bìhem klasifikace schopny zachytit a které mohou výrazným podílem napomoct pøesnosti klasifikace.
Ka¾dý druh klasifikovaného textu ale, mù¾e mít citlivost k rùzným znakùm zcela jinou, je proto nutné dbát na to, jak vybrat optimální znaky pro daný vstupní text a pou¾itou klasifikaèní metodu.
	\item Textové tokeny -- Mimo vý¹e zmínìných speciálních pøíznakù jsou v¹ak v textu pøíznaky jednotlivých slov, znakù, atd. , která jsou pro klasifikaci nejdùle¾itìj¹ími nositeli informace.
Proto také velmi zále¾í na výbìru, popøípadì úpravì, vstupních slov a zpùsobu jejich zpracování.
\end{itemize}

Nyní pøistoupíme k detailnìj¹ímu popisu vý¹e popsaných typù tokenù tak jak je k nim pøistupováno v praktické èásti této práce.
Následnì popí¹eme práci naivního Bayesovského klasifikátoru a klasifikátoru zalo¾eného na metodì SVM s vytvoøenými tokeny.

\section{Speciální pøíznaky v textu} \label{TOKENIZACE-SPEC}
Nyní rozebereme speciální pøíznaky v textu.
Jak ji¾ bylo zmínìno tyto pøíznaky jsou v¹echny implementovány v prakticé èásti této práce.

Budeme popisovat jednotlivé druhy speciálních pøíznakù, které jsou z textu extrahovány a které jsou následnì pøedávány klasifikátorùm, je¾ s nimi dále pracují.
Tyto pøíznaky jsou mnohdy v textu skryté a klasickými pøístupy pro tokenizaci textu (viz \ref{Textove_tokeny}) nejsme schopni tyto data z textu získat.
Tyto speciální pøíznaky mohou z textu urèovat náladu pisatele, zobecòovat informace obsa¾né v textu a tím napomoct klasifikaci atd.
Vzhledem k tomu, ¾e takovýchto speciálních pøíznakù bychom byli schopni vymyslet obrovské mno¾ství, je nemo¾né je popsat a implementovat v¹echny.
V této práci bylo zvoleno nìkolik typù tìchto speciálních pøíznakù, které byly imlpementovány a budou zde tedy popsány.

V následujícím popisu pøíznaky dìlíme do skupin podle typu informace, kterou se pomocí nich sna¾íme z textu dostat.
Tyto skupiny jsou potom dìleny na typy pøíznakù ze skupiny, které popisují jednotlivé varianty implementace této varianty, které jsouv praktické èásti implementovány.

\subsection{URL}
Skupina pøíznakù s názvem URL je skupinou, hledající v klasifikovaném textu internetové odkazy ("http://adresa.com", "ftp://adresa.cz", "adresa.cz/odkaz.html", atd.) a sna¾ící se z tìchto odkazù vytvoøit tokeny co nejvhodnìj¹í pro klasifikaci.
 
Skupina URL má v praktické èásti práce následující mo¾né varianty speciálních pøíznakù:

\begin{itemize}
  \item \emph{Celé URL} -- celé URL nalezené v textu je bráno jako pøíznak.
  \item \emph{Doména URL} -- doména daného URL je brána jako pøíznak. (napøíklad: \emph{"http://healthland.time.com/2013/04/02/bird-flu-is-back-in-china-but-this-time-its-h7n9/"} $\longrightarrow$ \emph{"time.com"})
  \item \emph{Existence URL ANO/NE} -- existuje-li v textu URL, pak se vytvoøí pro daný text pøíznak definující, ¾e se v textu URL vyskytovalo.
  Pokud se v textu ale URL nenachází, vytvoøí se namísto pøíznaku \emph{ANO} pøíznak \emph{NE} definující neexistenci URL v textu.
  \item \emph{Existence URL ANO} -- tento typ pøíznaku URL je variací na pøedchozí pøíznak \emph{Existence URL ANO/NE}.
  Pøíznak se ale vytvoøí pouze, pokud se v textu URL nachází.
  Jestli¾e se nenachází, pøíznak vytvoøen není. 
  \item \emph{Nepou¾ívat URL} -- tato varianta zcela zru¹í pou¾ívání URL jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Emailové adresy}
Skupina pøíznakù Emailové adresy sdru¾uje dohromady varianty pøíznakù pracující s nalezenými emailovými adresami v textu.

Mo¾né varianty pøíznakù v této skupinì jsou:
\begin{itemize}
  \item \emph{Celý email} -- celý email nalezený v textu je brán jako pøíznak.
  \item \emph{Existence emailu ANO/NE} -- pøíznak definuje, zda se v textu nachází nìjaká emailová adresa.
  Pokud ano, je vytvoøen pøíznak \emph{ANO} a pokud ne je vytvoøen pøíznak \emph{NE}.
  \item \emph{Existence emailu ANO} -- nachází-li se v textu nìkde emailová adresa, vytvoøí se pøíznak definující, existenci této emailové adresy.
  Na rozdíl od pøedchozí varianty \emph{Existence emailu ANO/NE} se nevytváøí pøíznak NE v pøípadì nenalezení emailové adresy v textu.
  \item \emph{Nepou¾ívat emailové adresy} -- tato varianta zcela zru¹í pou¾ívání pøíznakù emailových adres jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Emotikony}
Dal¹í skupinou pøíznakù, které v této práci rozpoznáváme a pøedáváme klasifikátorùm jsou pøíznaky emotkonù (smajlíkù).

Tato skupina pøíznakù se v na¹í implementaci dìlí na následující typy:
\begin{itemize}
  \item \emph{Existence jednotlivých emotikonù} -- pro ka¾dý nalezený emotikon je vytvoøen pøíznak.
  \item \emph{Existence emotikonù obecnì ANO/NE} -- jestli¾e se v textu nachází nìjaké emotikony, je vytvoøen pøíznak \emph{ANO}, jestli¾e se nenacházejí je vytvoøen pøíznak \emph{NE}.
  \item \emph{Existence emotikonù obecnì ANO} -- obdobné jako \emph{Existence emotikonù obecnì ANO/NE} pouze s tím rozdílem, ¾e jestli¾e se v textu nenacházejí ¾ádné emotikony, ¾ádný pøíznak se nevytvoøí.
  \item \emph{Nálada emotikonù} -- v textu se vyhledávají emotkiony a zji¹»uje se jejich nálada. 
  Typy emotikounù jsou rozdìleny do tøech tøíd -- smutné(":-(", ":'(", \ldots), veselé(":-)", ":-P", \ldots ) a ostatní ("o.O", ":-O", \ldots).
  Jestli¾e je v textu nalezen nìjaký emotikon z vý¹e zmínìných tøíd, pak je vytvoøen odpovídající pøíznak \emph{SAD}, \emph{HAPPY}, nebo \emph{OTHER}.
  Nachází-li se v textu více typù emotikonù, jsou samozøejmì vytvoøeno více odpovídajících pøíznakù. (tzn. pokud se v textu nachází napøíklad následující mno¾ina emotikonù:":-)", ":-P", ":(", jsou pro daný text vytvoøeny dva pøíznaky \emph{HAPPY} a \emph{SAD})
  \item \emph{Nepou¾ívat emotikony} -- tato varianta zcela zru¹í pou¾ívání emotikonù jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Tagy}
Takzvané tagy jsou textové znaèky (klíèová slova), které autoøi pøipisují k textu, aby daný text pøiøadili k nìjakému tématu a umo¾nili tak ostatním u¾ivatelùm snáze nalézt pøíspìvky k jimi hledanému tématu.
Tyto tagy bývají ve tvaru \#TAG (napøíklad "\#influenza", "\#sick", \ldots).
Do této skupiny byly v této práci zaøazeny také tagy u¾ivatelù pou¾ívané ve formátu @JMÉNO\_U®IVATELE, které odkazjují na zmínìného u¾ivatele.

Mo¾né typy jsou v pøípadì této skupiny jsou následující:
\begin{itemize}
  \item \emph{Celý tag} -- celý tag nalezený v textu je pou¾it jako pøíznak pro klasifikaci.
  \item \emph{Existence tagù ANO/NE} -- jestli¾e se v textu nachází nìjaké tagy, je vytvoøen pøíznak \emph{ANO}, jestli¾e se nenacházejí je vytvoøen pøíznak \emph{NE}.
  \item \emph{Existence tagù ANO} -- jako v \emph{Existence tagù ANO/NE}, ale v pøípadì ¾e ¾ádný tag nebyl nalezen, tak se nevytváøí pøíznak NE.
  \item \emph{Nepou¾ívat tagy} -- tato varianta zcela zru¹í pou¾ívání tagù jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Vìty}
V pøípadì této skupiny se jedná pouze o jediný typ pøíznaku.
V celém vstupním textu se spoèítá poèet napsaných vìt a toto èíslo se pou¾ije jako pøíznak pro klasifikaci.
Samozøejmì, stejnì jako ve vý¹e popsaných skupinách pøíznakù, i tento pøíznak se dá zcela zru¹it a nepou¾ívat.

\subsection{Èasy}
V textu se velmi èasto nacházejí rùzné èasové údaje, které by mohly velkou mírou pøispìt výsledkùm na¹eho klasifikátoru.
Proto byla vytvoøena skupina pøíznakù nazvána \emph{Èasy}, která vytváøí pøíznaky z nalezených èasových informací v textu.

Obsahuje tyto typy:
\begin{itemize}
  \item \emph{Celý èas} -- jako pøíznaky pro klasifikaci se pou¾ijí v¹echny èasové infomrace nalezené v textu.
  \item \emph{Èas ve 24h formátu} -- èasové informace nalezené v textu jsou konvertovány do 24 hodinového formátu a následnì pou¾ity jako pøíznaky.
  \item \emph{Pouze hodiny ve 24h formátu} -- èasové informace jsou stejnì jako v \emph{Èas ve 24h formátu} pøevdeny do 24h formátu, ale jako pøíznaky jsou pou¾ity pouze informace o hodinách.
  \item \emph{Nepou¾ívat èasy} --  tato varianta zcela zru¹í pou¾ívání èasù jako pøíznakù pro klasifikaci.
\end{itemize}

\subsection{Data}
Podobnì jako skupina \emph{Èasy}, funguje i skupina \emph{Data}, která ale pracuje s daty, ze kterých vytváøí pøíznaky.
V následujícím textu budeme písmenem D oznaèovat dny, písmenem M mìsíce a písmenem Y roky (tzn. datum 15.2.1998 je ve formátu DMY).

Obsahuje tyto mo¾né typy pøíznakù:

\begin{itemize}
  \item \emph{Celé datum} -- jako pøíznak je v pøípadì tohoto typu pøíznakù pou¾ito datum nalezené v textu.
  \item \emph{Datum ve formátu DMY} --v pøípadì tohoto typu je datum nalezené v textu pøevedeno do formátu DMY a v tomto formátu je pou¾ito jako pøíznak.
  \item \emph{Datum ve formátu MY} -- podobnì jako v pøedchozím typu je datum pøevedeno do formátu DMY, ale jako pøíznak jsou pou¾ity pouze informace o mìsíci a roku (tzn. MY)
  \item \emph{Datum ve formátu Y} -- opìt podobné jako \emph{Datum ve formátu DMY} ale pro vytvoøení pøíznaku je pou¾it pouze rok z data nalezeného v textu.
  \item \emph{Nepou¾ívat data} -- tato varianta zcela zru¹í pou¾ívání èasù jako pøíznakù pro klasifikaci.
\end{itemize}

\section{Textové tokeny} \label{Textove_tokeny}
Jak ji¾ bylo zmínìno v pøedchozím textu, 

Pro tokenizaci obecnì existuje velmi mnoho pøístupù a zpùsobù zde v¹ak popí¹eme pouze tøi, jejich¾ výsledky následnì v kapitole \ref{TESTY} porovnáne.


\subsubsection{Základní pøístup k tokenizaci}
Základní tokenizace dìlí vstupní vìtu pouze na jednotlivá slova, která jsou posléze pou¾ita jako tokeny. 

To znamená:

Mìjme vstupní text \textit{'Damned headache. I have to sleep.'}
takovýto text se rozdìlí na slova, tzn. vznikne následující seznam slov:

$['Damned', 'headache', 'I', 'have', 'to', 'sleep']$.

Takto vytvoøená slova jsou tedy tokeny s nimi¾ se klasifikátor uèí, nebo které klasifikuje.

\subsubsection{Základní pøístup k tokenizaci se stematizací}
Tento pøístup je velmi podobný pøedchozímu.
Li¹í se od nìj pouze tím, ¾e slova které jsou vytvoøeny rozdìlením pøevede na koøeny (stematizace).
Tímto krokem se sní¾í poèet slov ve slovníku klasifikátoru.
To zpùsobí, ¾e u Bayesovského klasifikátoru dojde k výraznému zmen¹ení velikosti slovníku vytvoøeného pøi uèení a u klasifikátoru SVM se sní¾í poèet dimenzí pøíznakù a zjednodu¹í se tak nalezení optimální nadroviny.

To znamená:

Mìjme vstupní text \textit{'Damned headache. I have to sleep.'}
takovýto text se rozdìlí na slova, tzn. vznikne následující seznam slov:

$['Damned', 'headache', 'I', 'have', 'to', 'sleep']$.

Nyní v¹echna tato slova pøevedeme na tvar koøene (stematizace), èím¾ získáme seznam následujících slov:

$['Damn', 'headache', 'I', 'have', 'to', 'sleep']$.

Takto vytvoøené koøeny slov se ji¾ pou¾ijí jako tokeny pro uèení klasifikátorù, nebo samozøejmì také mù¾e být vstupní text rozdìlený na tato slova klasifikátory klasifikován do daných tøíd.


\subsubsection{Alternativní pøístup k tokenizaci}
Základní pøístup k tokenizaci vstupního textu ale trpí jedním velkým neduhem, a to, ¾e tokeny jsou zcela samostatné bloky, které se, a» u¾ naivní Bayesovský klasifikátor, nebo klasifikátor zalo¾ený na support vector machines, uèí ani¾ by byl schopný z nich urèit kontext tìchto slov.
V¾dy» jak bylo ji¾ bylo vý¹e zmínìno, Bayesovský klasifikátor je oznaèován jako naivní právì proto, ¾e nebere v potaz závislosti jednotlivých tokenù na sobì (co¾ vychází z odvození rovnice \ref{equation_used} pou¾ívané pro klasifikaci viz. \ref{SF-BAYES-MAT-bayes_theorem-deriv}).
Proto jsem se sna¾il tuto nevýhodu kompenzovat tím, ¾e jsem pou¾il tento alternativní pøístup k tokenizaci zachovávající nejbli¾¹í kontext.
Ten na rozdíl od základní tokenizace nevytváøí tokeny pouze z jednotlivých slov, ale také z $N$ za sebou jdoucích slov, èím¾ bere v potaz jejich kontext ve vìtì.
Nevýhodou tohoto pøístupu je, ¾e pro velké uèící mno¾iny výraznì roste velikost slovníku u Bayesovského klasifikátoru a SVM klasifikátoru se zvìt¹uje poèet dimenzí pøíznakù, co¾ vede k obtí¾nìj¹ímu nalezení optimální dìlící nadroviny.
Tato vlastnost ale opìt mù¾e být alespoò èásteènì kompenzována stematizací jednotlivých slov, a tím i sní¾ením celkového poètu rùzných tokenù.
Uka¾me si alternativní pøístup k tokenizaci na pøíkladu:

Mìjme opìt zprávu \textit{'Damned headache. I have to sleep.'}.
Prvním krokem pøi zpracování této zprávy je její rozdìlení na vìty, proto¾e kontext za sebou jdoucích slov funguje v¾dy v rámci jedné vìty.
Pro rozdìlení vstupního textu na jednotlivé vìty je s úspìchem pou¾ita knihovna NLTK (viz. \ref{IMP-OBEC-NLTK}).
Vstupní text je nyní v tomto tvaru
$$['Damned\ headache.', 'I\ have\ to\ sleep]$$\\
Na rozdíl od základního pøístupu, který vytváøí tokeny pouze z jednotlivých slov, vytváøíme tokeny z libovolných za sebou jdoucích $N$-tic o maximální délce $N$, v¾dy tak, ¾e $N$-tice se skládají v¾dy jen ze slov dané vìty.
Hodnota $N$ je nastavena na optimální hodnotu urèenou experimentálnì za pomocí testù tak, aby stále je¹tì vylep¹ovala výslednou pøesnost klasifikátoru a pøitom aby velikost slovníku nepøesáhla pøijatelnou mez.
Pøed tím, ne¾ se samozøejmì ze slov udìlají tokeny, pøevedou se v¹echna slova na koøeny.
Tokeny pro vstupní text a $N = 3$ budou tedy vypadat následovnì pro první vìtu:
$$['Damn', 'headache', ('Damn', 'headache')]$$
a pro druhou vìtu:
$$['I', 'have', 'to', 'sleep', ['I', 'have'], ['have', 'to'], ['to', 'sleep'], ['I', 'have', 'to'], ['have', 'to', 'sleep']]$$
S tìmito tokeny pak klasifikátor pracuje stejnì jako s jakýmikoliv jinými tokeny.

\section{Pøíznaky a tokeny v Bayesovském klasifikátoru}

Bayesovský klasifikátor bere postupnì v¹echny tokeny a pøíznaky z trénovací mo¾inu a v závisosti na 
labelech trénovacích dat poèítá pravdìpodobnosti s jakými dané tokeny a pøíznaky nále¾í do tøídy spamu.
Takto vypoèítané pravdìpodobnosti pro jednotlivé tokeny a pøíznaky si ukládá do svého slovníku, pomocí neho¾ následnì klasifikuje nové vstupní texty.

V Bayesovském klasifikátoru je velmi dùle¾ité vybrat pro danou trénovací mno¾inu dat vhodné pøíznaky, které se z textu budou extrahovat a pou¾ívat pro klasifikaci.
Proto je v této práci klasifikátor trénován se v¹emi mo¾nými kombinacemi pou¾itých pøíznakù a jsou zvoleny ty pøíznaky, pro které mají výsledky klasifikátoru na testovacích datech s tìmito daty nejvìt¹í korelaci.

\section{Pøíznaky a tokeny v SVM klasifikátoru}

Na rozdíl od Bayesovského klasifikátoru, problém volby vhodných pøíznakù a tokenù u klasifikátoru zalo¾eném na SVM odpadá, co¾ vychází z definice SVM.
Jeliko¾ si klasifikátor pøi uèení vytváøí váhový vektor, který urèuje míru dùle¾itosti jednotlivých pøíznakù v prostoru pøíznakù na výsledek klasifikace, tak je jasné, ¾e klasifikátor doká¾e irelevantní pøíznaky ignorovat a øídit se hlavnì pøíznaky, které na výslednou klasifikaci budou mít nejlep¹í vliv.











\chapter{Vlastní implementace} \label{IMPLEMENTACE}
V této kapitole se budeme zabývat vlastní implementací obou klasifikaèních algoritmù z kapitol \ref{BAYES} a \ref{SVM}.
Kromì pou¾itých knihoven zde popí¹eme také architekturu programu implementovaného v praktické èásti této práce.
Také zde budou pøedstaveny problémy, ke kterým pøi implementaci do¹lo a také jakým zpùsobem byly vyøe¹eny

Celý program je implementován v programovacím jazyce Python 2.7.
Jazyk Python byl zvolen hlavnì z toho dùvodu, ¾e pro nìj existuje mnoho knihoven, které mohly být v této práci pou¾ity a za jejich pomocí se výraznì zjednodu¹ila implementace.
Mezi tyto pou¾ité knihovny patøí zejména knihovna pro zpracování pøirozené øeèi NLTK, knihovna Numpy (Numerical python) která umo¾òuje v prostøedí jazyka Python velmi jednodu¹e pracovat se slo¾itìj¹í matematikou a knihovna CVXOPT vyu¾itá v implementaci SVM klasifikátoru pro øe¹ení úloh kvadratického programování.

Tyto knihovny budou mimo jiné také podrobnìji popsány dále v této kapitole.

\section{Pou¾ité knihovny}
\subsection{NLTK} \label{IMP-NLTK}
NLTK neboli Natural Language ToolKit je balík knihoven pro skriptovací jazyk pyhton 2.7 urèený pro symbolické a statistické zpracovávání pøirozené øeèi.
Nabízí jednoduché rozhraní pro velké mno¾ství rùzných nástrojù pro zpracování textu, ale také velké mno¾ství ukázkových dat, které lze pou¾ít jako testovací a tréhovací data pøi vyvíjení softwaru pracujícího s pøirozeným jazykem.
Díky vynikající dokumentaci je NLTK skvìlou knihovnou výraznì zjednodu¹ující implementaci softwaru pracujícího nìjakým zpùsobem s pøirozeným jazykem.

V praktické èásti této práce je knihovna NLTK pou¾ívána jako souèást pøedzpracování vstupního textu urèeného buï k uèení klasifikiátorù, nebo ji¾ pøímo pro klasifikaci.
Je pou¾ita k tìmto dvìma operacím:
textem.
\begin{enumerate}
 \item Rozdìlení vìt -- Prvním modulem z této knihovny který je v práci pou¾it je modul /emph{punkt} (\emph{nltk/tokenize/punkt}), jen¾ rozdìluje vstupní text na list vìt.
 \item Pøevedení slov na jejich koøeny -- Druhým modulem z této knihovny je modul /emph{snowball} \emph{nltk.stem.snowball} jeho¾ úkolem je z jednotlivých slov vstupního textu udìlat koøen slov. (více viz \ref{Textove_tokeny})
\end{enumerate}

\subsection{Numpy} \label{IMP-NUMPY}
Numpy je základním balíkem pro numerické výpoèty v programovacím jazyce Python.
Tato knihovna nabízí práci s mnohadimenzionálními poli, dal¹ími objekty od nich odvozenými (masked arrays, matice, $\ldots$), s velkým mno¾stvím rychlých matematických operací nad poli zahrnujících diskrétní Fourierovy transformace, tøídìní, základní lineární algebru, a mnoho dal¹ími.
Tato knihovna je pro programy øe¹ící vìt¹í mno¾ství slo¾itìj¹ích matematických výpoètù prakticky nutností, nebo» výraznì zrychlí výpoèty a  programátorovi zjednodu¹í práci s matematickými daty.
Mnoho dal¹ích knihoven øe¹ící nìjaké rozsáhlej¹í matematické úkony je zalo¾ených právì na Numpy.
Mezi tyto knihovny patøi i CVXOPT (viz \ref{IMP-CVXOPT}) pou¾itý pro implementaci SVM klasifikátoru v praktické èásti této práce.

\subsection{CVXOPT} \label{IMP-CVXOPT}
CVXOPT je volnì dostupný balík knihoven pro øe¹ení konvexních optimalizaèních problému (mimo jiné do této skupiny problémù patøí i kvadratické programování pou¾itév implementaci SVM klasifikátoru) v programovacím jazyce Python.
Jejím hlavním úkolem je zjednodu¹it vývoj softwaru, jen¾ potøebuje pro svùj bìh øe¹it konvexní optimalizaèní úlohy, ani¾ by bylo potøeba implementovat slo¾ité optimalizaèní algoritmy.


\section{Architektura a implementace programu}
V této èásti kapitoly o architektuøe a implementaci programu v praktické èásti si ve zkratce pøedstavíme jak ji¾ název napovídá architekturu, tedy návrh struktury implementovaného programu, a implementaci klasifikaèních algoritmù z praktické èásti této práce.

\subsection{Architektura programu}
\subsubsection{Fyzická architektura}
Fyzická architektura programu pøedstavuje popis fyzického rozdìlení zdrojových souborù programu.
Toto rozdìlení zde bude popsáno, aby se ètenáø orientoval v následujícím textu zabývajícím se popisem implementace programu z praktické èásti této práce.

V koøenovém adresáøi tohoto projektu se nachází spou¹tìcí skript \texttt{control.py} , pomocí nìho¾ je program ovládaný a který nabízí ve¹keré operace, které se dají s programem provést.
Výpis nápovìdy k programu bude vypsán v Pøíloze XXXXX. 

Nad koøenovým adresáøem se nachází adresáø \texttt{/src/} obsahující tøi podadresáøe, ka¾dý obsahující jednu logickou èást programu:

\begin{enumerate}
 \item Adresáø \texttt{common} -- v tomto adresáøi se nacházejí ve¹keré operace, které jsou pro oba implementované klasifikátory spoleèné, a to extrakce textových tokenù a tokenù speciálních pøíznakù z textu.
 \item Adresáø \texttt{bayes} -- v tomto adresáøi se nachází kompletní implementace naivního Bayesovského klasifikátoru.
 \item Adresáø \texttt{svm} -- v tomto adresáøi se nachází kompletní implementace klasifikátoru zalo¾eného na SVM.
\end{enumerate}

Mimo èást se zdrojovými kódy se potom nachází adresáø data, který obsahuje ve¹kerá ruènì anotovaná data a natrénované klasifikaèní modely pro oba klasifikátory, které po umo¾òují klasifikátor pou¾ívat bez toho, ani¾ by se musel poka¾dé znovu spou¹tìt trénovací algoritmus na mno¾inì tréninkových dat.

\subsubsection{Architektura tøíd}
V této èásti dokumentu bude popsána architektura tøíd s jejich krátkým popisem, ketrá umo¾ní èitateli velmi rychle pochopit architekturu a funkcnost programu.

Program implementovaný jako praktická èást této práce je, jak ji¾ bylo vý¹e zmínìno (\ref{IMPLEMENTACE}), implementován  v programovacím jazyce Python, jen¾ umo¾òuje vyu¾ití objektovì orientovaného pøístupu k vývoji aplikací.
Díky této vlastnosti Pythonu bylo mo¾né implementovat program objektovì a vyu¾ít tak výhod tohoto pøístupu.

Jak Bayesovský klasifikátor, tak klasifikátor zalo¾ený na support vector machines jsou v praktické èásti této práce vytvoøeny tak, aby bylo mo¾né je pou¾ívat jako modulù programovacího jazyka Python.
To znamená, ¾e lze oba klasifikátory importovat jako modul do novì implementované aplikace a pou¾ít pro klasifikaci libovolného vstupního textu.
Tohoto bylo vyu¾ito v projektu M-eco, v nìm¾ byl Bayesovský klasifikátor pou¾itý ve skriptu vkládajícím nová data do databáze, kde filtroval relevantní data od nerelevantní a relevantní data byla následnì vkládána do databáze.
V projektu jsou tedy implementovány dvì základní tøídy \texttt{SVM} a \texttt{BayesianClassifier}.
Obì tyto tøídy obsahují metody pro trénování klasifikátoru z trénovacích dat a metody umo¾òující po pøedchozím nauèení klasifikátoru klasifikovat data do daných tøíd.

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=15cm,keepaspectratio]{fig/Features-diag}
      \caption{Diagram tøíd dìdících z tøídy Features} 
      \label{features-diag}
    \end{center}
\end{figure}

Jeliko¾ jak Bayesovský klasifikátor, tak klasifikátor zalo¾ený na support vector machines pracují se vstupním textem prakticky stejnì, byla vytvoøena tøída \texttt{Entry} a tøídy dìdící z rodièovské tøídy \texttt{Feature} (viz obr. \ref{features-diag}), nacházející se v adresáøi texttt{common}.
Tyto tøídy pou¾ívají oba klasifikátory pro práci s ve¹kerým vstupním textem, tedy jak pro klasifikaci, tak pro uèení.
Instance tøídy \texttt{Entry} se vytváøí pro ka¾dý text vstupující do klasifikátoru, jejím úkolem je text rozdìlit na tokeny tak, aby samotné klasifikátory ji¾ s textem nemusely nijak pracovat, ale aby dostaly pouze seznam textových a speciálních tokenù, jen¾ vstupní text obsahoval.
Tato tøída tedy zodpovídá za vyhledávání jednotlivých speciálních tokenù (viz \ref{TOKENIZACE-SPEC}), jen¾ potom jako instance tøíd dìdících ze tøídy \texttt{Features} spoleènì s $N$-ticemi jednotlivých textových tokenù (viz \ref{Textove_tokeny}) pøedává klasifikátorum.

Aby bylo mo¾né oba klasifikátory testovat a posléze i vzájemnì porovnávat, byly pro ka¾dý klasifikátor vytvoøeny tøídy, je¾ mají na starost testování.
tyto tøídy se nazývají \texttt{SVMTest} pro klasifikátor zalo¾ený na support vector machines a 
\texttt{BayesianTest} pro Bayesovský klasifikátor.
Obì tyto tøídy obsahují metody pro spou¹tìní testù klasifikátorù a jejich následné vyhodnocení.

Jeliko¾ oba implementované klasifikátory pracují na diametrálnì odli¹ných pøístupech, má ka¾dý klasifikátor potøebu jinak nakládat s tokeny získanými z instancí tøídy \texttt{Entry}.
Proto ka¾dý klasifikátor obsluhuje nìkolik tøíd, je¾ pøi klasifikaci, nebo uèení pou¾ívá.

V pøípadì Bayesovského klasifikátoru je takovouto dal¹í pou¾ívanou tøídou pouze jediná tøída s názvem \texttt{WordDictionary}, která se stará o slovník tokenù Bayesovského klasifikátoru.
Tento slovník tokenù obsahuje pravdìpodobnostní hodnoty jednotlivých tokenù nalezených bìhem uèení a pravdìpodobnost jejich nále¾itosti do urèité tøídy. 
Tato tøída také zabezpeèuje ve¹keré operace provádìné s tímto slovnikem tokenù, jako napøíklad jeho ulo¾ení do souboru a opìtovné naètení, díky èemu¾ je mo¾né Bayesovsý klasifikátor, stejnì jako klasifikátor zalo¾ený na metodì support vector machines pou¾ívat, ani¾ bychom jej museli bezprostøednì pøed klasifikací uèit, co¾ je velmi zdlouhavý proces.
Mimo operací exportu a importu slovníku také tato tøída umo¾òuje exportovat daný slovník do souboru v jazyce XML.

Podobnì jako Bayesovský klasifikátor, potøebuje i klasifikátor zalo¾ený na metodì support vector machines tøídou, jen¾ pracuje se vstupní textem a tokeny z nìj vytvoøenými a pøipraví ji do podoby, ve které je klasifikátor schopen s ním pracovat.
Tato tøída se u klasifikátoru zalo¾eného na SVM jmenuje \texttt{Data}.
Její funkcí je vytvoøit prostor pøíznakù ze vstupních dat a data rozdìlit na bloky trénovacích a testovacích dat.
Dal¹í specifickou tøídou, jen¾ klasifikátor zalo¾ený na SVM pou¾ívá je tøída \texttt{Kernel} a její podtøídy, jen¾ klasifikátor potøebuje pro výpoèet jaderné funkce pro klasifikaci.
Tuto tøídu dìdí v implementovaném programu následující tøi specifické jaderné funkce:

\begin{itemize}
  \item Lineární jaderná funkce
  \item Polynomiální jaderná funkce
  \item Radiální bázová jaderná funkce
\end{itemize}

Poslední tøídou která je v souvislosti s klasifikátorem zalo¾eným na SVM spojená je tøída \texttt{Annealing}, je¾ øe¹í obtí¾ný úkol vhodného výbìru parametrù u klasifikátoru a jím pou¾ívané jaderné funkce.
Více o tomto výbìru bude popsáno v èásti o implementaci klasifikátoru SVM (viz \ref{IMPLEMENTATION-SVM}).

Tímto jsme velmi obecnì pro¹li v¹echny tøídy implementované v prakcické èásti této práce.
Pro podrobnìj¹í popis tøíd je vgenerována dokumentace pomocí dokumentaèního nástroje \texttt{epydoc}.
Dokumentace je pøilo¾ena na CD, je¾ je souèástí této práce.

\section{Implementace}
V této èásti kapitoly se budem podrobnìji zabývat postupem implementace jednotlivých vybraných èásti programu z praktické èásti této práce.
Pokusíme se zde rozebrat problémy, jen¾ pøi implementaci nastaly a jejich øe¹ení.
Tato sekce bude pro vìt¹í pøehlednost rozdìlena na implementaci Bayesovského klasifikátoru a klasifikátoru zalo¾eného na SVM, ve kterých budeme rozebírat odpovídající implementaèní problémy.


\section{Implementace Bayesovského klasifikátoru}
Bayesovský klasifikátor je implementován jako modul ve skriptovacím jazyce Python verze 2.7.
Pro výpoèet klasifikace je pou¾ita rovnice \ref{equation_used}. 
\subsection{Ukládání dat}
Klasifikátor pou¾ívá pro ukládání dat pøedev¹ím tedy databáze pravdìpodobností jednotlivých tokenù, na které klasifikátor pøi svém uèení narazil, serializaci.
To je proces, pøi nìm¾ je datová struktura, nebo objekt pøeveden na proud dat, jen¾ se dá jednodu¹e ulo¾it a zpìtnì pøevést zpìt na danou strukturu èi objekt.
V klasifikátoru je pou¾ita knihovní funkce pythonu s názvem \textit{pickle}, umo¾òující provést serializaci libovolné datové struktury pythonu a zpìt.


\section{Implementace klasifikátoru SVM}\label{IMPLEMENTATION-SVM}

\subsection{Parametry kvadratického programování}

\subsection{Volba vhodných parametrù klasifikátoru a Jaderných funkcí}






\chapter{Testy} \label{TESTY}

%./control.py common -d ../data/tweets/annotated.db -n 5 -c 100000 -t 1 -k RBF -a 0.5 -b 0.5
%##################################
%########## svm restults ##########
%##################################
%True positive = 93.0
%True negative = 92.8
%False positive = 7.2
%False negative = 7.2
%Precision = 0.928143712575
%Recall = 0.928143712575
%Accuracy = 0.928071928072
%F-measure = 0.928143712575
%##################################
%
%##################################
%########## bayes restults ##########
%##################################
%True positive = 90.0
%True negative = 91.2
%False positive = 10.0
%False negative = 10.0
%Unknown = 0.0
%Precision = 0.9
%Recall = 0.9
%Accuracy = 0.900596421471
%F-measure = 0.9
%Corelation = 0.836320369874
%##################################
%{'emoticon': 3, 'sentence': 1, 'url': 2, 'tag': 1, 'time': 0, 'date': 0, 'email': 2}





\section{Data}\label{TEST-DATA}
Jedním z úkolù vyplývajících ze zadání této práce bylo vytvoøit datovou sadu pro uèení a testování vytvoøených klasifikátorù.
Zdrojem tìchto dat byla databáze tweetù projektu M-eco\footnote{http://www.meco-project.eu/}, ve kterém participuje výzkumná skupina NLP pracující pøi Fakultì informaèních technologii VUT v Brnì.

Byly vytvoøeny dvì datové sady, jedna pro anglický jazyk a druhá pro nìmecký.
Celkovì bylo anotováno 4500 tweetù pro anglický jazyk a 3000 tweetù pro nìmecký.
Tyto datové sady potom byly rozdìleny na dvì èásti, z nich¾ jedna byla pou¾ita pro trénování klasifikátoru a druhá pro testování.
Aby klasifikátor fungoval správnì, mìlo by rozlo¾ení relevantních a nerelevantních tweetù v trénovací mno¾inì být zhruba 50/50.
Tento pomìr vycházi z odvození klasifikaèní rovnice (viz \ref{SF-BAYES-MAT-bayes_theorem-deriv}).
Pro anglický jazyk jsou tweety v trénovací mno¾inì zhruba takto rozlo¾eny, nicménì pro nìmecký jazyk jsou tweety rozlo¾eny nerovnomìrnì (asi 80\% relevantních a 20\% nerelevantních).
Mno¾ina nìmeckých trénovacích dat se v¹ak bude je¹tì roz¹iøovat a tento pomìr bude vyrovnán.


\section{Úloha implementovaného klasifikátoru}
Jak bylo ji¾ zmínìno v úvodu práce (\ref{UVOD-MOTIVACE}), jedním z mých cílù je v této práci vytvoøit klasifikátor pro klasifikaci tweetù v projektu M-eco.
Implementovaný klasifikátor je nauèen na rozdìlení vstupních tweetù do dvou tøíd -- \textbf{zabývající se osobními zku¹enostmi pisatelù s nemocemi (ale také zku¹enostmi z pisatelova okolí)}, nebo \textbf{nezabývající se tìmito zku¹enostmi}.
Tedy napøíklad tweet \textit{'I have a huge headache'} nebo \textit{'My dad feels sick'} jsou relevantní, naproti tomu \textit{'Canadians should expect to see more severe cases of swine flu.'} je irelevantní. 

\section{Testovací metriky} \label{TEST-METRIKY}
Aby bylo mo¾né nìjakým zpùsobem porovnat rùzné výsledky klasifikátoru a také posléze klasifikátory mezi sebou, je tøeba zavést vhodné metriky popisující vlastnosti klasifikátoru.
V této èásti práce budou tyto pou¾ité metriky vysvìtleny.
\subsection{Korelace}
Korelace je statistické metoda, která definuje vzájemný vztah mezi velièinami $X$ a $Y$.
Míru korelace urèuje koeficient korelace nabývající hodnot $<-1, 1>$.
Jestli¾e korelaèní koeficient nabývá hodnotu -1, pak to znaèí, ¾e velièiny $X$ a $Y$ jsou na sobì zcela nezávislé.
Naopak nabývá-li korelaèní koeficient hodnoty 1, pak jsou na sobì velièiny pøímo závislé.

Korelaèní koeficient se vypoèítá jako 
$$
  K(X,Y) = \frac{E(XY) - E(X)E(Y)}{\sqrt{E(X^2 - E(X)^2)} \sqrt{(E(Y^2) - E(Y)^2)}}
$$
kde $X$ jsou klasifikátorem automaticky spoèítané pravdìpodobnosti a $X$ jsou pravdìpodobnosti zadané u¾ivatelem pøi anotaci ($0.01 = spam$ a $0.99 = ham$).

Pøi testování klasifikátoru se poèítá korelace mezi u¾ivatelem zadanou hodnotou vstupního textu (pøi anotaci testovací mno¾iny) a výsledkem klasifikátoru.
Èím více se tedy korelaèní koeficient blí¾í hodnotì 1, tím lep¹í výsledek klasifikátoru pøedstavuje.

\subsection{Výsledek klasifikace}\label{TEST-VYS_KLAS}
Abychom byli schopni pøesnì zjistit jak klasifikátor testovací mno¾inu oklasifikoval, rozdìlíme oklasifikovaná data do ètyø skupin.
\begin{itemize}
 \item \textit{True positive} -- poèet tweetù, které byly klasifikátorem správnì zaøazeny do relevantní tøídy.
 \item \textit{True negative} -- poèet tweetù, které byly klasifikátorem správnì zaøazeny do nerelevantní tøídy.
 \item \textit{False positive} -- poèet tweetù, které byly klasifikátorem ¹patnì zaøazeny do relevantní tøídy.
 \item \textit{False negative} -- poèet tweetù, které byly klasifikátorem ¹patnì zaøazeny do nerelevantní tøídy.
\end{itemize}
Tyto hodnoty jsou tedy výsledkem porovnání pøedpokládaného výstupu klasifikátoru s reálným výstupem implementovaného klasifikátoru.
Je zøejmé, ¾e èím ménì záznamù se nachází ve False negative a False positive, tím lépe klasifikátor funguje.

\subsection{Precision, Recall, Accuracy, F--measure}
Dal¹ími metrikami, pomocí nich¾ budeme moci porovnávat výsledky klasifikátorù, jsou funkce Precision(pøesnost), Recall(odezva), Accuracy(pøesnost\footnote{'Accuracy' a 'Precision' se do èeského jazyka pøekládají stejnì, tzn. jako pøesnost, nicménì jde o jiné metriky pro porovnávání klasifikátorù. Z toho dùvodu budeme metriky nazývat anglickými názvy.}) a F--measure.
Pro následující metody se pøedpokládá, ¾e jsme schopni vypoèítat hodnoty \textit{True positive}, \textit{True negative}, \textit{False positive} a \textit{False negative} (viz \ref{TEST-VYS_KLAS})
\subsubsection{Accuracy}
Nejjednodu¹¹í metrikou pro porovnávání pøesnosti klasifikátorù je metrika Accuracy.
Lze ji jednodu¹e vypoèítat následovnì:
$$
  Accuracy = \frac{true\_positive + true\_negative}{true\_positive + true\_negative + false\_positive + false\_negative}
$$
Nevýhodou této metriky v¹ak je, ¾e nebere v potaz poèty záznamù v jednotlivých tídách.

\subsubsection{Precision. Recall}
$$
  Precision = \frac{true\_positive}{true\_positive + false\_positive}
$$
$$
  Recall = \frac{true\_positive}{true\_positive + false\_negative}
$$
Precision tedy mù¾eme definovat jako pomìr správnì oklasifikovaných relevantních záznamù vùèi v¹em oklasifikovaným relevantním záznamùm.
Recall je potom pomìr správnì oklasifikovaných relevantních záznamù vùèi skuteènì relevantním záznamùm
\subsubsection{F--measure}
$$
  Fmeasure = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
$$
F--measure je jedna z metrik nejèastìji pou¾ívaných pro hodnocení klasifikátorù.
De facto jde o váhovaný prùmìr Precision a Recall.
F--measure nabývá hodnot $<0,1>$, kde 0 je nejhor¹í skóre popisující výsledek klasifikátoru a 1 je nejlep¹í.

\section{Testy Bayesovského klasifikátoru}
Nyní pøistupme k pøedstavení výsledku testù Bayesovského klasifikátoru.
Hlavním cílem jeho testování bylo nalézt optimální délku tokenù pøi pou¾ití nového zpùsobu tokenizace (viz \ref{IMP-BAYES-TOKEN}).
Testy jsou provádìny na manuálnì anotovaných datech(viz. \ref{TEST-DATA}.
Testy provádíme samostatnì pro anglický jazyk na anglických tweetech a pro nìmecký jazyk na nìmeckých tweetech.

\subsection{Angliètina}
Velikost testovací sady pro anglický jazyk bylo 120 tweetù, z nich¾ 60 bylo relevantních a dal¹ích 60 irelevantních k tématu.
Pro zji¹tìní ideální délky $N$-tic tokenù jsme aplikovali testy pro postupnì zvìt¹ující se $N$, a to dokud se zlep¹ovala hodnota korelaèního koeficientu (korelace) a F--measure (viz \ref{TEST-METRIKY}).
Následující tabulka øíká, jak se klasifikátor choval pøi zmìnì nastavení maximální délky za sebou jdoucích $N$-tic slov(tokenù).
\begin{center}
\begin{scriptsize}
\begin{tabular}{| c || c | c | c | c | c | c |}
  N-length       & 1             & 2              & 3             & 4              & 5              & 6              \\
  Korelace       & 0.5184        & 0.5409         & 0.5541        & 0.559          & 0.5668         & 0.5668         \\
  True positive  & 51.0 (42.5\%) & 52.0 (43.33\%) & 54.0 (45.0\%) & 55.0 (45.83\%) & 55.0 (45.83\%) & 55.0 (45.83\%) \\
  True negative  & 48.0 (40.0\%) & 48.0 (40.0\%)  & 48.0 (40.0\%) & 48.0 (40.0\%)  & 48.0 (40.0\%)  & 48.0 (40.0\%)  \\
  False positive & 12.0 (10.0\%) & 12.0 (10.0\%)  & 12.0 (10.0\%) & 12.0 (10.0\%)  & 12.0 (10.0\%)  & 12.0 (10.0\%)  \\
  False negative & 9.0 (7.5\%)   & 8.0 (6.67\%)   & 6.0 (5.0\%)   & 5.0 (4.17\%)   & 5.0 (4.17\%)   & 5.0 (4.17\%)   \\
  Precision      & 0.8095        & 0.8125         & 0.8182        & 0.8209         & 0.8209         & 0.8209         \\
  Recall         & 0.85          & 0.8667         & 0.9           & 0.9167         & 0.9167         & 0.9167         \\
  Accuracy       & 0.825         & 0.8334         & 0.85          & 0.8583         & 0.8583         & 0.8583         \\
  F-measure      & 0.829         & 0.8387         & 0.8572        & 0.8661         & 0.8661         & 0.8661         \\
\end{tabular}
\end{scriptsize}
\end{center}

\subsection{Nìmecký jazyk}
Pro nìmecký jazyk jsme spou¹tìli obdobné testy jako pro jazyk anglický, tzn. zji¹»ovali jsme optimální délku $N$-tic pro tokenizaci.
Testovací mno¾ina pro nìmecký jazyk sestává z 25 relevantních tweetù a 191 irelevantních. 
\begin{center}
\begin{scriptsize}
\begin{tabular}{| c || c | c | c | c | c | c |}
  N-length       & 1              & 2               & 3               & 4               & 5               & 6               \\
  Korelace       & 0.2723         & 0.2423          & 0.2433          & 0.2429          & 0.2429          & 0.2429          \\
  True positive  & 22.0 (10.19\%) & 22.0 (10.19\%)  & 22.0 (10.19\%)  & 22.0 (10.19\%)  & 22.0 (10.19\%)  & 22.0 (10.19\%)  \\
  True negative  & 103.0 (47.69\%)& 106.0 (49.07\%) & 107.0 (49.54\%) & 107.0 (49.54\%) & 107.0 (49.54\%) & 107.0 (49.54\%) \\
  False positive & 88.0 (40.74\%) & 85.0 (39.35\%)  & 84.0 (38.89\%)  & 84.0 (38.89\%)  & 84.0 (38.89\%)  & 84.0 (38.89\%)  \\
  False negative & 3.0 (1.39\%)   & 3.0 (1.39\%)    & 3.0 (1.39\%)    & 3.0 (1.39\%)    & 3.0 (1.39\%)    & 3.0 (1.39\%)    \\
  Precision      & 0.2            & 0.2056          & 0.2075          & 0.2075          & 0.2075          & 0.2075          \\
  Recall         & 0.88           & 0.88            & 0.88            & 0.88            & 0.88            & 0.88            \\
  Accuracy       & 0.5787         & 0.5926          & 0.5972          & 0.5972          & 0.5972          & 0.5972          \\
  F-measure      & 0.3259         & 0.3333          & 0.3358          & 0.3358          & 0.3358          & 0.3358          \\
\end{tabular}
\end{scriptsize}
\end{center}

\subsection{Shrnutí výsledkù testù}
Z výsledkù pro jednotlivé jazyky je zøejmé, ¾e pro anglický jazyk klasifikátor pracuje vcelku spolehlivì a s pomìrnì vysokou pøesností (F--measure = 0.8661).
Pro nìmecký jazyk jsou v¹ak výsledky klasifikace pomìrnì ¹patné (F--measure = 0.3358).
Nepøesnost klasifikátoru je zøejmì zpùsobena nevyvá¾eností trénovacích dat, ale také tím, ¾e nìmecký jazyk je gramaticky výraznì slo¾itìj¹í, ne¾ jazyk anglický.

Zamìøme se nyní na vylep¹ení klasifikátoru, které pøiná¹í alternativní pøístup k tokenizaci oproti klasickému pøístupu (viz \ref{IMP-BAYES-TOKEN}).
Podíváme-li se do tabulky výsledkù klasifikátoru, je jasnì vidìt, ¾e pøesnost klasifikátoru roste s rostoucím $N$.
Tento rùst pøesnosti v¹ak není nekoneèný, a to pøedev¹ím kvùli tomu, ¾e slova která jsou spolu v kontextu, bývají ve vìtì velmi èasto blízko sebe.
Pokud je $N$ nastaveno na 1, pak je pro tokenizaci pou¾it klasický pøístup.

Pøi testování klasifikátoru na anglických tweetech je z tabulky zøejmé, ¾e hodnota korelace roste a¾ do $N = 5$, tzn. tokeny jsou vytvoøeny a¾ z pìti za sebou jdoucích slov.
Bylo by tedy vhodné pøi nasazení klasifikátoru nastavit $N$ na hodnotu 5.
I v tabulce popisující klasifikátor nìmeckých tweetù je vidìt, ¾e pou¾ití nové metody tokenizace je výhodné, aèkoliv v tomto pøípadì jsou výsledky klasifikátoru zlep¹eny jen malou mìrou.

\chapter{Závìr} 
V této semestrální práci jsem se vìnoval obecným klasifikaèním metodám se zamìøením na klasifikaci textu.
Uvedl jsem zde také struènì historický vývoj oboru zpracování pøirozeného jazyka a velmi struèný výèet nìkterých jeho základních úloh.

Hlavním tématem v¹ak byla klasifikace textu podle tématu, co¾ bylo také zadáním této práce.
Jako hlavní metoda pro øe¹ení tohoto problému jsem vybral metodu filtrování spamu nazývanou Bayesovský klasifikátor (Bayesian Classifier).
Rozebral jsme dopodrobna matematický model této metody a pøedstavil jsem pøíklady jejího u¾ití pøi klasifikaci textových dokumentù a nìkteré mo¾nosti jejího vylep¹ení.

V dal¹í èásti práce jsem implementován Bayesovský klasifikátor pro klasifikaci tweetù z internetové sociální sítì Twitter.
Klasifikátor mìl tyto tweety rozdìlit do dvou tøíd urèujících relevanci, nebo naopak nerelevanci daných tweetù z hlediska jejich vyu¾ití v epidemiologických systémech.
Pro tokenizaci vstupního tweetu jsem vytvoøil pøístup sna¾ící se alespoò èásteènì odstranit slabinu Bayesovského klasifikátoru zpùsobující, ¾e klasifikátor nebere v potaz kontext slov ve vìtì, ale pouze samostatná slova.
Tato vlastnost klasifikátoru vychází z odvození rovnice, pou¾ité pro klasifikaci, z Bayesova teorému.
Pomocí testù jsem nakonec dokázal, ¾e implementovaný pøístup vylep¹ení klasické tokenizace pomáhá zlep¹it výsledky Bayesovského klasifikátoru.

V této práci bych rád pokraèoval a roz¹íøil ji na diplomovou práci.
V té bych chtìl na doposud implementovaný Bayesovský klasifikátor pou¾ívající alternativní metodu tokenizace pou¾ít metodu boosting pro dal¹í zlep¹ení pøesnosti klasifikace.
Druhým klasifikátorem, který bych pro svou diplomovou práci chtìl implementovat, je neprobabilistický klasifikátor SVM (Support vector machine).
Tyto dva implementované klasifikátory zamý¹lím porovnat podle zavedených metrik a lep¹í klasifikátor vyu¾ít rovnì¾ ve své dal¹í práci pøi klasifikaci tweetù pro projekt M-eco.

\nocite{automatic_sumarization}
\nocite{anonymisation}
\nocite{nlg}
\nocite{clustering}
\nocite{Manning}
\nocite{SRM}
\nocite{boosting_bayesian}
\nocite{Malik2007thesis}

%=========================================================================
